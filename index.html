<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>只在此山中，云深不知处</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="只在此山中，云深不知处">
<meta property="og:url" content="https://nrliangxy.github.io/yunshenBlog.github.io/index.html">
<meta property="og:site_name" content="只在此山中，云深不知处">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/yunshenBlog.github.io/atom.xml" title="只在此山中，云深不知处" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/yunshenBlog.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/yunshenBlog.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/yunshenBlog.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/yunshenBlog.github.io/" id="logo">只在此山中，云深不知处</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/yunshenBlog.github.io/">Home</a>
        
          <a class="main-nav-link" href="/yunshenBlog.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/yunshenBlog.github.io/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nrliangxy.github.io/yunshenBlog.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-我的第一篇博客" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" class="article-date">
  <time class="dt-published" datetime="2025-02-07T23:57:10.000Z" itemprop="datePublished">2025-02-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>spark集群在standalone模式下，调度模式FIFO和FAIR针对的调度对象是单个应用内的所有的Schedulable实例，<br>不会对spark集群里面不同的应用app产生调度作用</li>
<li>spark集群在standalone模式下，不同应用app的调度默认为FIFO模式调度，并且<code>不支持修改</code></li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>组内spark集群为standalone模式，有几个关键性任务需要优先给他们分配资源，让它们优先运行。我们通过网上搜索相关资料<br>和询问相关大模型，获得可以通过修改spark的调度模式来调整不用应用的调度优先级方法。</p>
<h2 id="查询资料"><a href="#查询资料" class="headerlink" title="查询资料"></a><em>查询资料</em></h2><h3 id="资料链接"><a href="#资料链接" class="headerlink" title="资料链接"></a>资料链接</h3><ul>
<li><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_16213300/10500039">spark调度模式分析</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cuishuai/p/7910348.html">spark调度FAIR策略</a></li>
<li>大模型搜索：<code>spark集群standalone模式下，fair模式的详细配置</code>或者<code>spark集群standalone模式下，如何修改调度模式</code></li>
</ul>
<h3 id="上面资料的错误结论"><a href="#上面资料的错误结论" class="headerlink" title="上面资料的错误结论"></a>上面资料的<em>错误结论</em></h3><ul>
<li>资料里面都是指出spark集群有两种调度模式，分别是FIFO和FAIR，集群默认的调度方式是FIFO。</li>
<li>可以通过调整spark集群配置，将配置改为FAIR模式，添加fairscheduler.xml配置文件，<br>来实现集群里面不同应用之间的优先级调度。</li>
</ul>
<h3 id="实验测试"><a href="#实验测试" class="headerlink" title="实验测试"></a><em>实验测试</em></h3><h4 id="1，实验结论"><a href="#1，实验结论" class="headerlink" title="1，实验结论"></a>1，实验结论</h4><ul>
<li>经过多次测试后，发现每次同时提交多个任务，并将优先级高的任务提交到权重高的子pool里面。这些任务获得资源的<br>策略依然是：<ul>
<li>FIFO先到先得策略，先提交的任务会优先获得资源</li>
<li>集群会默认使用资源平均分配原则对每个任务进行资源分配</li>
</ul>
</li>
<li>优先级高的任务并没有优先获得资源</li>
</ul>
<h4 id="2，原因分析"><a href="#2，原因分析" class="headerlink" title="2，原因分析"></a>2，原因分析</h4><h5 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h5><ul>
<li>在standalone模式下，使用FAIR或者FIFO进行排序的核心函数是<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;Pool.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.Pool#getSortedTaskSetQueue</li>
<li>以下为getSortedTaskSetQueue详细代码<ul>
<li>其中schedulableQueue队列存储的是需要进行排序的Schedulable，taskSetSchedulingAlgorithm.comparator为排序算法<br>分别为FAIR和FIFO</li>
<li>现在可以通过确认schedulableQueue队列存储的是整个集群里面应用的Schedulable，还是只存储单个应用里面的Schedulable。如<br>果存储的是整个集群里面应用的Schedulable对象，那就能证明fair调度模式可以改变spark集群内不同应用之间的任务调度顺序，如果只是<br>存储的单个应用里面的Schedulable对象，那就说明fair调度模式不能改变spark集群内不同应用之间的任务调度顺序</li>
<li>通过上面的分析，现在只需要确定schedulableQueue队列存储是什么对象就能确定答案了<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">override def getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] = &#123;</span><br><span class="line">//    这行代码创建了一个空的 ArrayBuffer，用于存储排序后的 TaskSetManager 对象。这将是返回的最终结果。</span><br><span class="line">    val sortedTaskSetQueue = new ArrayBuffer[TaskSetManager]</span><br><span class="line">//    collection.sortWith((x, y) =&gt; comparison)</span><br><span class="line">//    x 和 y：集合中的两个元素。</span><br><span class="line">//    comparison：一个布尔表达式，表示排序规则。如果 comparison 返回 true，则 x 会排在 y 的前面。</span><br><span class="line">    /**</span><br><span class="line">     * TaskSchedulerImpl.scala submitTasks 调用schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line">     * schedulableBuilder是从initialize函数里面指定了FIFOSchedulableBuilder模式或者FairSchedulableBuilder模式，调用这两个类里面的</span><br><span class="line">     * addTaskSetManager，然后调用Pool.scala 里面的addSchedulable函数</span><br><span class="line">     * schedulableQueue 是用来管理单个应用内的任务调度的。它存储的是该应用的所有 Schedulable，包括 TaskSetManager（代表具体的 TaskSet）和可能的子 Pool。</span><br><span class="line">     */</span><br><span class="line">    val sortedSchedulableQueue =</span><br><span class="line">      schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)</span><br><span class="line">//    这行代码开始遍历排序后的 schedulableQueue（现在是 sortedSchedulableQueue）。每个 schedulable 都是一个 Schedulable 对象，</span><br><span class="line">//    它可能是一个任务集（TaskSetManager）的父类或者接口。</span><br><span class="line">    for (schedulable &lt;- sortedSchedulableQueue) &#123;</span><br><span class="line">      sortedTaskSetQueue ++= schedulable.getSortedTaskSetQueue.filter(_.isSchedulable)</span><br><span class="line">    &#125;</span><br><span class="line">    sortedTaskSetQueue</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>上面的函数getSortedTaskSetQueue里面，变量schedulableQueue为val schedulableQueue &#x3D; new ConcurrentLinkedQueue[Schedulable]<ul>
<li>该变量的元素是通过什么函数进行添加的？</li>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;Pool.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.Pool#addSchedulable</li>
<li>addSchedulable详细代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * 通过SchedulableBuilder.scala 的addTaskSetManager方法将FIFOSchedulableBuilder.scala对应的Schedulable或者FairSchedulableBuilder.scala</span><br><span class="line">   * 对应的parentPool添加到schedulableQueue队列里面</span><br><span class="line">   * @param schedulable</span><br><span class="line">   */</span><br><span class="line">  override def addSchedulable(schedulable: Schedulable): Unit = &#123;</span><br><span class="line">    require(schedulable != null)</span><br><span class="line">    schedulableQueue.add(schedulable)</span><br><span class="line">    schedulableNameToSchedulable.put(schedulable.name, schedulable)</span><br><span class="line">    schedulable.parent = this</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个函数在调用addSchedulable函数来为schedulableQueue队列添加元素<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;SchedulableBuilder.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.FairSchedulableBuilder#addTaskSetManager<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * 运行示例和工作机制</span><br><span class="line">   *  假设有以下调度场景：</span><br><span class="line">   *</span><br><span class="line">   *  用户提交了一个任务，指定了 FAIR_SCHEDULER_PROPERTIES=highPriority。</span><br><span class="line">   *</span><br><span class="line">   *  系统在 rootPool 中查找是否已有名称为 highPriority 的调度池：</span><br><span class="line">   *</span><br><span class="line">   *  如果存在，将任务添加到 highPriority 池中。</span><br><span class="line">   *  如果不存在，系统创建 highPriority 池并将任务添加到其中。</span><br><span class="line">   *  如果用户未指定 FAIR_SCHEDULER_PROPERTIES，任务将被分配到默认池 DEFAULT_POOL_NAME 中。</span><br><span class="line">   *</span><br><span class="line">   *  通过这种方式，Spark 实现了任务与调度池之间的动态分配和管理。</span><br><span class="line">   *</span><br><span class="line">   * @param manager</span><br><span class="line">   * @param properties</span><br><span class="line">   */</span><br><span class="line">  override def addTaskSetManager(manager: Schedulable, properties: Properties): Unit = &#123;</span><br><span class="line">//    检查 properties 是否为空。</span><br><span class="line">//    properties 是任务的属性对象，可能包含用户指定的调度池名称。</span><br><span class="line">//    如果 properties 不为空，从中获取 FAIR_SCHEDULER_PROPERTIES（即调度池名称）的值。</span><br><span class="line">//    如果未指定，则使用默认池名 DEFAULT_POOL_NAME。</span><br><span class="line">//    如果 properties 为空，直接使用默认池名。</span><br><span class="line">//    目的：确定当前任务所属的调度池名称。</span><br><span class="line">    val poolName = if (properties != null) &#123;</span><br><span class="line">        properties.getProperty(FAIR_SCHEDULER_PROPERTIES, DEFAULT_POOL_NAME)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        DEFAULT_POOL_NAME</span><br><span class="line">      &#125;</span><br><span class="line">//    从 rootPool 中根据 poolName 查找对应的调度池。</span><br><span class="line">//    rootPool 是调度池的根对象，存储了所有子调度池。</span><br><span class="line">    var parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">//    如果未找到对应的调度池（即 parentPool == null）：</span><br><span class="line">//    1，创建一个新的调度池 Pool。</span><br><span class="line">//    2，使用默认配置初始化新调度池：</span><br><span class="line">//    调度模式：DEFAULT_SCHEDULING_MODE。</span><br><span class="line">//    最小共享资源：DEFAULT_MINIMUM_SHARE。</span><br><span class="line">//    权重：DEFAULT_WEIGHT。</span><br><span class="line">//    目的：确保所有任务都能分配到某个调度池，即使该池未事先配置。</span><br><span class="line">    if (parentPool == null) &#123;</span><br><span class="line">      // we will create a new pool that user has configured in app</span><br><span class="line">      // instead of being defined in xml file</span><br><span class="line">      parentPool = new Pool(poolName, DEFAULT_SCHEDULING_MODE,</span><br><span class="line">        DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)</span><br><span class="line">//    将新创建的调度池添加到 rootPool。</span><br><span class="line">      rootPool.addSchedulable(parentPool)</span><br><span class="line">      logWarning(s&quot;A job was submitted with scheduler pool $poolName, which has not been &quot; +</span><br><span class="line">        &quot;configured. This can happen when the file that pools are read from isn&#x27;t set, or &quot; +</span><br><span class="line">        s&quot;when that file doesn&#x27;t contain $poolName. Created $poolName with default &quot; +</span><br><span class="line">        s&quot;configuration (schedulingMode: $DEFAULT_SCHEDULING_MODE, &quot; +</span><br><span class="line">        s&quot;minShare: $DEFAULT_MINIMUM_SHARE, weight: $DEFAULT_WEIGHT)&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">//  将当前的 TaskSetManager（Schedulable 对象）添加到所属子调度池 parentPool 中。</span><br><span class="line">    parentPool.addSchedulable(manager)</span><br><span class="line">    logInfo(&quot;Added task set &quot; + manager.name + &quot; tasks to pool &quot; + poolName)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用上面的addTaskSetManager函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;TaskSchedulerImpl.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks</li>
<li>submitTasks函数如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">override def submitTasks(taskSet: TaskSet): Unit = &#123;</span><br><span class="line">    val tasks = taskSet.tasks</span><br><span class="line">    logInfo(&quot;Adding task set &quot; + taskSet.id + &quot; with &quot; + tasks.length + &quot; tasks &quot;</span><br><span class="line">      + &quot;resource profile &quot; + taskSet.resourceProfileId)</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      val stage = taskSet.stageId</span><br><span class="line">      /**</span><br><span class="line">       * val stageTaskSets = Map(</span><br><span class="line">       *  1 -&gt; TaskSetManager(</span><br><span class="line">       *  taskSet = taskSet1,  // 对应任务集 taskSet1</span><br><span class="line">       *  isZombie = false     // 当前 TaskSetManager 不是僵尸状态</span><br><span class="line">       *  ),</span><br><span class="line">       *  2 -&gt; TaskSetManager(</span><br><span class="line">       *  taskSet = taskSet2,  // 对应任务集 taskSet2</span><br><span class="line">       *  isZombie = true      // 当前 TaskSetManager 被标记为僵尸</span><br><span class="line">       *  )</span><br><span class="line">       *  )</span><br><span class="line">       *  taskSet1 和 taskSet2 是两个不同的任务集，它们可能属于同一个阶段（stageId 为 1），但是由于 stageAttemptId 不同，</span><br><span class="line">       *  它们分别被分配给了不同的 TaskSetManager 实例。</span><br><span class="line">       *  isZombie 字段标记了该任务集是否处于僵尸状态。当新的任务集尝试加入时，旧的 TaskSetManager 被标记为僵尸，</span><br><span class="line">       *  表示它不再活跃，但仍保留其状态。</span><br><span class="line">       *</span><br><span class="line">       * case class TaskSetManager(</span><br><span class="line">       *  taskSet: TaskSet,        // 任务集</span><br><span class="line">       *  isZombie: Boolean        // 是否是僵尸状态</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  // 示例任务集</span><br><span class="line">       *  val taskSet1 = TaskSet(</span><br><span class="line">       *  id = 101,               // 任务集 ID</span><br><span class="line">       *  stageId = 1,            // 阶段 ID</span><br><span class="line">       *  stageAttemptId = 1,     // 阶段尝试 ID</span><br><span class="line">       *  tasks = List(task1, task2) // 任务集包含的任务</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  val taskSet2 = TaskSet(</span><br><span class="line">       *  id = 102,               // 任务集 ID</span><br><span class="line">       *  stageId = 1,            // 阶段 ID</span><br><span class="line">       *  stageAttemptId = 2,     // 阶段尝试 ID</span><br><span class="line">       *  tasks = List(task3, task4) // 任务集包含的任务</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  val task1 = Task(id = 1, data = &quot;task1_data&quot;)</span><br><span class="line">       *  val task2 = Task(id = 2, data = &quot;task2_data&quot;)</span><br><span class="line">       *  val task3 = Task(id = 3, data = &quot;task3_data&quot;)</span><br><span class="line">       *  val task4 = Task(id = 4, data = &quot;task4_data&quot;)</span><br><span class="line">       *  taskSet1 和 taskSet2 代表两个不同的任务集，它们属于同一阶段（stageId = 1），但是具有不同的 stageAttemptId（1 和 2）。</span><br><span class="line">       *  TaskSetManager 会根据这些任务集创建，并根据状态是否为“僵尸”来管理它们。</span><br><span class="line">       */</span><br><span class="line">      val stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])</span><br><span class="line"></span><br><span class="line">      // Mark all the existing TaskSetManagers of this stage as zombie, as we are adding a new one.</span><br><span class="line">      // This is necessary to handle a corner case. Let&#x27;s say a stage has 10 partitions and has 2</span><br><span class="line">      // TaskSetManagers: TSM1(zombie) and TSM2(active). TSM1 has a running task for partition 10</span><br><span class="line">      // and it completes. TSM2 finishes tasks for partition 1-9, and thinks he is still active</span><br><span class="line">      // because partition 10 is not completed yet. However, DAGScheduler gets task completion</span><br><span class="line">      // events for all the 10 partitions and thinks the stage is finished. If it&#x27;s a shuffle stage</span><br><span class="line">      // and somehow it has missing map outputs, then DAGScheduler will resubmit it and create a</span><br><span class="line">      // TSM3 for it. As a stage can&#x27;t have more than one active task set managers, we must mark</span><br><span class="line">      // TSM2 as zombie (it actually is).</span><br><span class="line">      /**</span><br><span class="line">       * 在 Spark 中，任务集（TaskSetManager）是用于管理某个阶段（stage）的任务的。当一个任务集（TaskSetManager）进入僵尸状态时，</span><br><span class="line">       * 它表示该任务集不再活跃，不能继续调度新的任务。因此，将现有的 TaskSetManager 设置为僵尸，主要是为了防止其继续参与到后续的任务调度中。</span><br><span class="line">       *</span><br><span class="line">       * 在 Spark 中，每个阶段（stage）只能有一个活动的任务集管理器（TaskSetManager）。当重新调度任务时，</span><br><span class="line">       * 新的任务集管理器会被添加到 stageTaskSets 中，并且必须标记所有现有的任务集管理器为 &quot;僵尸&quot;，防止它们继续影响任务调度。</span><br><span class="line">       * 这种操作可以避免出现多个任务集管理器同时试图调度同一个阶段的任务的情况。因为如果两个任务集管理器都试图调度同一个阶段，</span><br><span class="line">       * 可能会导致资源竞争或任务调度的混乱。</span><br><span class="line">       * 处理任务集失败的情况：当一个任务集中的任务失败并需要重新调度时，当前的 TaskSetManager 被认为已经不再有效，因此将其标记为 &quot;僵尸&quot;。</span><br><span class="line">       * 然后新的任务集管理器（manager）将接管当前阶段的任务调度。</span><br><span class="line">       * 防止错误的任务集状态：如果不标记现有的 TaskSetManager 为僵尸，可能会导致任务集在某些情况下被错误地认为仍然是活动状态，</span><br><span class="line">       * 进而继续尝试调度任务，这会破坏任务调度的一致性。</span><br><span class="line">       */</span><br><span class="line">      stageTaskSets.foreach &#123; case (_, ts) =&gt;</span><br><span class="line">        ts.isZombie = true</span><br><span class="line">      &#125;</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">//    schedulableBuilder 在初始化的时候，在initialize函数里面指定了FIFOSchedulableBuilder模式或者FairSchedulableBuilder模式</span><br><span class="line">//    调用addTaskSetManager函数是调用的类SchedulableBuilder.scala里面的FIFOSchedulableBuilder或者FairSchedulableBuilder里面的方法</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">      if (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">        starvationTimer.scheduleAtFixedRate(new TimerTask() &#123;</span><br><span class="line">          override def run(): Unit = &#123;</span><br><span class="line">            if (!hasLaunchedTask) &#123;</span><br><span class="line">              logWarning(&quot;Initial job has not accepted any resources; &quot; +</span><br><span class="line">                &quot;check your cluster UI to ensure that workers are registered &quot; +</span><br><span class="line">                &quot;and have sufficient resources&quot;)</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              this.cancel()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)</span><br><span class="line">      &#125;</span><br><span class="line">      hasReceivedTask = true</span><br><span class="line">    &#125;</span><br><span class="line">//  调用在 CoarseGrainedSchedulerBackend.scala 中实现了 reviveOffers 方法</span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用上面的submitTasks函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;DAGScheduler.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.DAGScheduler#submitMissingTasks</li>
<li>submitMissingTasks函数如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br></pre></td><td class="code"><pre><span class="line">private def submitMissingTasks(stage: Stage, jobId: Int): Unit = &#123;</span><br><span class="line">//    记录调试信息，标明当前正在处理哪个 Stage。</span><br><span class="line">    logDebug(&quot;submitMissingTasks(&quot; + stage + &quot;)&quot;)</span><br><span class="line"></span><br><span class="line">    // Before find missing partition, do the intermediate state clean work first.</span><br><span class="line">    // The operation here can make sure for the partially completed intermediate stage,</span><br><span class="line">    // `findMissingPartitions()` returns all partitions every time.</span><br><span class="line">//    如果 Stage 是一个不确定性 (indeterminate) 的 ShuffleMapStage 且尚未完成：</span><br><span class="line">//    清除之前记录的所有 Shuffle 输出。</span><br><span class="line">//    重新初始化 Shuffle 的合并状态。</span><br><span class="line">    stage match &#123;</span><br><span class="line">//    sms: ShuffleMapStage：匹配当前阶段是一个 ShuffleMapStage</span><br><span class="line">//    stage.isIndeterminate：检查该阶段是否被标记为 不确定性</span><br><span class="line">//    !sms.isAvailable：检查该阶段的输出是否还不可用（即没有完成）</span><br><span class="line">      case sms: ShuffleMapStage if stage.isIndeterminate &amp;&amp; !sms.isAvailable =&gt;</span><br><span class="line">//       mapOutputTracker：管理 Shuffle 输出状态的组件，跟踪各个分区的 Shuffle 数据存储位置</span><br><span class="line">//       unregisterAllMapAndMergeOutput：清理指定 ShuffleId 的所有输出状态，</span><br><span class="line">        //       包括映射输出（Map Outputs）和合并输出（Merged Outputs）</span><br><span class="line">        mapOutputTracker.unregisterAllMapAndMergeOutput(sms.shuffleDep.shuffleId)</span><br><span class="line">//        shuffleDep：表示 Shuffle 的依赖关系，包含该 Shuffle 操作的元数据信息</span><br><span class="line">//        newShuffleMergeState：重新初始化 Shuffle 的合并状态，用于 Push-based Shuffle 的功能</span><br><span class="line">        sms.shuffleDep.newShuffleMergeState()</span><br><span class="line">      case _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Figure out the indexes of partition ids to compute.</span><br><span class="line">//    调用 findMissingPartitions 获取当前阶段需要重新计算的分区 ID 列表。</span><br><span class="line">    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">    // Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span><br><span class="line">    // with this Stage</span><br><span class="line">//    获取与当前任务相关的配置（如调度池、作业分组等），并对 PySpark 特定的配置进行增强。</span><br><span class="line">    val properties = jobIdToActiveJob(jobId).properties</span><br><span class="line">    addPySparkConfigsToProperties(stage, properties)</span><br><span class="line"></span><br><span class="line">//        将当前 Stage 添加到正在运行的 runningStages 集合中。</span><br><span class="line">    runningStages += stage</span><br><span class="line">    // SparkListenerStageSubmitted should be posted before testing whether tasks are</span><br><span class="line">    // serializable. If tasks are not serializable, a SparkListenerStageCompleted event</span><br><span class="line">    // will be posted, which should always come after a corresponding SparkListenerStageSubmitted</span><br><span class="line">    // event.</span><br><span class="line">    stage match &#123;</span><br><span class="line">      case s: ShuffleMapStage =&gt;</span><br><span class="line">//        向OutputCommitCoordinator发出阶段开始的信号，该组件负责管理各执行器之间任务输出的提交协调。</span><br><span class="line">//        stage.id: 当前阶段的标识符</span><br><span class="line">//        maxPartitionId: 当前阶段分区的最大索引，即 s.numPartitions - 1。</span><br><span class="line">        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)</span><br><span class="line">        // Only generate merger location for a given shuffle dependency once.</span><br><span class="line">        if (s.shuffleDep.shuffleMergeAllowed) &#123;</span><br><span class="line">          if (!s.shuffleDep.isShuffleMergeFinalizedMarked) &#123;</span><br><span class="line">            prepareShuffleServicesForShuffleMapStage(s)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            // Disable Shuffle merge for the retry/reuse of the same shuffle dependency if it has</span><br><span class="line">            // already been merge finalized. If the shuffle dependency was previously assigned</span><br><span class="line">            // merger locations but the corresponding shuffle map stage did not complete</span><br><span class="line">            // successfully, we would still enable push for its retry.</span><br><span class="line">            s.shuffleDep.setShuffleMergeAllowed(false)</span><br><span class="line">            logInfo(s&quot;Push-based shuffle disabled for $stage ($&#123;stage.name&#125;) since it&quot; +</span><br><span class="line">              &quot; is already shuffle merge finalized&quot;)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      case s: ResultStage =&gt;</span><br><span class="line">        outputCommitCoordinator.stageStart(</span><br><span class="line">          stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)</span><br><span class="line">    &#125;</span><br><span class="line">//    为每个需要重新计算的分区，确定最佳的任务执行位置（TaskLocation）。如果失败，将 Stage 标记为失败，并从 runningStages 中移除。</span><br><span class="line">    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try &#123;</span><br><span class="line">      stage match &#123;</span><br><span class="line">        case s: ShuffleMapStage =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">        case s: ResultStage =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            val p = s.partitions(id)</span><br><span class="line">            (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">          &#125;.toMap</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case NonFatal(e) =&gt;</span><br><span class="line">        stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">        listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo,</span><br><span class="line">          Utils.cloneProperties(properties)))</span><br><span class="line">        abortStage(stage, s&quot;Task creation failed: $e\n$&#123;Utils.exceptionString(e)&#125;&quot;, Some(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line"></span><br><span class="line">    // If there are tasks to execute, record the submission time of the stage. Otherwise,</span><br><span class="line">    // post the even without the submission time, which indicates that this stage was</span><br><span class="line">    // skipped.</span><br><span class="line">    if (partitionsToCompute.nonEmpty) &#123;</span><br><span class="line">      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">//    发布 StageSubmitted 事件,通知监听器当前阶段已经提交</span><br><span class="line">    listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo,</span><br><span class="line">      Utils.cloneProperties(properties)))</span><br><span class="line"></span><br><span class="line">    // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.</span><br><span class="line">    // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast</span><br><span class="line">    // the serialized copy of the RDD and for each task we will deserialize it, which means each</span><br><span class="line">    // task gets a different copy of the RDD. This provides stronger isolation between tasks that</span><br><span class="line">    // might modify state of objects referenced in their closures. This is necessary in Hadoop</span><br><span class="line">    // where the JobConf/Configuration object is not thread-safe.</span><br><span class="line">//    对任务的 RDD 或函数 (stage.func) 进行序列化。</span><br><span class="line">//        生成字节数组后，使用 Spark 的 Broadcast 机制广播给所有执行器。</span><br><span class="line">    var taskBinary: Broadcast[Array[Byte]] = null</span><br><span class="line">    var partitions: Array[Partition] = null</span><br><span class="line">    try &#123;</span><br><span class="line">      // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).</span><br><span class="line">      // For ResultTask, serialize and broadcast (rdd, func).</span><br><span class="line">      var taskBinaryBytes: Array[Byte] = null</span><br><span class="line">      // taskBinaryBytes and partitions are both effected by the checkpoint status. We need</span><br><span class="line">      // this synchronization in case another concurrent job is checkpointing this RDD, so we get a</span><br><span class="line">      // consistent view of both variables.</span><br><span class="line">      RDDCheckpointData.synchronized &#123;</span><br><span class="line">        taskBinaryBytes = stage match &#123;</span><br><span class="line">          case stage: ShuffleMapStage =&gt;</span><br><span class="line">            JavaUtils.bufferToArray(</span><br><span class="line">              closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))</span><br><span class="line">          case stage: ResultStage =&gt;</span><br><span class="line">            JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        partitions = stage.rdd.partitions</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if (taskBinaryBytes.length &gt; TaskSetManager.TASK_SIZE_TO_WARN_KIB * 1024) &#123;</span><br><span class="line">        logWarning(s&quot;Broadcasting large task binary with size &quot; +</span><br><span class="line">          s&quot;$&#123;Utils.bytesToString(taskBinaryBytes.length)&#125;&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      // In the case of a failure during serialization, abort the stage.</span><br><span class="line">      case e: NotSerializableException =&gt;</span><br><span class="line">        abortStage(stage, &quot;Task not serializable: &quot; + e.toString, Some(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line"></span><br><span class="line">        // Abort execution</span><br><span class="line">        return</span><br><span class="line">      case e: Throwable =&gt;</span><br><span class="line">        abortStage(stage, s&quot;Task serialization failed: $e\n$&#123;Utils.exceptionString(e)&#125;&quot;, Some(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line"></span><br><span class="line">        // Abort execution</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line">//    根据 ShuffleMapStage 或 ResultStage 类型，分别生成 ShuffleMapTask 或 ResultTask。</span><br><span class="line">//    每个任务包括分区信息、执行位置、序列化的二进制数据等。</span><br><span class="line">    val tasks: Seq[Task[_]] = try &#123;</span><br><span class="line">//  这一行将当前 Stage 的任务度量信息（taskMetrics）序列化成字节数组。taskMetrics 记录了关于任务执行的统计信息，</span><br><span class="line">//  比如运行时间、输入输出的大小等。这些度量信息将被附加到每个任务中，并在任务执行时收集。</span><br><span class="line">      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">      stage match &#123;</span><br><span class="line">        case stage: ShuffleMapStage =&gt;</span><br><span class="line">//          清空 pendingPartitions 列表，它记录了当前阶段正在处理的分区。</span><br><span class="line">          stage.pendingPartitions.clear()</span><br><span class="line">//          对需要计算的每个分区 ID（partitionsToCompute），进行迭代并为每个分区创建任务。</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">//            获取当前分区 id 对应的执行位置（taskIdToLocations 方法会根据分区 ID 返回执行该分区的节点位置）。</span><br><span class="line">            val locs = taskIdToLocations(id)</span><br><span class="line">//            获取分区对象 part，partitions(id) 会返回 RDD 对应的分区信息。</span><br><span class="line">            val part = partitions(id)</span><br><span class="line">//            将当前正在处理的分区 ID 添加到 pendingPartitions 列表中。</span><br><span class="line">            stage.pendingPartitions += id</span><br><span class="line">            /**</span><br><span class="line">             * 创建一个新的 ShuffleMapTask 实例，代表当前阶段的一个任务。传入的参数包括：</span><br><span class="line">             * stage.id: 当前阶段的 ID</span><br><span class="line">             * stage.latestInfo.attemptNumber: 当前阶段的尝试编号</span><br><span class="line">             * taskBinary: 序列化的任务二进制数据，用于在执行器上执行任务</span><br><span class="line">             * part: 当前任务对应的分区</span><br><span class="line">             * locs: 当前任务的执行位置</span><br><span class="line">             * properties: 与任务相关的属性（如调度池、作业分组等）</span><br><span class="line">             * serializedTaskMetrics: 序列化后的任务度量信息</span><br><span class="line">             * Option(jobId): 任务所在的作业 ID</span><br><span class="line">             * Option(sc.applicationId): 当前 Spark 应用的 ID</span><br><span class="line">             * sc.applicationAttemptId: 当前应用的尝试 ID</span><br><span class="line">             * stage.rdd.isBarrier(): 判断是否为障碍性 RDD（如果是，表示该任务必须是一个屏障任务，即必须按顺序执行）。</span><br><span class="line">             */</span><br><span class="line">            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),</span><br><span class="line">              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">          &#125;</span><br><span class="line">//          这是 ResultStage 类型的处理部分，表示当前 Stage 是一个结果阶段，即产生最终结果的阶段。</span><br><span class="line">        case stage: ResultStage =&gt;</span><br><span class="line">//          对需要计算的每个分区 ID（partitionsToCompute），进行迭代并为每个分区创建任务。</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">//            获取 ResultStage 对应的分区 ID。</span><br><span class="line">            val p: Int = stage.partitions(id)</span><br><span class="line">//            获取该分区对象。</span><br><span class="line">            val part = partitions(p)</span><br><span class="line">//            获取当前分区 id 的执行位置</span><br><span class="line">            val locs = taskIdToLocations(id)</span><br><span class="line">//            创建一个新的 ResultTask 实例，代表当前阶段的一个结果任务。参数与 ShuffleMapTask 相似，但此时它对应的是 ResultStage。</span><br><span class="line">            new ResultTask(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">              taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">              stage.rdd.isBarrier())</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case NonFatal(e) =&gt;</span><br><span class="line">//        如果在任务创建过程中发生异常（如任务序列化失败），则执行以下异常处理逻辑：</span><br><span class="line">//        调用 abortStage 函数来中止当前阶段，并记录异常信息。</span><br><span class="line">        abortStage(stage, s&quot;Task creation failed: $e\n$&#123;Utils.exceptionString(e)&#125;&quot;, Some(e))</span><br><span class="line">//        从正在运行的阶段列表中移除当前阶段。</span><br><span class="line">        runningStages -= stage</span><br><span class="line">//        终止当前函数的执行，避免后续代码执行。</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line">//    如果有任务需要执行，则将它们封装为 TaskSet 并提交给 TaskScheduler。</span><br><span class="line">//    如果没有任务需要执行，直接标记 Stage 为完成，并提交后续阶段。</span><br><span class="line">    if (tasks.nonEmpty) &#123;</span><br><span class="line">      logInfo(s&quot;Submitting $&#123;tasks.size&#125; missing tasks from $stage ($&#123;stage.rdd&#125;) (first 15 &quot; +</span><br><span class="line">        s&quot;tasks are for partitions $&#123;tasks.take(15).map(_.partitionId)&#125;)&quot;)</span><br><span class="line">//      一个stage生成一个或者多个TaskSet，但是一个job可以生成多个stage</span><br><span class="line">//      将该stage转换的tasks组合为TaskSet提交给submitTasks，然后通过TaskSchedulerImpl.scala -&gt; submitTasks -&gt;</span><br><span class="line">      //      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties) 将任务提交给Pool</span><br><span class="line">//      jobId对应的TaskSet里面的priority字段</span><br><span class="line">      taskScheduler.submitTasks(new TaskSet(</span><br><span class="line">        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,</span><br><span class="line">        stage.resourceProfileId))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // Because we posted SparkListenerStageSubmitted earlier, we should mark</span><br><span class="line">      // the stage as completed here in case there are no tasks to run</span><br><span class="line">      markStageAsFinished(stage, None)</span><br><span class="line"></span><br><span class="line">      stage match &#123;</span><br><span class="line">        case stage: ShuffleMapStage =&gt;</span><br><span class="line">          logDebug(s&quot;Stage $&#123;stage&#125; is actually done; &quot; +</span><br><span class="line">              s&quot;(available: $&#123;stage.isAvailable&#125;,&quot; +</span><br><span class="line">              s&quot;available outputs: $&#123;stage.numAvailableOutputs&#125;,&quot; +</span><br><span class="line">              s&quot;partitions: $&#123;stage.numPartitions&#125;)&quot;)</span><br><span class="line">          markMapStageJobsAsFinished(stage)</span><br><span class="line">        case stage : ResultStage =&gt;</span><br><span class="line">          logDebug(s&quot;Stage $&#123;stage&#125; is actually done; (partitions: $&#123;stage.numPartitions&#125;)&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      submitWaitingChildStages(stage)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用上面的submitMissingTasks函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;DAGScheduler.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.DAGScheduler#submitStage</li>
<li>submitStage函数如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">  /** Submits stage, but first recursively submits any missing parents.</span><br><span class="line">   * submitStage 是 DAGScheduler 的核心方法之一，用于提交一个 Stage。如果 Stage 的依赖还没有满足，它会递归地提交父 Stage，</span><br><span class="line">   * 从而形成多个 Stage 的依赖关系。</span><br><span class="line">   * getMissingParentStages 方法： 通过检查当前 Stage 的父依赖，判断是否还有未完成的 Stage。对于宽依赖（ShuffleDependency），会创建一个新的 ShuffleMapStage。</span><br><span class="line">   * 1，Job 被拆分为多个 Stage 的过程</span><br><span class="line">   *  通过 getMissingParentStages 遍历 RDD 的依赖链。</span><br><span class="line">   *  对于宽依赖（ShuffleDependency），需要创建 ShuffleMapStage。</span><br><span class="line">   *  窄依赖（NarrowDependency）直接沿着依赖链向上递归。</span><br><span class="line">   * 2，多个 Stage 的递归提交：</span><br><span class="line">   *  当前 Stage 的父 Stage 未完成时，会递归调用 submitStage 提交所有依赖的父 Stage。</span><br><span class="line">   *  一旦所有父 Stage 完成，当前 Stage 会通过 submitMissingTasks 提交任务。</span><br><span class="line">   * */</span><br><span class="line">  private def submitStage(stage: Stage): Unit = &#123;</span><br><span class="line">    // 获取当前Stage的jobId</span><br><span class="line">//    获取当前 Stage 对象关联的所有 Job ID，并将这些 Job ID 从集合 stage.jobIds 转换为有序的数组（升序排序），所以获取的jobid从小到大</span><br><span class="line">    val jobId = activeJobForStage(stage)</span><br><span class="line">    if (jobId.isDefined) &#123;</span><br><span class="line">      logDebug(s&quot;submitStage($stage (name=$&#123;stage.name&#125;;&quot; +</span><br><span class="line">        s&quot;jobs=$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;))&quot;)</span><br><span class="line">//      检查当前 Stage 是否已经在以下状态之一</span><br><span class="line">//      waitingStages</span><br><span class="line">//      ：表示等待执行的 Stage 集合</span><br><span class="line">//      runningStages</span><br><span class="line">//      ：当前正在执行的 Stage 集合</span><br><span class="line">//      failedStages</span><br><span class="line">//      ：执行失败的 Stage 集合</span><br><span class="line">//      只有当 Stage 不在这些集合中时</span><br><span class="line">//      ，才会继续处理</span><br><span class="line">      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">//        调用 getMissingParentStages(stage) 获取当前 Stage 所依赖但尚未完成的父 Stage 列表。</span><br><span class="line">//        按 Stage.id 进行排序，确保以一致的顺序提交依赖。</span><br><span class="line">//        从这个入口可以通过 DAGScheduler.createResultStage()来构建 ShuffleMapStage</span><br><span class="line">        val missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">        logDebug(&quot;missing: &quot; + missing)</span><br><span class="line">        if (missing.isEmpty) &#123;</span><br><span class="line">//          如果没有缺失的依赖 Stage，说明所有父 Stage 已完成。</span><br><span class="line">//          直接调用 submitMissingTasks(stage, jobId.get) 提交当前 Stage 的任务。</span><br><span class="line">          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)</span><br><span class="line">          submitMissingTasks(stage, jobId.get)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">//          遍历每个未完成的父 Stage（missing 列表中的元素），递归调用 submitStage(parent) 提交这些父 Stage。</span><br><span class="line">//          将当前 Stage 添加到 waitingStages 集合中，标记为等待父 Stage 完成。</span><br><span class="line">          for (parent &lt;- missing) &#123;</span><br><span class="line">            submitStage(parent)</span><br><span class="line">          &#125;</span><br><span class="line">          waitingStages += stage</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用了上面的buildPools的函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;DAGScheduler.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted</li>
<li>handleJobSubmitted函数如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">private[scheduler] def handleJobSubmitted(jobId: Int,</span><br><span class="line">      finalRDD: RDD[_],</span><br><span class="line">      func: (TaskContext, Iterator[_]) =&gt; _,</span><br><span class="line">      partitions: Array[Int],</span><br><span class="line">      callSite: CallSite,</span><br><span class="line">      listener: JobListener,</span><br><span class="line">      properties: Properties): Unit = &#123;</span><br><span class="line">    var finalStage: ResultStage = null</span><br><span class="line">    try &#123;</span><br><span class="line">      // New stage creation may throw an exception if, for example, jobs are run on a</span><br><span class="line">      // HadoopRDD whose underlying HDFS files have been deleted.</span><br><span class="line">//      这一段代码的作用是创建一个表示整个 Job 最终计算阶段的 ResultStage，从而为 DAGScheduler 提交并管理这个 Job 提供核心抽象。</span><br><span class="line">      /**</span><br><span class="line">       * 为何优先创建 ResultStage？</span><br><span class="line">       * 1，ResultStage 是 Job 的最终输出阶段</span><br><span class="line">       * 每个 Spark Job 的任务是从输入数据出发，通过一系列依赖（RDD 转换）生成最终的结果（例如写入磁盘、显示结果等）。</span><br><span class="line">       * ResultStage 是 DAG 的叶子节点，代表 Job 的最终结果。创建 ResultStage 意味着为 Job 定义了目标阶段，DAGScheduler 可以根据这个阶段向上推导其所有依赖（父阶段）。</span><br><span class="line">       * 2,计算整个 DAG 的依赖关系：</span><br><span class="line">       * createResultStage 不仅创建了 ResultStage，还触发了对整个 DAG 的依赖关系分析。</span><br><span class="line">       * 它会检查 finalRDD 的所有依赖链，找到其父 ShuffleMapStage，从而明确了任务调度的顺序。</span><br><span class="line">       * 这样，DAGScheduler 可以从 ResultStage 开始向上逐层解析所有需要提交的阶段，确保依赖的父阶段（如 ShuffleMapStage）已被正确计算。</span><br><span class="line">       * 3,便于提交任务：</span><br><span class="line">       * handleJobSubmitted 的最终目的是提交 Job 对应的所有任务。创建 ResultStage 后，DAGScheduler 会调用 submitStage(finalStage)</span><br><span class="line">       * 方法，递归提交当前阶段及其所有未完成的父阶段。</span><br><span class="line">       * ResultStage 是整个任务提交的起点，其递归逻辑负责确保父阶段（如 ShuffleMapStage）优先执行。</span><br><span class="line">       */</span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: BarrierJobSlotsNumberCheckFailed =&gt;</span><br><span class="line">        // If jobId doesn&#x27;t exist in the map, Scala coverts its value null to 0: Int automatically.</span><br><span class="line">        val numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">          (_: Int, value: Int) =&gt; value + 1)</span><br><span class="line"></span><br><span class="line">        logWarning(s&quot;Barrier stage in job $jobId requires $&#123;e.requiredConcurrentTasks&#125; slots, &quot; +</span><br><span class="line">          s&quot;but only $&#123;e.maxConcurrentTasks&#125; are available. &quot; +</span><br><span class="line">          s&quot;Will retry up to $&#123;maxFailureNumTasksCheck - numCheckFailures + 1&#125; more times&quot;)</span><br><span class="line"></span><br><span class="line">        if (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">          messageScheduler.schedule(</span><br><span class="line">            new Runnable &#123;</span><br><span class="line">              override def run(): Unit = eventProcessLoop.post(JobSubmitted(jobId, finalRDD, func,</span><br><span class="line">                partitions, callSite, listener, properties))</span><br><span class="line">            &#125;,</span><br><span class="line">            timeIntervalNumTasksCheck,</span><br><span class="line">            TimeUnit.SECONDS</span><br><span class="line">          )</span><br><span class="line">          return</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          // Job failed, clear internal data.</span><br><span class="line">          barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">          listener.jobFailed(e)</span><br><span class="line">          return</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        logWarning(&quot;Creating new stage failed due to exception - job: &quot; + jobId, e)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line">    // Job submitted, clear internal data.</span><br><span class="line">    barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line"></span><br><span class="line">    val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">    clearCacheLocs()</span><br><span class="line">    logInfo(&quot;Got job %s (%s) with %d output partitions&quot;.format(</span><br><span class="line">      job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">    logInfo(&quot;Final stage: &quot; + finalStage + &quot; (&quot; + finalStage.name + &quot;)&quot;)</span><br><span class="line">    logInfo(&quot;Parents of final stage: &quot; + finalStage.parents)</span><br><span class="line">    logInfo(&quot;Missing parents: &quot; + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">    val jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">    jobIdToActiveJob(jobId) = job</span><br><span class="line">    activeJobs += job</span><br><span class="line">    finalStage.setActiveJob(job)</span><br><span class="line">    val stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">    val stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">    listenerBus.post(</span><br><span class="line">      SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos,</span><br><span class="line">        Utils.cloneProperties(properties)))</span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>确定在standalone模式下，spark的app应用在FAIR模式下，spark 3.3.0源码哪个位置在读取fairscheduler.xml配置文件<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;SchedulableBuilder.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.FairSchedulableBuilder#buildPools</li>
<li>buildPools函数如下:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">//  buildPools 负责加载调度配置文件，解析其中的 &lt;pool&gt; 节点，并根据配置构建调度池。</span><br><span class="line">  override def buildPools(): Unit = &#123;</span><br><span class="line">//    定义了一个可选值 fileData，用于存储配置文件的数据流 (InputStream) 和文件名。</span><br><span class="line">//    初始值为 None，表示未找到配置文件。</span><br><span class="line">    var fileData: Option[(InputStream, String)] = None</span><br><span class="line">    try &#123;</span><br><span class="line">      fileData = schedulerAllocFile.map &#123; f =&gt;</span><br><span class="line">//      schedulerAllocFile：这是通过 spark.scheduler.allocation.file 配置的文件路径，指定了 Fair Scheduler 的调度配置文件（通常为 fairscheduler.xml）。</span><br><span class="line">//      new Path(f)：将传入的路径字符串 f 转换为 Hadoop 的 Path 对象。</span><br><span class="line">        val filePath = new Path(f)</span><br><span class="line">//          getFileSystem 会根据路径和 Hadoop 配置（sc.hadoopConfiguration）解析路径的文件系统。</span><br><span class="line">//          如果路径以 hdfs:// 开头，则表示文件位于 HDFS 上。</span><br><span class="line">//          如果路径是本地路径（例如 /path/to/file 或 file:///path/to/file），则文件系统会解析为本地文件系统。</span><br><span class="line">        val fis = filePath.getFileSystem(sc.hadoopConfiguration).open(filePath)</span><br><span class="line">        logInfo(s&quot;Creating Fair Scheduler pools from $f&quot;)</span><br><span class="line">        Some((fis, f))</span><br><span class="line">      &#125;.getOrElse &#123;</span><br><span class="line">        val is = Utils.getSparkClassLoader.getResourceAsStream(DEFAULT_SCHEDULER_FILE)</span><br><span class="line">        if (is != null) &#123;</span><br><span class="line">          logInfo(s&quot;Creating Fair Scheduler pools from default file: $DEFAULT_SCHEDULER_FILE&quot;)</span><br><span class="line">          Some((is, DEFAULT_SCHEDULER_FILE))</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          logWarning(&quot;Fair Scheduler configuration file not found so jobs will be scheduled in &quot; +</span><br><span class="line">            s&quot;FIFO order. To use fair scheduling, configure pools in $DEFAULT_SCHEDULER_FILE or &quot; +</span><br><span class="line">            s&quot;set $&#123;SCHEDULER_ALLOCATION_FILE.key&#125; to a file that contains the configuration.&quot;)</span><br><span class="line">          None</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      fileData.foreach &#123; case (is, fileName) =&gt; buildFairSchedulerPool(is, fileName) &#125;</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case NonFatal(t) =&gt;</span><br><span class="line">        val defaultMessage = &quot;Error while building the fair scheduler pools&quot;</span><br><span class="line">        val message = fileData.map &#123; case (is, fileName) =&gt; s&quot;$defaultMessage from $fileName&quot; &#125;</span><br><span class="line">          .getOrElse(defaultMessage)</span><br><span class="line">        logError(message, t)</span><br><span class="line">        throw t</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      fileData.foreach &#123; case (is, fileName) =&gt; is.close() &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // finally create &quot;default&quot; pool</span><br><span class="line">    buildDefaultPool()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用了上面的buildPools的函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;TaskSchedulerImpl.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.TaskSchedulerImpl#initialize</li>
<li>initialize函数如下:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  def initialize(backend: SchedulerBackend): Unit = &#123;</span><br><span class="line">    this.backend = backend</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">//	  通过配置spark.scheduler.mode为FAIR还是FIFO模式</span><br><span class="line">      schedulingMode match &#123;</span><br><span class="line">        case SchedulingMode.FIFO =&gt;</span><br><span class="line">          new FIFOSchedulableBuilder(rootPool)</span><br><span class="line">        case SchedulingMode.FAIR =&gt;</span><br><span class="line">          new FairSchedulableBuilder(rootPool, sc)</span><br><span class="line">        case _ =&gt;</span><br><span class="line">          throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; +</span><br><span class="line">          s&quot;$schedulingMode&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 如果是fair模式的话，buildPools会读取配置，将配置里面的子pool添加到根pool里面。如果是FIFO，不做操作</span><br><span class="line">     */</span><br><span class="line">    schedulableBuilder.buildPools()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用了上面的initialize的函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;SparkContext.scala</li>
<li>执行函数路径为：org.apache.spark.SparkContext#createTaskScheduler</li>
<li>createTaskScheduler函数执行代码段如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">case SPARK_REGEX(sparkUrl) =&gt;</span><br><span class="line">        val scheduler = new TaskSchedulerImpl(sc)</span><br><span class="line">        val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)</span><br><span class="line">//      standalone模式下执行</span><br><span class="line">        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">//      调用上面的initialize函数</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用了上面的createTaskScheduler函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;SparkContext.scala</li>
<li>对SparkContext.scala进行实例化的时候，直接会执行下面代码<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// We need to register &quot;HeartbeatReceiver&quot; before &quot;createTaskScheduler&quot; because Executor will</span><br><span class="line">    // retrieve &quot;HeartbeatReceiver&quot; in the constructor. (SPARK-6640)</span><br><span class="line">    _heartbeatReceiver = env.rpcEnv.setupEndpoint(</span><br><span class="line">      HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))</span><br><span class="line"></span><br><span class="line">    // Initialize any plugins before the task scheduler is initialized.</span><br><span class="line">    _plugins = PluginContainer(this, _resources.asJava)</span><br><span class="line">//    val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)</span><br><span class="line">//    val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)</span><br><span class="line">//    (backend, scheduler)</span><br><span class="line">    // Create and start the scheduler 开始调用createTaskScheduler执行</span><br><span class="line">    val (sched, ts) = SparkContext.createTaskScheduler(this, master)</span><br><span class="line">    _schedulerBackend = sched</span><br><span class="line">    _taskScheduler = ts</span><br><span class="line">//    通过DAGScheduler将job分解为stage，然后分解为taskset</span><br><span class="line">    _dagScheduler = new DAGScheduler(this)</span><br><span class="line">    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置开始对SparkContext类进行实例化<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;SparkContext.scala</li>
<li>执行函数路径为：org.apache.spark.SparkContext#getOrCreate</li>
<li>getOrCreate函数执行代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def getOrCreate(config: SparkConf): SparkContext = &#123;</span><br><span class="line">    // Synchronize to ensure that multiple create requests don&#x27;t trigger an exception</span><br><span class="line">    // from assertNoOtherContextIsRunning within setActiveContext</span><br><span class="line">//		使用 SPARK_CONTEXT_CONSTRUCTOR_LOCK 对象作为同步锁，确保在多线程环境下只有一个线程可以执行 SparkContext 的创建逻辑。</span><br><span class="line">    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized &#123;</span><br><span class="line">//			如果没有活跃的 SparkContext，则创建一个新的 SparkContext 实例，并将其设置为活跃的 SparkContext。</span><br><span class="line">      if (activeContext.get() == null) &#123;</span><br><span class="line">//				调用 SparkContext 的构造函数，传入 SparkConf 对象 config，创建一个新的 SparkContext 实例。</span><br><span class="line">//				在 SparkContext 的构造函数中，会初始化任务调度器、DAG 调度器、存储系统等核心组件。</span><br><span class="line">//				将新创建的 SparkContext 实例设置为全局活跃的 SparkContext。</span><br><span class="line">//				源码位置: org.apache.spark.SparkContext#setActiveContext。</span><br><span class="line">        setActiveContext(new SparkContext(config))</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        if (config.getAll.nonEmpty) &#123;</span><br><span class="line">          logWarning(&quot;Using an existing SparkContext; some configuration may not take effect.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      activeContext.get()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用函数getOrCreate<ul>
<li>代码文件路径为：sql&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;sql&#x2F;SparkSession.scala</li>
<li>执行函数路径为：org.apache.spark.sql.SparkSession.Builder#getOrCreate</li>
<li>getOrCreate函数执行代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new</span><br><span class="line">     * one based on the options set in this builder.</span><br><span class="line">     *</span><br><span class="line">     * This method first checks whether there is a valid thread-local SparkSession,</span><br><span class="line">     * and if yes, return that one. It then checks whether there is a valid global</span><br><span class="line">     * default SparkSession, and if yes, return that one. If no valid global default</span><br><span class="line">     * SparkSession exists, the method creates a new SparkSession and assigns the</span><br><span class="line">     * newly created SparkSession as the global default.</span><br><span class="line">     *</span><br><span class="line">     * In case an existing SparkSession is returned, the non-static config options specified in</span><br><span class="line">     * this builder will be applied to the existing SparkSession.</span><br><span class="line">     *</span><br><span class="line">     * @since 2.0.0</span><br><span class="line">     */</span><br><span class="line">    def getOrCreate(): SparkSession = synchronized &#123;</span><br><span class="line">      val sparkConf = new SparkConf()</span><br><span class="line">      options.foreach &#123; case (k, v) =&gt; sparkConf.set(k, v) &#125;</span><br><span class="line"></span><br><span class="line">      if (!sparkConf.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) &#123;</span><br><span class="line">        assertOnDriver()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Get the session from current thread&#x27;s active session.</span><br><span class="line">      var session = activeThreadSession.get()</span><br><span class="line">      if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">        applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava))</span><br><span class="line">        return session</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Global synchronization so we will only set the default session once.</span><br><span class="line">      SparkSession.synchronized &#123;</span><br><span class="line">        // If the current thread does not have an active session, get it from the global session.</span><br><span class="line">        session = defaultSession.get()</span><br><span class="line">        if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">          applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava))</span><br><span class="line">          return session</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // No active nor global default session. Create a new one.</span><br><span class="line">//		使用getOrElse，先查看全局是否存在sparkContext，如果不存在就直接调用SparkContext.getOrCreate(sparkConf)重新创建</span><br><span class="line">        val sparkContext = userSuppliedContext.getOrElse &#123;</span><br><span class="line">          // set a random app name if not given.</span><br><span class="line">          if (!sparkConf.contains(&quot;spark.app.name&quot;)) &#123;</span><br><span class="line">            sparkConf.setAppName(java.util.UUID.randomUUID().toString)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          SparkContext.getOrCreate(sparkConf)</span><br><span class="line">          // Do not update `SparkConf` for existing `SparkContext`, as it&#x27;s shared by all sessions.</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        loadExtensions(extensions)</span><br><span class="line">        applyExtensions(</span><br><span class="line">          sparkContext.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty),</span><br><span class="line">          extensions)</span><br><span class="line"></span><br><span class="line">        session = new SparkSession(sparkContext, None, None, extensions, options.toMap)</span><br><span class="line">        setDefaultSession(session)</span><br><span class="line">        setActiveSession(session)</span><br><span class="line">        registerContextListener(sparkContext)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      return session</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>如何调用上面的SparkSession里面的getOrCreate函数<ul>
<li>就是在创建spark application的时候，用户进行初始化的时候进行手动调用</li>
<li>代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.builder</span><br><span class="line">     .master(&quot;local&quot;)</span><br><span class="line">     .appName(&quot;Word Count&quot;)</span><br><span class="line">     .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" data-id="cm6vrnau80001y0mff3qzfk3y" data-title="spark在standalone模式下，FIFO和FAIR调度模式的对象分析" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/spark-3-3-0/" rel="tag">spark 3.3.0</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/standalone/" rel="tag">standalone</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F/" rel="tag">调度模式</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/yunshenBlog.github.io/2025/02/07/hello-world/" class="article-date">
  <time class="dt-published" datetime="2025-02-07T13:28:03.106Z" itemprop="datePublished">2025-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/yunshenBlog.github.io/2025/02/07/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/07/hello-world/" data-id="cm6vrnau40000y0mfhg5e73rz" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/spark-3-3-0/" rel="tag">spark 3.3.0</a></li><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/standalone/" rel="tag">standalone</a></li><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></li><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F/" rel="tag">调度模式</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/yunshenBlog.github.io/tags/spark-3-3-0/" style="font-size: 10px;">spark 3.3.0</a> <a href="/yunshenBlog.github.io/tags/standalone/" style="font-size: 10px;">standalone</a> <a href="/yunshenBlog.github.io/tags/%E6%8A%80%E6%9C%AF/" style="font-size: 10px;">技术</a> <a href="/yunshenBlog.github.io/tags/%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">调度模式</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/yunshenBlog.github.io/archives/2025/02/">February 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</a>
          </li>
        
          <li>
            <a href="/yunshenBlog.github.io/2025/02/07/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/yunshenBlog.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/yunshenBlog.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/yunshenBlog.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/yunshenBlog.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/yunshenBlog.github.io/js/script.js"></script>





  </div>
</body>
</html>