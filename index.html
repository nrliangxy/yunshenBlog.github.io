<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>只在此山中，云深不知处</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="只在此山中，云深不知处">
<meta property="og:url" content="https://nrliangxy.github.io/yunshenBlog.github.io/index.html">
<meta property="og:site_name" content="只在此山中，云深不知处">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/yunshenBlog.github.io/atom.xml" title="只在此山中，云深不知处" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/yunshenBlog.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/yunshenBlog.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/yunshenBlog.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/yunshenBlog.github.io/" id="logo">只在此山中，云深不知处</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/yunshenBlog.github.io/">Home</a>
        
          <a class="main-nav-link" href="/yunshenBlog.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/yunshenBlog.github.io/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nrliangxy.github.io/yunshenBlog.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-我的第一篇博客" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" class="article-date">
  <time class="dt-published" datetime="2025-02-07T23:57:10.000Z" itemprop="datePublished">2025-02-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>spark集群在standalone模式下，调度模式FIFO和FAIR针对的调度对象是单个应用内的所有的Schedulable实例，<br>不会对spark集群里面不同的应用app产生调度作用</li>
<li>spark集群在standalone模式下，不同应用app的调度默认为FIFO模式调度，并且<code>不支持修改</code></li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>组内spark集群为standalone模式，有几个关键性任务需要优先给他们分配资源，让它们优先运行。我们通过网上搜索相关资料<br>和询问相关大模型，获得可以通过修改spark的调度模式来调整不用应用的调度优先级方法。</p>
<h2 id="查询资料"><a href="#查询资料" class="headerlink" title="查询资料"></a><em>查询资料</em></h2><h3 id="资料链接"><a href="#资料链接" class="headerlink" title="资料链接"></a>资料链接</h3><ul>
<li><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_16213300/10500039">spark调度模式分析</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cuishuai/p/7910348.html">spark调度FAIR策略</a></li>
<li>大模型搜索：<code>spark集群standalone模式下，fair模式的详细配置</code>或者<code>spark集群standalone模式下，如何修改调度模式</code></li>
</ul>
<h3 id="上面资料的错误结论"><a href="#上面资料的错误结论" class="headerlink" title="上面资料的错误结论"></a>上面资料的<em>错误结论</em></h3><ul>
<li>资料里面都是指出spark集群有两种调度模式，分别是FIFO和FAIR，集群默认的调度方式是FIFO。</li>
<li>可以通过调整spark集群配置，将配置改为FAIR模式，添加fairscheduler.xml配置文件，<br>来实现集群里面不同应用之间的优先级调度。</li>
</ul>
<h3 id="实验测试"><a href="#实验测试" class="headerlink" title="实验测试"></a><em>实验测试</em></h3><h4 id="1，实验结论"><a href="#1，实验结论" class="headerlink" title="1，实验结论"></a>1，实验结论</h4><ul>
<li>经过多次测试后，发现每次同时提交多个任务，并将优先级高的任务提交到权重高的子pool里面。这些任务获得资源的<br>策略依然是：<ul>
<li>FIFO先到先得策略，先提交的任务会优先获得资源</li>
<li>集群会默认使用资源平均分配原则对每个任务进行资源分配</li>
</ul>
</li>
<li>优先级高的任务并没有优先获得资源</li>
</ul>
<h4 id="2，原因分析"><a href="#2，原因分析" class="headerlink" title="2，原因分析"></a>2，原因分析</h4><h5 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h5><ul>
<li>在standalone模式下，使用FAIR或者FIFO进行排序的核心函数是<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;Pool.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.Pool#getSortedTaskSetQueue</li>
<li>以下为getSortedTaskSetQueue详细代码<ul>
<li>其中schedulableQueue队列存储的是需要进行排序的Schedulable，taskSetSchedulingAlgorithm.comparator为排序算法<br>分别为FAIR和FIFO</li>
<li>现在可以通过确认schedulableQueue队列存储的是整个集群里面应用的Schedulable，还是只存储单个应用里面的Schedulable。<br>如果存储的是整个集群里面应用的Schedulable对象，那就能证明fair调度模式可以改变spark集群内不同应用之间的任务调度顺序，<br>如果只是存储的单个应用里面的Schedulable对象，那就说明fair调度模式不能改变spark集群内不同应用之间的任务调度顺序</li>
<li>通过上面的分析，现在只需要确定schedulableQueue队列存储是什么对象就能确定答案了<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">override def getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] = &#123;</span><br><span class="line">//    这行代码创建了一个空的 ArrayBuffer，用于存储排序后的 TaskSetManager 对象。这将是返回的最终结果。</span><br><span class="line">    val sortedTaskSetQueue = new ArrayBuffer[TaskSetManager]</span><br><span class="line">//    collection.sortWith((x, y) =&gt; comparison)</span><br><span class="line">//    x 和 y：集合中的两个元素。</span><br><span class="line">//    comparison：一个布尔表达式，表示排序规则。如果 comparison 返回 true，则 x 会排在 y 的前面。</span><br><span class="line">    /**</span><br><span class="line">     * TaskSchedulerImpl.scala submitTasks 调用schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line">     * schedulableBuilder是从initialize函数里面指定了FIFOSchedulableBuilder模式或者FairSchedulableBuilder模式，调用这两个类里面的</span><br><span class="line">     * addTaskSetManager，然后调用Pool.scala 里面的addSchedulable函数</span><br><span class="line">     * schedulableQueue 是用来管理单个应用内的任务调度的。它存储的是该应用的所有 Schedulable，包括 TaskSetManager（代表具体的 TaskSet）和可能的子 Pool。</span><br><span class="line">     */</span><br><span class="line">    val sortedSchedulableQueue =</span><br><span class="line">      schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)</span><br><span class="line">//    这行代码开始遍历排序后的 schedulableQueue（现在是 sortedSchedulableQueue）。每个 schedulable 都是一个 Schedulable 对象，</span><br><span class="line">//    它可能是一个任务集（TaskSetManager）的父类或者接口。</span><br><span class="line">    for (schedulable &lt;- sortedSchedulableQueue) &#123;</span><br><span class="line">      sortedTaskSetQueue ++= schedulable.getSortedTaskSetQueue.filter(_.isSchedulable)</span><br><span class="line">    &#125;</span><br><span class="line">    sortedTaskSetQueue</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>上面的函数getSortedTaskSetQueue里面，变量schedulableQueue为val schedulableQueue &#x3D; new ConcurrentLinkedQueue[Schedulable]<ul>
<li>该变量的元素是通过什么函数进行添加的？</li>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;Pool.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.Pool#addSchedulable</li>
<li>addSchedulable详细代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * 通过SchedulableBuilder.scala 的addTaskSetManager方法将FIFOSchedulableBuilder.scala对应的Schedulable或者FairSchedulableBuilder.scala</span><br><span class="line">   * 对应的parentPool添加到schedulableQueue队列里面</span><br><span class="line">   * @param schedulable</span><br><span class="line">   */</span><br><span class="line">  override def addSchedulable(schedulable: Schedulable): Unit = &#123;</span><br><span class="line">    require(schedulable != null)</span><br><span class="line">    schedulableQueue.add(schedulable)</span><br><span class="line">    schedulableNameToSchedulable.put(schedulable.name, schedulable)</span><br><span class="line">    schedulable.parent = this</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个函数在调用addSchedulable函数来为schedulableQueue队列添加元素<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;SchedulableBuilder.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.FairSchedulableBuilder#addTaskSetManager<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * 运行示例和工作机制</span><br><span class="line">   *  假设有以下调度场景：</span><br><span class="line">   *</span><br><span class="line">   *  用户提交了一个任务，指定了 FAIR_SCHEDULER_PROPERTIES=highPriority。</span><br><span class="line">   *</span><br><span class="line">   *  系统在 rootPool 中查找是否已有名称为 highPriority 的调度池：</span><br><span class="line">   *</span><br><span class="line">   *  如果存在，将任务添加到 highPriority 池中。</span><br><span class="line">   *  如果不存在，系统创建 highPriority 池并将任务添加到其中。</span><br><span class="line">   *  如果用户未指定 FAIR_SCHEDULER_PROPERTIES，任务将被分配到默认池 DEFAULT_POOL_NAME 中。</span><br><span class="line">   *</span><br><span class="line">   *  通过这种方式，Spark 实现了任务与调度池之间的动态分配和管理。</span><br><span class="line">   *</span><br><span class="line">   * @param manager</span><br><span class="line">   * @param properties</span><br><span class="line">   */</span><br><span class="line">  override def addTaskSetManager(manager: Schedulable, properties: Properties): Unit = &#123;</span><br><span class="line">//    检查 properties 是否为空。</span><br><span class="line">//    properties 是任务的属性对象，可能包含用户指定的调度池名称。</span><br><span class="line">//    如果 properties 不为空，从中获取 FAIR_SCHEDULER_PROPERTIES（即调度池名称）的值。</span><br><span class="line">//    如果未指定，则使用默认池名 DEFAULT_POOL_NAME。</span><br><span class="line">//    如果 properties 为空，直接使用默认池名。</span><br><span class="line">//    目的：确定当前任务所属的调度池名称。</span><br><span class="line">    val poolName = if (properties != null) &#123;</span><br><span class="line">        properties.getProperty(FAIR_SCHEDULER_PROPERTIES, DEFAULT_POOL_NAME)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        DEFAULT_POOL_NAME</span><br><span class="line">      &#125;</span><br><span class="line">//    从 rootPool 中根据 poolName 查找对应的调度池。</span><br><span class="line">//    rootPool 是调度池的根对象，存储了所有子调度池。</span><br><span class="line">    var parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">//    如果未找到对应的调度池（即 parentPool == null）：</span><br><span class="line">//    1，创建一个新的调度池 Pool。</span><br><span class="line">//    2，使用默认配置初始化新调度池：</span><br><span class="line">//    调度模式：DEFAULT_SCHEDULING_MODE。</span><br><span class="line">//    最小共享资源：DEFAULT_MINIMUM_SHARE。</span><br><span class="line">//    权重：DEFAULT_WEIGHT。</span><br><span class="line">//    目的：确保所有任务都能分配到某个调度池，即使该池未事先配置。</span><br><span class="line">    if (parentPool == null) &#123;</span><br><span class="line">      // we will create a new pool that user has configured in app</span><br><span class="line">      // instead of being defined in xml file</span><br><span class="line">      parentPool = new Pool(poolName, DEFAULT_SCHEDULING_MODE,</span><br><span class="line">        DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)</span><br><span class="line">//    将新创建的调度池添加到 rootPool。</span><br><span class="line">      rootPool.addSchedulable(parentPool)</span><br><span class="line">      logWarning(s&quot;A job was submitted with scheduler pool $poolName, which has not been &quot; +</span><br><span class="line">        &quot;configured. This can happen when the file that pools are read from isn&#x27;t set, or &quot; +</span><br><span class="line">        s&quot;when that file doesn&#x27;t contain $poolName. Created $poolName with default &quot; +</span><br><span class="line">        s&quot;configuration (schedulingMode: $DEFAULT_SCHEDULING_MODE, &quot; +</span><br><span class="line">        s&quot;minShare: $DEFAULT_MINIMUM_SHARE, weight: $DEFAULT_WEIGHT)&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">//  将当前的 TaskSetManager（Schedulable 对象）添加到所属子调度池 parentPool 中。</span><br><span class="line">    parentPool.addSchedulable(manager)</span><br><span class="line">    logInfo(&quot;Added task set &quot; + manager.name + &quot; tasks to pool &quot; + poolName)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用上面的addTaskSetManager函数<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;TaskSchedulerImpl.scala</li>
<li>执行函数路径为：org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks</li>
<li>截至到submitTasks函数，单个spark app应用提交后，DAGScheduler会将一个job分解为多个stage，一个stage会对应唯一的一个taskSet，<br>该taskSet存储的是该stage对应的所有task</li>
<li>下面的submitTasks函数通过val manager &#x3D; createTaskSetManager(taskSet, maxTaskFailures)生成Schedulable对象，<br>然后使用初始化后的schedulableBuilder对象，调用addTaskSetManager函数，将Schedulable对象添加到schedulableQueue队列里面</li>
<li>submitTasks函数如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">override def submitTasks(taskSet: TaskSet): Unit = &#123;</span><br><span class="line">    val tasks = taskSet.tasks</span><br><span class="line">    logInfo(&quot;Adding task set &quot; + taskSet.id + &quot; with &quot; + tasks.length + &quot; tasks &quot;</span><br><span class="line">      + &quot;resource profile &quot; + taskSet.resourceProfileId)</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      val stage = taskSet.stageId</span><br><span class="line">      /**</span><br><span class="line">       * val stageTaskSets = Map(</span><br><span class="line">       *  1 -&gt; TaskSetManager(</span><br><span class="line">       *  taskSet = taskSet1,  // 对应任务集 taskSet1</span><br><span class="line">       *  isZombie = false     // 当前 TaskSetManager 不是僵尸状态</span><br><span class="line">       *  ),</span><br><span class="line">       *  2 -&gt; TaskSetManager(</span><br><span class="line">       *  taskSet = taskSet2,  // 对应任务集 taskSet2</span><br><span class="line">       *  isZombie = true      // 当前 TaskSetManager 被标记为僵尸</span><br><span class="line">       *  )</span><br><span class="line">       *  )</span><br><span class="line">       *  taskSet1 和 taskSet2 是两个不同的任务集，它们可能属于同一个阶段（stageId 为 1），但是由于 stageAttemptId 不同，</span><br><span class="line">       *  它们分别被分配给了不同的 TaskSetManager 实例。</span><br><span class="line">       *  isZombie 字段标记了该任务集是否处于僵尸状态。当新的任务集尝试加入时，旧的 TaskSetManager 被标记为僵尸，</span><br><span class="line">       *  表示它不再活跃，但仍保留其状态。</span><br><span class="line">       *</span><br><span class="line">       * case class TaskSetManager(</span><br><span class="line">       *  taskSet: TaskSet,        // 任务集</span><br><span class="line">       *  isZombie: Boolean        // 是否是僵尸状态</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  // 示例任务集</span><br><span class="line">       *  val taskSet1 = TaskSet(</span><br><span class="line">       *  id = 101,               // 任务集 ID</span><br><span class="line">       *  stageId = 1,            // 阶段 ID</span><br><span class="line">       *  stageAttemptId = 1,     // 阶段尝试 ID</span><br><span class="line">       *  tasks = List(task1, task2) // 任务集包含的任务</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  val taskSet2 = TaskSet(</span><br><span class="line">       *  id = 102,               // 任务集 ID</span><br><span class="line">       *  stageId = 1,            // 阶段 ID</span><br><span class="line">       *  stageAttemptId = 2,     // 阶段尝试 ID</span><br><span class="line">       *  tasks = List(task3, task4) // 任务集包含的任务</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  val task1 = Task(id = 1, data = &quot;task1_data&quot;)</span><br><span class="line">       *  val task2 = Task(id = 2, data = &quot;task2_data&quot;)</span><br><span class="line">       *  val task3 = Task(id = 3, data = &quot;task3_data&quot;)</span><br><span class="line">       *  val task4 = Task(id = 4, data = &quot;task4_data&quot;)</span><br><span class="line">       *  taskSet1 和 taskSet2 代表两个不同的任务集，它们属于同一阶段（stageId = 1），但是具有不同的 stageAttemptId（1 和 2）。</span><br><span class="line">       *  TaskSetManager 会根据这些任务集创建，并根据状态是否为“僵尸”来管理它们。</span><br><span class="line">       */</span><br><span class="line">      val stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])</span><br><span class="line"></span><br><span class="line">      // Mark all the existing TaskSetManagers of this stage as zombie, as we are adding a new one.</span><br><span class="line">      // This is necessary to handle a corner case. Let&#x27;s say a stage has 10 partitions and has 2</span><br><span class="line">      // TaskSetManagers: TSM1(zombie) and TSM2(active). TSM1 has a running task for partition 10</span><br><span class="line">      // and it completes. TSM2 finishes tasks for partition 1-9, and thinks he is still active</span><br><span class="line">      // because partition 10 is not completed yet. However, DAGScheduler gets task completion</span><br><span class="line">      // events for all the 10 partitions and thinks the stage is finished. If it&#x27;s a shuffle stage</span><br><span class="line">      // and somehow it has missing map outputs, then DAGScheduler will resubmit it and create a</span><br><span class="line">      // TSM3 for it. As a stage can&#x27;t have more than one active task set managers, we must mark</span><br><span class="line">      // TSM2 as zombie (it actually is).</span><br><span class="line">      /**</span><br><span class="line">       * 在 Spark 中，任务集（TaskSetManager）是用于管理某个阶段（stage）的任务的。当一个任务集（TaskSetManager）进入僵尸状态时，</span><br><span class="line">       * 它表示该任务集不再活跃，不能继续调度新的任务。因此，将现有的 TaskSetManager 设置为僵尸，主要是为了防止其继续参与到后续的任务调度中。</span><br><span class="line">       *</span><br><span class="line">       * 在 Spark 中，每个阶段（stage）只能有一个活动的任务集管理器（TaskSetManager）。当重新调度任务时，</span><br><span class="line">       * 新的任务集管理器会被添加到 stageTaskSets 中，并且必须标记所有现有的任务集管理器为 &quot;僵尸&quot;，防止它们继续影响任务调度。</span><br><span class="line">       * 这种操作可以避免出现多个任务集管理器同时试图调度同一个阶段的任务的情况。因为如果两个任务集管理器都试图调度同一个阶段，</span><br><span class="line">       * 可能会导致资源竞争或任务调度的混乱。</span><br><span class="line">       * 处理任务集失败的情况：当一个任务集中的任务失败并需要重新调度时，当前的 TaskSetManager 被认为已经不再有效，因此将其标记为 &quot;僵尸&quot;。</span><br><span class="line">       * 然后新的任务集管理器（manager）将接管当前阶段的任务调度。</span><br><span class="line">       * 防止错误的任务集状态：如果不标记现有的 TaskSetManager 为僵尸，可能会导致任务集在某些情况下被错误地认为仍然是活动状态，</span><br><span class="line">       * 进而继续尝试调度任务，这会破坏任务调度的一致性。</span><br><span class="line">       */</span><br><span class="line">      stageTaskSets.foreach &#123; case (_, ts) =&gt;</span><br><span class="line">        ts.isZombie = true</span><br><span class="line">      &#125;</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">//    schedulableBuilder 在初始化的时候，在initialize函数里面指定了FIFOSchedulableBuilder模式或者FairSchedulableBuilder模式</span><br><span class="line">//    调用addTaskSetManager函数是调用的类SchedulableBuilder.scala里面的FIFOSchedulableBuilder或者FairSchedulableBuilder里面的方法</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">      if (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">        starvationTimer.scheduleAtFixedRate(new TimerTask() &#123;</span><br><span class="line">          override def run(): Unit = &#123;</span><br><span class="line">            if (!hasLaunchedTask) &#123;</span><br><span class="line">              logWarning(&quot;Initial job has not accepted any resources; &quot; +</span><br><span class="line">                &quot;check your cluster UI to ensure that workers are registered &quot; +</span><br><span class="line">                &quot;and have sufficient resources&quot;)</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              this.cancel()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)</span><br><span class="line">      &#125;</span><br><span class="line">      hasReceivedTask = true</span><br><span class="line">    &#125;</span><br><span class="line">//  调用在 CoarseGrainedSchedulerBackend.scala 中实现了 reviveOffers 方法</span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置开始对DAGScheduler类进行初始化，将job分解为多个stage<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;SparkContext.scala</li>
<li>对SparkContext.scala进行实例化的时候，直接会执行下面代码<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">// We need to register &quot;HeartbeatReceiver&quot; before &quot;createTaskScheduler&quot; because Executor will</span><br><span class="line">    // retrieve &quot;HeartbeatReceiver&quot; in the constructor. (SPARK-6640)</span><br><span class="line">    _heartbeatReceiver = env.rpcEnv.setupEndpoint(</span><br><span class="line">      HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))</span><br><span class="line"></span><br><span class="line">    // Initialize any plugins before the task scheduler is initialized.</span><br><span class="line">    _plugins = PluginContainer(this, _resources.asJava)</span><br><span class="line">//    val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)</span><br><span class="line">//    val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)</span><br><span class="line">//    (backend, scheduler)</span><br><span class="line">    // Create and start the scheduler 开始调用createTaskScheduler执行</span><br><span class="line">    val (sched, ts) = SparkContext.createTaskScheduler(this, master)</span><br><span class="line">    _schedulerBackend = sched</span><br><span class="line">    _taskScheduler = ts</span><br><span class="line">//    通过DAGScheduler将job分解为stage，然后分解为taskset</span><br><span class="line">    _dagScheduler = new DAGScheduler(this)</span><br><span class="line">    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)</span><br><span class="line">        val _executorMetricsSource =</span><br><span class="line">      if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) &#123;</span><br><span class="line">        Some(new ExecutorMetricsSource)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        None</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    // create and start the heartbeater for collecting memory metrics</span><br><span class="line">    _heartbeater = new Heartbeater(</span><br><span class="line">      () =&gt; SparkContext.this.reportHeartBeat(_executorMetricsSource),</span><br><span class="line">      &quot;driver-heartbeater&quot;,</span><br><span class="line">      conf.get(EXECUTOR_HEARTBEAT_INTERVAL))</span><br><span class="line">    _heartbeater.start()</span><br><span class="line"></span><br><span class="line">    // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&#x27;s</span><br><span class="line">    // constructor</span><br><span class="line">    _taskScheduler.start()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>上面的代码中val (sched, ts) &#x3D; SparkContext.createTaskScheduler(this, master)会直接调用createTaskScheduler函数<ul>
<li>createTaskSchedule函数会直接执行initialize函数，initialize函数会根据提交的调度模式来进行初始化操作，<br><em>创建Pool对象和空的schedulableQueue队列</em></li>
<li>createTaskSchedule函数代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;SparkContext.scala</li>
<li>createTaskSchedule函数执行路径为：org.apache.spark.SparkContext.createTaskScheduler</li>
<li>initialize函数代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;TaskSchedulerImpl.scala</li>
<li>initialize函数执行路径为：org.apache.spark.scheduler.TaskSchedulerImpl.initialize<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">case SPARK_REGEX(sparkUrl) =&gt;</span><br><span class="line">        val scheduler = new TaskSchedulerImpl(sc)</span><br><span class="line">        val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)</span><br><span class="line">//      standalone模式下执行</span><br><span class="line">        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">//      调用上面的initialize函数</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def initialize(backend: SchedulerBackend): Unit = &#123;</span><br><span class="line">  this.backend = backend</span><br><span class="line">  schedulableBuilder = &#123;</span><br><span class="line">    schedulingMode match &#123;</span><br><span class="line">      case SchedulingMode.FIFO =&gt;</span><br><span class="line">        new FIFOSchedulableBuilder(rootPool)</span><br><span class="line">      case SchedulingMode.FAIR =&gt;</span><br><span class="line">        new FairSchedulableBuilder(rootPool, sc)</span><br><span class="line">      case _ =&gt;</span><br><span class="line">        throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; +</span><br><span class="line">        s&quot;$schedulingMode&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * 如果是fair模式的话，buildPools会读取配置，将配置里面的子pool添加到根pool里面。如果是FIFO，不做操作</span><br><span class="line">   */</span><br><span class="line">  schedulableBuilder.buildPools()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置开始对SparkContext类进行实例化<ul>
<li>代码文件路径为：core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;SparkContext.scala</li>
<li>执行函数路径为：org.apache.spark.SparkContext#getOrCreate</li>
<li>getOrCreate函数执行代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def getOrCreate(config: SparkConf): SparkContext = &#123;</span><br><span class="line">    // Synchronize to ensure that multiple create requests don&#x27;t trigger an exception</span><br><span class="line">    // from assertNoOtherContextIsRunning within setActiveContext</span><br><span class="line">//		使用 SPARK_CONTEXT_CONSTRUCTOR_LOCK 对象作为同步锁，确保在多线程环境下只有一个线程可以执行 SparkContext 的创建逻辑。</span><br><span class="line">    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized &#123;</span><br><span class="line">//			如果没有活跃的 SparkContext，则创建一个新的 SparkContext 实例，并将其设置为活跃的 SparkContext。</span><br><span class="line">      if (activeContext.get() == null) &#123;</span><br><span class="line">//				调用 SparkContext 的构造函数，传入 SparkConf 对象 config，创建一个新的 SparkContext 实例。</span><br><span class="line">//				在 SparkContext 的构造函数中，会初始化任务调度器、DAG 调度器、存储系统等核心组件。</span><br><span class="line">//				将新创建的 SparkContext 实例设置为全局活跃的 SparkContext。</span><br><span class="line">//				源码位置: org.apache.spark.SparkContext#setActiveContext。</span><br><span class="line">        setActiveContext(new SparkContext(config))</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        if (config.getAll.nonEmpty) &#123;</span><br><span class="line">          logWarning(&quot;Using an existing SparkContext; some configuration may not take effect.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      activeContext.get()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>哪个位置调用函数getOrCreate<ul>
<li>代码文件路径为：sql&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;sql&#x2F;SparkSession.scala</li>
<li>执行函数路径为：org.apache.spark.sql.SparkSession.Builder#getOrCreate</li>
<li>getOrCreate函数执行代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new</span><br><span class="line">     * one based on the options set in this builder.</span><br><span class="line">     *</span><br><span class="line">     * This method first checks whether there is a valid thread-local SparkSession,</span><br><span class="line">     * and if yes, return that one. It then checks whether there is a valid global</span><br><span class="line">     * default SparkSession, and if yes, return that one. If no valid global default</span><br><span class="line">     * SparkSession exists, the method creates a new SparkSession and assigns the</span><br><span class="line">     * newly created SparkSession as the global default.</span><br><span class="line">     *</span><br><span class="line">     * In case an existing SparkSession is returned, the non-static config options specified in</span><br><span class="line">     * this builder will be applied to the existing SparkSession.</span><br><span class="line">     *</span><br><span class="line">     * @since 2.0.0</span><br><span class="line">     */</span><br><span class="line">    def getOrCreate(): SparkSession = synchronized &#123;</span><br><span class="line">      val sparkConf = new SparkConf()</span><br><span class="line">      options.foreach &#123; case (k, v) =&gt; sparkConf.set(k, v) &#125;</span><br><span class="line"></span><br><span class="line">      if (!sparkConf.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) &#123;</span><br><span class="line">        assertOnDriver()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Get the session from current thread&#x27;s active session.</span><br><span class="line">      var session = activeThreadSession.get()</span><br><span class="line">      if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">        applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava))</span><br><span class="line">        return session</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Global synchronization so we will only set the default session once.</span><br><span class="line">      SparkSession.synchronized &#123;</span><br><span class="line">        // If the current thread does not have an active session, get it from the global session.</span><br><span class="line">        session = defaultSession.get()</span><br><span class="line">        if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">          applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava))</span><br><span class="line">          return session</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // No active nor global default session. Create a new one.</span><br><span class="line">//		使用getOrElse，先查看全局是否存在sparkContext，如果不存在就直接调用SparkContext.getOrCreate(sparkConf)重新创建</span><br><span class="line">        val sparkContext = userSuppliedContext.getOrElse &#123;</span><br><span class="line">          // set a random app name if not given.</span><br><span class="line">          if (!sparkConf.contains(&quot;spark.app.name&quot;)) &#123;</span><br><span class="line">            sparkConf.setAppName(java.util.UUID.randomUUID().toString)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          SparkContext.getOrCreate(sparkConf)</span><br><span class="line">          // Do not update `SparkConf` for existing `SparkContext`, as it&#x27;s shared by all sessions.</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        loadExtensions(extensions)</span><br><span class="line">        applyExtensions(</span><br><span class="line">          sparkContext.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty),</span><br><span class="line">          extensions)</span><br><span class="line"></span><br><span class="line">        session = new SparkSession(sparkContext, None, None, extensions, options.toMap)</span><br><span class="line">        setDefaultSession(session)</span><br><span class="line">        setActiveSession(session)</span><br><span class="line">        registerContextListener(sparkContext)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      return session</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>如何调用上面的SparkSession里面的getOrCreate函数<ul>
<li>就是在创建spark application的时候，用户进行初始化的时候进行手动调用</li>
<li>代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.builder</span><br><span class="line">     .master(&quot;local&quot;)</span><br><span class="line">     .appName(&quot;Word Count&quot;)</span><br><span class="line">     .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h5 id="代码分析总结"><a href="#代码分析总结" class="headerlink" title="代码分析总结"></a>代码分析总结</h5><ul>
<li>在提交单个spark应用后<ul>
<li>SparkSession初始化，调用getOrCreate()函数</li>
<li>使用getOrElse，先查看全局是否存在sparkContext，如果不存在就直接调用SparkContext.getOrCreate(sparkConf)重新创建</li>
<li>调用SparkContext里面的getOrCreate函数，创建SparkContext实例并变化活跃状态<ul>
<li>在SparkContext实例化过程中，代码val (sched, ts) &#x3D; SparkContext.createTaskScheduler(this, master)调用<br>createTaskScheduler函数，对TaskSchedulerImpl类进行实例化，同时使用initialize确定该应用是使用FIFO还是FAIR调度，创建Pool对象和空的schedulableQueue队列</li>
<li>代码_dagScheduler &#x3D; new DAGScheduler(this)，将job分解为多个stage，然后一个stage转化为多个task，最后将这些task都合并到一个taskSet里面，也就是一个stage对应一个taskSet</li>
</ul>
</li>
<li>最终在stage转化为对应的taskSet后，调用TaskSchedulerImpl里面的submitTasks函数，使用addTaskSetManager函数将生成的taskSet添加到schedulableQueue里面</li>
<li>最终在org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers里面，为待调度的任务分配资源的时候，调用了getSortedTaskSetQueue函数根据配置的调度模式来进行调度</li>
</ul>
</li>
<li><em>从上面分析可以得出，FIFO和FAIR模式，只是针对单个spark应用里面的schedulableQueue队列的Scheduler对象进行排序调度，不会对整个spark集群里面的所有应用程序进行调度</em></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" data-id="cm6vrnau80001y0mff3qzfk3y" data-title="spark在standalone模式下，FIFO和FAIR调度模式的对象分析" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/spark-3-3-0/" rel="tag">spark 3.3.0</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/standalone/" rel="tag">standalone</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/yunshenBlog.github.io/tags/%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F/" rel="tag">调度模式</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/yunshenBlog.github.io/2025/02/07/hello-world/" class="article-date">
  <time class="dt-published" datetime="2025-02-07T13:28:03.106Z" itemprop="datePublished">2025-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/yunshenBlog.github.io/2025/02/07/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/07/hello-world/" data-id="cm6vrnau40000y0mfhg5e73rz" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/spark-3-3-0/" rel="tag">spark 3.3.0</a></li><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/standalone/" rel="tag">standalone</a></li><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></li><li class="tag-list-item"><a class="tag-list-link" href="/yunshenBlog.github.io/tags/%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F/" rel="tag">调度模式</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/yunshenBlog.github.io/tags/spark-3-3-0/" style="font-size: 10px;">spark 3.3.0</a> <a href="/yunshenBlog.github.io/tags/standalone/" style="font-size: 10px;">standalone</a> <a href="/yunshenBlog.github.io/tags/%E6%8A%80%E6%9C%AF/" style="font-size: 10px;">技术</a> <a href="/yunshenBlog.github.io/tags/%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">调度模式</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/yunshenBlog.github.io/archives/2025/02/">February 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</a>
          </li>
        
          <li>
            <a href="/yunshenBlog.github.io/2025/02/07/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/yunshenBlog.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/yunshenBlog.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/yunshenBlog.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/yunshenBlog.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/yunshenBlog.github.io/js/script.js"></script>





  </div>
</body>
</html>