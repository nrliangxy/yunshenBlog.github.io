<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>spark在standalone模式下，driver/application与master的rpc通信流程 | 只在此山中，云深不知处</title><meta name="author" content="nrliangxy"><meta name="copyright" content="nrliangxy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="&emsp;&emsp; 背景 spark集群在standalone模式下，在使用client模式下提交application的时候，driver运行在客户端机器上 如果客户端机器崩溃或者driver进程退出，application会失败 由于driver不在集群中，master无法直接管理driver的生命周期 本次只讨论在standalone模式下，使用client模式向master提交app">
<meta property="og:type" content="article">
<meta property="og:title" content="spark在standalone模式下，driver&#x2F;application与master的rpc通信流程">
<meta property="og:url" content="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/index.html">
<meta property="og:site_name" content="只在此山中，云深不知处">
<meta property="og:description" content="&emsp;&emsp; 背景 spark集群在standalone模式下，在使用client模式下提交application的时候，driver运行在客户端机器上 如果客户端机器崩溃或者driver进程退出，application会失败 由于driver不在集群中，master无法直接管理driver的生命周期 本次只讨论在standalone模式下，使用client模式向master提交app">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-02-25T12:27:00.000Z">
<meta property="article:modified_time" content="2025-03-06T01:21:24.294Z">
<meta property="article:author" content="nrliangxy">
<meta property="article:tag" content="spark 3.3.0">
<meta property="article:tag" content="standalone">
<meta property="article:tag" content="rpc">
<meta property="article:tag" content="driver">
<meta property="article:tag" content="application">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "spark在standalone模式下，driver/application与master的rpc通信流程",
  "url": "https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/",
  "image": "https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png",
  "datePublished": "2025-02-25T12:27:00.000Z",
  "dateModified": "2025-03-06T01:21:24.294Z",
  "author": [
    {
      "@type": "Person",
      "name": "nrliangxy",
      "url": "https://nrliangxy.github.io/yunshenBlog.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/yunshenBlog.github.io/img/favicon.png"><link rel="canonical" href="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/yunshenBlog.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/yunshenBlog.github.io/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'spark在standalone模式下，driver/application与master的rpc通信流程',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/yunshenBlog.github.io/"><span class="site-name">只在此山中，云深不知处</span></a><a class="nav-page-title" href="/yunshenBlog.github.io/"><span class="site-name">spark在standalone模式下，driver/application与master的rpc通信流程</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">spark在standalone模式下，driver/application与master的rpc通信流程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-02-25T12:27:00.000Z" title="Created 2025-02-25 20:27:00">2025-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-06T01:21:24.294Z" title="Updated 2025-03-06 09:21:24">2025-03-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/">大数据计算</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/%E6%89%B9%E5%A4%84%E7%90%86/">批处理</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/%E6%89%B9%E5%A4%84%E7%90%86/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>&emsp;&emsp;</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li>spark集群在standalone模式下，在使用client模式下提交application的时候，driver运行在客户端机器上</li>
<li>如果客户端机器崩溃或者driver进程退出，application会失败</li>
<li>由于driver不在集群中，master无法直接管理driver的生命周期</li>
<li>本次只讨论在standalone模式下，使用client模式向master提交application的时候，driver和application与master的通信流程</li>
</ul>
<h2 id="流程梳理"><a href="#流程梳理" class="headerlink" title="流程梳理"></a>流程梳理</h2><h3 id="1-在客户端服务器上面提交application"><a href="#1-在客户端服务器上面提交application" class="headerlink" title="1.在客户端服务器上面提交application"></a>1.在客户端服务器上面提交application</h3><h4 id="1-在客户端提交命令样例"><a href="#1-在客户端提交命令样例" class="headerlink" title="(1) 在客户端提交命令样例"></a>(1) 在客户端提交命令样例</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master spark://&lt;master-ip&gt;:&lt;master-port&gt; \</span><br><span class="line">  --deploy-mode client \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  &lt;application-args&gt;</span><br></pre></td></tr></table></figure>

<h4 id="2-代码"><a href="#2-代码" class="headerlink" title="(2) 代码"></a>(2) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.builder</span><br><span class="line">     .appName(&quot;Word Count&quot;)</span><br><span class="line">     .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure>
<h4 id="3-代码解析"><a href="#3-代码解析" class="headerlink" title="(3) 代码解析"></a>(3) 代码解析</h4><ul>
<li>在开发人员编写的application里面，开头需要初始化一个spark对象，定义该app名字为”Word Count”，并使用config方法添加需要使用的配置</li>
<li>调用getOrCreate()方法，获取或创建一个 SparkSession 实例</li>
</ul>
<h3 id="2-通过调用getOrCreate-方法，获取或创建一个SparkSession实例"><a href="#2-通过调用getOrCreate-方法，获取或创建一个SparkSession实例" class="headerlink" title="2.通过调用getOrCreate()方法，获取或创建一个SparkSession实例"></a>2.通过调用getOrCreate()方法，获取或创建一个SparkSession实例</h3><h4 id="1-代码"><a href="#1-代码" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">    /**</span><br><span class="line">     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new</span><br><span class="line">     * one based on the options set in this builder.</span><br><span class="line">     *</span><br><span class="line">     * This method first checks whether there is a valid thread-local SparkSession,</span><br><span class="line">     * and if yes, return that one. It then checks whether there is a valid global</span><br><span class="line">     * default SparkSession, and if yes, return that one. If no valid global default</span><br><span class="line">     * SparkSession exists, the method creates a new SparkSession and assigns the</span><br><span class="line">     * newly created SparkSession as the global default.</span><br><span class="line">     *</span><br><span class="line">     * In case an existing SparkSession is returned, the non-static config options specified in</span><br><span class="line">     * this builder will be applied to the existing SparkSession.</span><br><span class="line">     * getOrCreate() 函数的作用：获取或创建一个 SparkSession 实例。</span><br><span class="line">     *</span><br><span class="line">     *  核心逻辑：</span><br><span class="line">     *</span><br><span class="line">     *  检查当前线程或全局默认的 SparkSession 是否存在且有效。</span><br><span class="line">     *</span><br><span class="line">     *  如果不存在，则创建新的 SparkSession。</span><br><span class="line">     *</span><br><span class="line">     *  支持动态更新配置和加载扩展。</span><br><span class="line">     *</span><br><span class="line">     *  线程安全：通过 synchronized 关键字确保线程安全。</span><br><span class="line">     *</span><br><span class="line">     *  通过这种机制，getOrCreate() 函数能够高效地管理 SparkSession 的生命周期，并确保 Spark 应用程序的正确执行。</span><br><span class="line">     *</span><br><span class="line">     * @since 2.0.0</span><br><span class="line">     */</span><br><span class="line">//    synchronized: 确保线程安全，避免多个线程同时创建 SparkSession。</span><br><span class="line">    def getOrCreate(): SparkSession = synchronized &#123;</span><br><span class="line">//  sparkConf: 创建一个新的 SparkConf 对象，用于存储 Spark 配置。</span><br><span class="line">      val sparkConf = new SparkConf()</span><br><span class="line">//  options.foreach &#123; case (k, v) =&gt; sparkConf.set(k, v) &#125;: 将传入的配置项（options）设置到 sparkConf 中。</span><br><span class="line">      options.foreach &#123; case (k, v) =&gt; sparkConf.set(k, v) &#125;</span><br><span class="line">//  EXECUTOR_ALLOW_SPARK_CONTEXT: 检查是否允许在 Executor 上创建 SparkContext。</span><br><span class="line">      if (!sparkConf.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) &#123;</span><br><span class="line">//        assertOnDriver(): 如果配置不允许在 Executor 上创建 SparkContext，则确保当前代码运行在 Driver 上。</span><br><span class="line">        assertOnDriver()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Get the session from current thread&#x27;s active session.</span><br><span class="line">      // activeThreadSession.get(): 获取当前线程的活跃 SparkSession。</span><br><span class="line">      var session = activeThreadSession.get()</span><br><span class="line">      //  (session ne null) &amp;&amp; !session.sparkContext.isStopped: 如果当前线程有活跃的 SparkSession，</span><br><span class="line">      //  并且其 SparkContext 未停止，则直接返回该 SparkSession。</span><br><span class="line">      if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">//        applyModifiableSettings(): 如果传入的配置项（options）有变化，则更新 SparkSession 的配置。</span><br><span class="line">        applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava))</span><br><span class="line">        return session</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Global synchronization so we will only set the default session once.</span><br><span class="line">      SparkSession.synchronized &#123;</span><br><span class="line">        // If the current thread does not have an active session, get it from the global session.</span><br><span class="line">        session = defaultSession.get()</span><br><span class="line">        if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">          applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava))</span><br><span class="line">          return session</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // No active nor global default session. Create a new one.</span><br><span class="line">        //    使用getOrElse，先查看全局是否存在sparkContext，如果不存在就直接调用SparkContext.getOrCreate(sparkConf)重新创建</span><br><span class="line">        val sparkContext = userSuppliedContext.getOrElse &#123;</span><br><span class="line">          // set a random app name if not given.</span><br><span class="line">          if (!sparkConf.contains(&quot;spark.app.name&quot;)) &#123;</span><br><span class="line">            sparkConf.setAppName(java.util.UUID.randomUUID().toString)</span><br><span class="line">          &#125;</span><br><span class="line">//          SparkContext.getOrCreate(sparkConf): 获取或创建 SparkContext。</span><br><span class="line">          SparkContext.getOrCreate(sparkConf)</span><br><span class="line">          // Do not update `SparkConf` for existing `SparkContext`, as it&#x27;s shared by all sessions.</span><br><span class="line">        &#125;</span><br><span class="line">//        loadExtensions(extensions): 加载用户定义的 SparkSession 扩展。</span><br><span class="line">        loadExtensions(extensions)</span><br><span class="line">//        applyExtensions(): 应用扩展，将其注册到 SparkSession 中。</span><br><span class="line">        applyExtensions(</span><br><span class="line">          sparkContext.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty),</span><br><span class="line">          extensions)</span><br><span class="line">//        new SparkSession(...): 创建新的 SparkSession 实例。</span><br><span class="line">        session = new SparkSession(sparkContext, None, None, extensions, options.toMap)</span><br><span class="line">//        setDefaultSession(session): 将新创建的 SparkSession 设置为全局默认的 SparkSession。</span><br><span class="line">        setDefaultSession(session)</span><br><span class="line">//        setActiveSession(session): 将新创建的 SparkSession 设置为当前线程的活跃 SparkSession。</span><br><span class="line">        setActiveSession(session)</span><br><span class="line">//        registerContextListener(sparkContext): 注册 SparkContext 的监听器，以便在 SparkContext 停止时清理 SparkSession。</span><br><span class="line">        registerContextListener(sparkContext)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      return session</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-代码解析"><a href="#2-代码解析" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org.apache.spark.sql.SparkSession.Builder#getOrCreate()</li>
<li>先获取当前线程是否有活跃的SparkSession，如果有就更新当前SparkSession的配置，如果没有就重新创建</li>
<li>在创建新的SparkSession之前，使用代码SparkContext.getOrCreate(sparkConf)获取或创建SparkContext</li>
<li>在创建新的SparkContext对象的时候，会生成和初始化application和driver对象，同时开始了与master进行rpc通信</li>
</ul>
<h3 id="3-通过调用SparkContext-getOrCreate-sparkConf-获取或创建SparkContext"><a href="#3-通过调用SparkContext-getOrCreate-sparkConf-获取或创建SparkContext" class="headerlink" title="3.通过调用SparkContext.getOrCreate(sparkConf)获取或创建SparkContext"></a>3.通过调用SparkContext.getOrCreate(sparkConf)获取或创建SparkContext</h3><h4 id="1-代码-1"><a href="#1-代码-1" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def getOrCreate(config: SparkConf): SparkContext = &#123;</span><br><span class="line">  // Synchronize to ensure that multiple create requests don&#x27;t trigger an exception</span><br><span class="line">  // from assertNoOtherContextIsRunning within setActiveContext</span><br><span class="line">  //   使用 SPARK_CONTEXT_CONSTRUCTOR_LOCK 对象作为同步锁，确保在多线程环境下只有一个线程可以执行 SparkContext 的创建逻辑。</span><br><span class="line">  SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized &#123;</span><br><span class="line">    //    如果没有活跃的 SparkContext，则创建一个新的 SparkContext 实例，并将其设置为活跃的 SparkContext。</span><br><span class="line">    if (activeContext.get() == null) &#123;</span><br><span class="line">      //     调用 SparkContext 的构造函数，传入 SparkConf 对象 config，创建一个新的 SparkContext 实例。</span><br><span class="line">      //     在 SparkContext 的构造函数中，会初始化任务调度器、DAG 调度器、存储系统等核心组件。</span><br><span class="line">      //     将新创建的 SparkContext 实例设置为全局活跃的 SparkContext。</span><br><span class="line">      //     源码位置: org.apache.spark.SparkContext#setActiveContext。</span><br><span class="line">      setActiveContext(new SparkContext(config))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      if (config.getAll.nonEmpty) &#123;</span><br><span class="line">        logWarning(&quot;Using an existing SparkContext; some configuration may not take effect.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    activeContext.get()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-1"><a href="#2-代码解析-1" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org.apache.spark.SparkContext#getOrCreate</li>
<li>在加锁的情况下，先判断是否存在活跃的SparkContext，如果没有，需要重新创建一个SparkContext实例，并将其设置未活跃的SparkContext</li>
<li>在调用setActiveContext(new SparkContext(config))代码的时候，会自动执行类SparkContext初始化的参数</li>
<li>在初始化类SparkContext的参数的时候，会初始化创建application和driver，以及DAGScheduler开始将job分解未stage，进而转化为taskset</li>
</ul>
<h3 id="4-SparkContext类进行初始化，运行默认初始化代码"><a href="#4-SparkContext类进行初始化，运行默认初始化代码" class="headerlink" title="4.SparkContext类进行初始化，运行默认初始化代码"></a>4.SparkContext类进行初始化，运行默认初始化代码</h3><h4 id="1-代码-2"><a href="#1-代码-2" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">val (sched, ts) = SparkContext.createTaskScheduler(this, master)</span><br><span class="line">_schedulerBackend = sched</span><br><span class="line">_taskScheduler = ts</span><br><span class="line">//    通过DAGScheduler将job分解为stage，然后分解为taskset</span><br><span class="line">_dagScheduler = new DAGScheduler(this)</span><br><span class="line">_heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)</span><br><span class="line"></span><br><span class="line">val _executorMetricsSource =</span><br><span class="line">  if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) &#123;</span><br><span class="line">    Some(new ExecutorMetricsSource)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    None</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">// create and start the heartbeater for collecting memory metrics</span><br><span class="line">_heartbeater = new Heartbeater(</span><br><span class="line">  () =&gt; SparkContext.this.reportHeartBeat(_executorMetricsSource),</span><br><span class="line">  &quot;driver-heartbeater&quot;,</span><br><span class="line">  conf.get(EXECUTOR_HEARTBEAT_INTERVAL))</span><br><span class="line">_heartbeater.start()</span><br><span class="line"></span><br><span class="line">// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&#x27;s</span><br><span class="line">// constructor</span><br><span class="line">_taskScheduler.start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">case SPARK_REGEX(sparkUrl) =&gt;</span><br><span class="line">//        生成</span><br><span class="line">val scheduler = new TaskSchedulerImpl(sc)</span><br><span class="line">val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)</span><br><span class="line">//      standalone模式下执行</span><br><span class="line">val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">scheduler.initialize(backend)</span><br><span class="line">(backend, scheduler)</span><br></pre></td></tr></table></figure>

<h4 id="2-代码解析-2"><a href="#2-代码解析-2" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>调用object SparkContext里面的createTaskScheduler函数进行初始化TaskSchedulerImpl，StandaloneSchedulerBackend</li>
<li>_schedulerBackend为new StandaloneSchedulerBackend(scheduler, sc, masterUrls)初始化对象，_taskScheduler为new TaskSchedulerImpl(sc)初始化对象</li>
<li>最终使用_taskScheduler.start()调用_taskScheduler对象里面的start()函数</li>
</ul>
<h3 id="5-将StandaloneSchedulerBackend对象付给类TaskSchedulerImpl的backend参数"><a href="#5-将StandaloneSchedulerBackend对象付给类TaskSchedulerImpl的backend参数" class="headerlink" title="5.将StandaloneSchedulerBackend对象付给类TaskSchedulerImpl的backend参数"></a>5.将StandaloneSchedulerBackend对象付给类TaskSchedulerImpl的backend参数</h3><h4 id="1-代码-3"><a href="#1-代码-3" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  def initialize(backend: SchedulerBackend): Unit = &#123;</span><br><span class="line">//    val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">//    在SparkContext.scala的createTaskScheduler函数里面initialize函数调用</span><br><span class="line">    this.backend = backend</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode match &#123;</span><br><span class="line">        case SchedulingMode.FIFO =&gt;</span><br><span class="line">          new FIFOSchedulableBuilder(rootPool)</span><br><span class="line">        case SchedulingMode.FAIR =&gt;</span><br><span class="line">          new FairSchedulableBuilder(rootPool, sc)</span><br><span class="line">        case _ =&gt;</span><br><span class="line">          throw new IllegalArgumentException(s&quot;Unsupported $SCHEDULER_MODE_PROPERTY: &quot; +</span><br><span class="line">          s&quot;$schedulingMode&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-3"><a href="#2-代码解析-3" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org.apache.spark.scheduler.TaskSchedulerImpl#initialize</li>
<li>将val backend &#x3D; new StandaloneSchedulerBackend(scheduler, sc, masterUrls)对象传给TaskSchedulerImpl.scala里面的this.backend，为后面调用backend.start()做准备</li>
</ul>
<h3 id="6-通过-taskScheduler-start-调用TaskSchedulerImpl类的start-函数"><a href="#6-通过-taskScheduler-start-调用TaskSchedulerImpl类的start-函数" class="headerlink" title="6.通过_taskScheduler.start()调用TaskSchedulerImpl类的start()函数"></a>6.通过_taskScheduler.start()调用TaskSchedulerImpl类的start()函数</h3><h4 id="1-代码-4"><a href="#1-代码-4" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">  override def start(): Unit = &#123;</span><br><span class="line">//		启动任务调度器的后端（backend），准备接收资源分配和任务调度请求。</span><br><span class="line">//    val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">    backend.start()</span><br><span class="line"></span><br><span class="line">    if (!isLocal &amp;&amp; conf.get(SPECULATION_ENABLED)) &#123;</span><br><span class="line">      logInfo(&quot;Starting speculative execution thread&quot;)</span><br><span class="line">//      作用：启动一个定时任务，定期检查是否有需要推测执行的任务。</span><br><span class="line">//			speculationScheduler:这是一个 ScheduledExecutorService 实例，用于调度定时任务。 它负责定期执行推测执行的检查逻辑。</span><br><span class="line">//			scheduleWithFixedDelay:这是一个定时调度方法，用于以固定的延迟时间重复执行任务。</span><br><span class="line">//			参数说明：第一个参数：要执行的任务（一个 Runnable 或函数）。</span><br><span class="line">//			第二个参数：初始延迟时间（SPECULATION_INTERVAL_MS）。</span><br><span class="line">//			第三个参数：每次执行任务的间隔时间（SPECULATION_INTERVAL_MS）。</span><br><span class="line">//			第四个参数：时间单位（TimeUnit.MILLISECONDS 表示毫秒）。</span><br><span class="line">//			() =&gt; Utils.tryOrStopSparkContext(sc) &#123; checkSpeculatableTasks() &#125;：这是一个匿名函数，作为定时任务的执行逻辑。Utils.tryOrStopSparkContext(sc)：捕获任务执行过程中的异常，并在发生异常时停止 SparkContext。checkSpeculatableTasks()：检查是否有需要推测执行的任务。</span><br><span class="line">//			SPECULATION_INTERVAL_MS：这是推测执行检查的时间间隔，默认值为 100 毫秒（可以通过 spark.speculation.interval 配置）。</span><br><span class="line">      speculationScheduler.scheduleWithFixedDelay(</span><br><span class="line">        () =&gt; Utils.tryOrStopSparkContext(sc) &#123; checkSpeculatableTasks() &#125;,</span><br><span class="line">        SPECULATION_INTERVAL_MS, SPECULATION_INTERVAL_MS, TimeUnit.MILLISECONDS)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-4"><a href="#2-代码解析-4" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;TaskSchedulerImpl.scala:232</li>
<li>backend为上面指定的this.backend</li>
<li>this.backend为val backend &#x3D; new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</li>
<li>backend.start()调用类StandaloneSchedulerBackend里面start()函数</li>
</ul>
<h3 id="7-正式创建application和driver对象，同时和master创建rpc连接"><a href="#7-正式创建application和driver对象，同时和master创建rpc连接" class="headerlink" title="7.正式创建application和driver对象，同时和master创建rpc连接"></a>7.正式创建application和driver对象，同时和master创建rpc连接</h3><h4 id="1-代码-5"><a href="#1-代码-5" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line">		/**</span><br><span class="line">	 * start()函数负责初始化并启动Standalone模式下的调度后端，包括创建Executor启动命令、设置资源需求、启动客户端、等待注册并更新应用状态，</span><br><span class="line">	 * 确保Spark应用顺利运行。</span><br><span class="line">	 */</span><br><span class="line">  override def start(): Unit = &#123;</span><br><span class="line">//    调用 CoarseGrainedSchedulerBackend 的 start</span><br><span class="line">//    StandaloneSchedulerBackend 在启动时，会调用其父类 CoarseGrainedSchedulerBackend 的 start() 方法，该方法会向 Master 注册 Driver。</span><br><span class="line">    super.start()</span><br><span class="line"></span><br><span class="line">    // SPARK-21159. The scheduler backend should only try to connect to the launcher when in client</span><br><span class="line">    // mode. In cluster mode, the code that submits the application to the Master needs to connect</span><br><span class="line">    // to the launcher instead.</span><br><span class="line">		/**</span><br><span class="line">		 * Client模式：Driver进程运行在提交应用的客户端机器上。</span><br><span class="line">		 * Cluster模式：Driver进程运行在集群中的某个节点上（由集群管理器启动）。</span><br><span class="line">		 * launcherBackend是LauncherBackend的实例，用于与Spark应用的启动器（如spark-submit）通信，报告应用的状态</span><br><span class="line">		 * （如SUBMITTED、RUNNING、FINISHED等）。connect()的作用是建立与启动器的连接，以便在应用运行期间发送状态更新。</span><br><span class="line">		 * Client模式：</span><br><span class="line">		 * Driver运行在客户端机器上，与启动器（如spark-submit）在同一环境中。</span><br><span class="line">		 * 启动器需要实时获取应用的状态（如是否启动成功、是否运行中、是否完成等），因此需要通过launcherBackend.connect()建立连接，</span><br><span class="line">		 * 以便将状态信息发送回启动器。</span><br><span class="line">		 * 如果未建立连接，启动器无法感知应用的状态变化，可能导致用户无法及时了解应用的运行情况。</span><br><span class="line">		 * Cluster模式：</span><br><span class="line">		 * Driver运行在集群中的某个节点上，与启动器（如spark-submit）不在同一环境中。</span><br><span class="line">		 * 启动器在提交应用后，通常不会继续监控应用的状态（因为Driver已经在集群中运行），因此不需要通过launcherBackend.connect()建立连接。</span><br><span class="line">		 * 集群管理器（如Standalone Master、YARN ResourceManager等）会负责监控应用的状态，并将状态信息反馈给用户。</span><br><span class="line">		 *</span><br><span class="line">		 * Client模式：Driver运行在客户端，启动器需要实时获取应用状态，因此需要调用launcherBackend.connect()建立连接。</span><br><span class="line">		 * Cluster模式：Driver运行在集群中，启动器不直接监控应用状态，因此不需要调用launcherBackend.connect()。</span><br><span class="line">		 */</span><br><span class="line">    if (sc.deployMode == &quot;client&quot;) &#123;</span><br><span class="line">      launcherBackend.connect()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // The endpoint for executors to talk to us</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：构建Driver的RPC地址，Executor需要通过该地址与Driver通信。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * sc.conf.get(config.DRIVER_HOST_ADDRESS)：从配置中获取Driver的主机地址。</span><br><span class="line">		 * sc.conf.get(config.DRIVER_PORT)：从配置中获取Driver的端口。</span><br><span class="line">		 * CoarseGrainedSchedulerBackend.ENDPOINT_NAME：Driver的RPC端点名称（固定为CoarseGrainedScheduler）。</span><br><span class="line">		 * 返回值：driverUrl是一个字符串，格式为spark://&lt;driver_host&gt;:&lt;driver_port&gt;，Executor会使用该URL连接到Driver。</span><br><span class="line">		 */</span><br><span class="line">    val driverUrl = RpcEndpointAddress(</span><br><span class="line">      sc.conf.get(config.DRIVER_HOST_ADDRESS),</span><br><span class="line">      sc.conf.get(config.DRIVER_PORT),</span><br><span class="line">      CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：定义启动Executor时需要的命令行参数。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * --driver-url：Driver的RPC地址，Executor通过该地址与Driver通信</span><br><span class="line">		 * --executor-id：Executor的唯一ID，由Worker节点动态生成并替换&#123;&#123;EXECUTOR_ID&#125;&#125;</span><br><span class="line">		 * --hostname：Executor运行的主机名，由Worker节点动态生成并替换&#123;&#123;HOSTNAME&#125;&#125;</span><br><span class="line">		 * --cores：Executor分配的CPU核数，由Worker节点动态生成并替换&#123;&#123;CORES&#125;&#125;</span><br><span class="line">		 * --app-id：Spark应用的唯一ID，由Worker节点动态生成并替换&#123;&#123;APP_ID&#125;&#125;</span><br><span class="line">		 * --worker-url：Worker节点的RPC地址，由Worker节点动态生成并替换&#123;&#123;WORKER_URL&#125;&#125;</span><br><span class="line">		 * 说明：这些参数中的占位符（如&#123;&#123;EXECUTOR_ID&#125;&#125;）会在Executor启动时由Worker节点替换为实际值。</span><br><span class="line">		 */</span><br><span class="line">    val args = Seq(</span><br><span class="line">      &quot;--driver-url&quot;, driverUrl,</span><br><span class="line">      &quot;--executor-id&quot;, &quot;&#123;&#123;EXECUTOR_ID&#125;&#125;&quot;,</span><br><span class="line">      &quot;--hostname&quot;, &quot;&#123;&#123;HOSTNAME&#125;&#125;&quot;,</span><br><span class="line">      &quot;--cores&quot;, &quot;&#123;&#123;CORES&#125;&#125;&quot;,</span><br><span class="line">      &quot;--app-id&quot;, &quot;&#123;&#123;APP_ID&#125;&#125;&quot;,</span><br><span class="line">      &quot;--worker-url&quot;, &quot;&#123;&#123;WORKER_URL&#125;&#125;&quot;)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：从配置中获取额外的JVM参数，用于启动Executor进程。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * config.EXECUTOR_JAVA_OPTIONS：配置项spark.executor.extraJavaOptions，用于设置Executor的JVM参数。</span><br><span class="line">		 * Utils.splitCommandString：将字符串形式的JVM参数拆分为一个字符串序列。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有额外的JVM参数。</span><br><span class="line">		 */</span><br><span class="line">    val extraJavaOpts = sc.conf.get(config.EXECUTOR_JAVA_OPTIONS)</span><br><span class="line">      .map(Utils.splitCommandString).getOrElse(Seq.empty)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：从配置中获取Executor的类路径。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * config.EXECUTOR_CLASS_PATH：配置项spark.executor.extraClassPath，用于设置Executor的额外类路径。</span><br><span class="line">		 * split(java.io.File.pathSeparator)：将类路径字符串按路径分隔符拆分为多个路径。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有类路径条目。</span><br><span class="line">		 */</span><br><span class="line">    val classPathEntries = sc.conf.get(config.EXECUTOR_CLASS_PATH)</span><br><span class="line">      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：从配置中获取Executor的库路径。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * config.EXECUTOR_LIBRARY_PATH：配置项spark.executor.extraLibraryPath，用于设置Executor的额外库路径。</span><br><span class="line">		 * split(java.io.File.pathSeparator)：将库路径字符串按路径分隔符拆分为多个路径。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有库路径条目。</span><br><span class="line">		 */</span><br><span class="line">    val libraryPathEntries = sc.conf.get(config.EXECUTOR_LIBRARY_PATH)</span><br><span class="line">      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)</span><br><span class="line">    // When testing, expose the parent class path to the child. This is processed by</span><br><span class="line">    // compute-classpath.&#123;cmd,sh&#125; and makes all needed jars available to child processes</span><br><span class="line">    // when the assembly is built with the &quot;*-provided&quot; profiles enabled.</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：在测试环境下，将父进程的类路径传递给子进程（Executor）。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * IS_TESTING.key：判断是否处于测试环境的配置项。</span><br><span class="line">		 * sys.props(&quot;java.class.path&quot;)：获取当前JVM进程的类路径。</span><br><span class="line">		 * 返回值：如果是测试环境，返回父进程的类路径；否则返回空列表。</span><br><span class="line">		 */</span><br><span class="line">    val testingClassPath =</span><br><span class="line">      if (sys.props.contains(IS_TESTING.key)) &#123;</span><br><span class="line">        sys.props(&quot;java.class.path&quot;).split(java.io.File.pathSeparator).toSeq</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        Nil</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    // Start executors with a few necessary configs for registering with the scheduler</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：构建Executor的完整JVM参数。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * Utils.sparkJavaOpts：从配置中提取与Executor启动相关的JVM参数。</span><br><span class="line">		 * SparkConf.isExecutorStartupConf：过滤出与Executor启动相关的配置项。</span><br><span class="line">		 * extraJavaOpts：额外的JVM参数。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有JVM参数。</span><br><span class="line">		 */</span><br><span class="line">    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)</span><br><span class="line">    val javaOpts = sparkJavaOpts ++ extraJavaOpts</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：定义启动Executor进程的命令。</span><br><span class="line">		 * &quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;：Executor的主类。</span><br><span class="line">		 * args：传递给Executor的参数</span><br><span class="line">		 * sc.executorEnvs：Executor的环境变量。</span><br><span class="line">		 * classPathEntries ++ testingClassPath：Executor的类路径</span><br><span class="line">		 * libraryPathEntries：Executor的库路径。</span><br><span class="line">		 * javaOpts：JVM选项。</span><br><span class="line">		 */</span><br><span class="line">    val command = Command(&quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;,</span><br><span class="line">      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：获取Spark应用的Web UI URL，用于监控和调试。</span><br><span class="line">		 */</span><br><span class="line">    val webUrl = sc.ui.map(_.webUrl).getOrElse(&quot;&quot;)</span><br><span class="line">//		从配置中获取每个Executor的CPU核数。</span><br><span class="line">    val coresPerExecutor = conf.getOption(config.EXECUTOR_CORES.key).map(_.toInt)</span><br><span class="line">    // If we&#x27;re using dynamic allocation, set our initial executor limit to 0 for now.</span><br><span class="line">    // ExecutorAllocationManager will send the real initial limit to the Master later.</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：如果启用了动态资源分配，初始Executor限制设为0，后续由ExecutorAllocationManager调整。</span><br><span class="line">		 */</span><br><span class="line">    val initialExecutorLimit =</span><br><span class="line">      if (Utils.isDynamicAllocationEnabled(conf)) &#123;</span><br><span class="line">        Some(0)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        None</span><br><span class="line">      &#125;</span><br><span class="line">//		解析Executor的资源需求，如GPU、内存等。</span><br><span class="line">    val executorResourceReqs = ResourceUtils.parseResourceRequirements(conf,</span><br><span class="line">      config.SPARK_EXECUTOR_PREFIX)</span><br><span class="line">//  如果未指定，默认为 0</span><br><span class="line">    val priority = conf.getInt(&quot;spark.submit.priority&quot;, 0)</span><br><span class="line">		/**</span><br><span class="line">		 * 描述Spark应用的基本信息。</span><br><span class="line">		 * sc.appName：应用名称。</span><br><span class="line">		 * maxCores：最大CPU核数。</span><br><span class="line">		 * sc.executorMemory：Executor内存。</span><br><span class="line">		 * command：启动Executor的命令。</span><br><span class="line">		 * webUrl：Web UI URL。</span><br><span class="line">		 * sc.eventLogDir：事件日志目录。</span><br><span class="line">		 * sc.eventLogCodec：事件日志编码。</span><br><span class="line">		 * coresPerExecutor：每个Executor的核数。</span><br><span class="line">		 * initialExecutorLimit：初始Executor限制。</span><br><span class="line">		 * executorResourceReqs：Executor资源需求。</span><br><span class="line">		 * priority：应用优先级。</span><br><span class="line">		 */</span><br><span class="line">    val appDesc = ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,</span><br><span class="line">      webUrl, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit,</span><br><span class="line">      resourceReqsPerExecutor = executorResourceReqs, priority = priority)</span><br><span class="line"></span><br><span class="line">		/**</span><br><span class="line">		 * 创建与集群管理器通信的客户端。</span><br><span class="line">		 * sc.env.rpcEnv：RPC环境。</span><br><span class="line">		 * masters：集群管理器地址。</span><br><span class="line">		 * appDesc：应用描述。</span><br><span class="line">		 * this：当前StandaloneSchedulerBackend实例。</span><br><span class="line">		 * conf：配置对象。</span><br><span class="line">		 */</span><br><span class="line">    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)</span><br><span class="line">//		启动客户端，开始与集群管理器通信。</span><br><span class="line">    client.start()</span><br><span class="line">//		将应用状态设置为SUBMITTED，表示应用已提交。</span><br><span class="line">    launcherBackend.setState(SparkAppHandle.State.SUBMITTED)</span><br><span class="line">//		等待Executor注册完成，确保资源分配到位。</span><br><span class="line">    waitForRegistration()</span><br><span class="line">//		将应用状态设置为RUNNING，表示应用正在运行。</span><br><span class="line">    launcherBackend.setState(SparkAppHandle.State.RUNNING)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="2-代码解析-5"><a href="#2-代码解析-5" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend#start</li>
<li>start()函数负责初始化并启动Standalone模式下的调度后端，包括创建Executor启动命令、设置资源需求、启动客户端、等待注册并更新应用状态，确保Spark应用顺利运行。</li>
<li>其中代码client &#x3D; new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf) client.start()开始启动客户端，与集群管理器进行通信</li>
</ul>
<h3 id="8-在StandaloneAppClient里面，在Spark的RPC环境里面注册一个新的RPC端点。自动开启rpc周期函数的调用"><a href="#8-在StandaloneAppClient里面，在Spark的RPC环境里面注册一个新的RPC端点。自动开启rpc周期函数的调用" class="headerlink" title="8.在StandaloneAppClient里面，在Spark的RPC环境里面注册一个新的RPC端点。自动开启rpc周期函数的调用"></a>8.在StandaloneAppClient里面，在Spark的RPC环境里面注册一个新的RPC端点。自动开启rpc周期函数的调用</h3><h4 id="1-代码-6"><a href="#1-代码-6" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"> * contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"> * this work for additional information regarding copyright ownership.</span><br><span class="line"> * The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"> * (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"> * the License.  You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> -  http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> * See the License for the specific language governing permissions and</span><br><span class="line"> * limitations under the License.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">package org.apache.spark.deploy.client</span><br><span class="line"></span><br><span class="line">import java.util.concurrent._</span><br><span class="line">import java.util.concurrent.&#123;Future =&gt; JFuture, ScheduledFuture =&gt; JScheduledFuture&#125;</span><br><span class="line">import java.util.concurrent.atomic.&#123;AtomicBoolean, AtomicReference&#125;</span><br><span class="line"></span><br><span class="line">import scala.concurrent.Future</span><br><span class="line">import scala.util.&#123;Failure, Success&#125;</span><br><span class="line">import scala.util.control.NonFatal</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.deploy.&#123;ApplicationDescription, ExecutorState&#125;</span><br><span class="line">import org.apache.spark.deploy.DeployMessages._</span><br><span class="line">import org.apache.spark.deploy.master.Master</span><br><span class="line">import org.apache.spark.internal.Logging</span><br><span class="line">import org.apache.spark.rpc._</span><br><span class="line">import org.apache.spark.scheduler.ExecutorDecommissionInfo</span><br><span class="line">import org.apache.spark.util.&#123;RpcUtils, ThreadUtils&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Interface allowing applications to speak with a Spark standalone cluster manager.</span><br><span class="line"> *</span><br><span class="line"> * Takes a master URL, an app description, and a listener for cluster events, and calls</span><br><span class="line"> * back the listener when various events occur.</span><br><span class="line"> *</span><br><span class="line"> * @param masterUrls Each url should look like spark://host:port.</span><br><span class="line"> */</span><br><span class="line">private[spark] class StandaloneAppClient(</span><br><span class="line">    rpcEnv: RpcEnv,</span><br><span class="line">    masterUrls: Array[String],</span><br><span class="line">    appDescription: ApplicationDescription,</span><br><span class="line">    listener: StandaloneAppClientListener,</span><br><span class="line">    conf: SparkConf)</span><br><span class="line">  extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  private val masterRpcAddresses = masterUrls.map(RpcAddress.fromSparkURL(_))</span><br><span class="line"></span><br><span class="line">  private val REGISTRATION_TIMEOUT_SECONDS = 20</span><br><span class="line">  private val REGISTRATION_RETRIES = 3</span><br><span class="line"></span><br><span class="line">  private val endpoint = new AtomicReference[RpcEndpointRef]</span><br><span class="line">  private val appId = new AtomicReference[String]</span><br><span class="line">  private val registered = new AtomicBoolean(false)</span><br><span class="line"></span><br><span class="line">  private class ClientEndpoint(override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint</span><br><span class="line">    with Logging &#123;</span><br><span class="line"></span><br><span class="line">    private var master: Option[RpcEndpointRef] = None</span><br><span class="line">    // To avoid calling listener.disconnected() multiple times</span><br><span class="line">    private var alreadyDisconnected = false</span><br><span class="line">    // To avoid calling listener.dead() multiple times</span><br><span class="line">    private val alreadyDead = new AtomicBoolean(false)</span><br><span class="line">    private val registerMasterFutures = new AtomicReference[Array[JFuture[_]]]</span><br><span class="line">    private val registrationRetryTimer = new AtomicReference[JScheduledFuture[_]]</span><br><span class="line"></span><br><span class="line">    // A thread pool for registering with masters. Because registering with a master is a blocking</span><br><span class="line">    // action, this thread pool must be able to create &quot;masterRpcAddresses.size&quot; threads at the same</span><br><span class="line">    // time so that we can register with all masters.</span><br><span class="line">    private val registerMasterThreadPool = ThreadUtils.newDaemonCachedThreadPool(</span><br><span class="line">      &quot;appclient-register-master-threadpool&quot;,</span><br><span class="line">      masterRpcAddresses.length // Make sure we can register with all masters at the same time</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    // A scheduled executor for scheduling the registration actions</span><br><span class="line">    private val registrationRetryThread =</span><br><span class="line">      ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;appclient-registration-retry-thread&quot;)</span><br><span class="line"></span><br><span class="line">		/**</span><br><span class="line">		 * onStart()是由Spark的RPC框架在RPC端点启动时自动调用的。具体来说，它的调用时机如下：</span><br><span class="line">		 * RPC端点的启动流程:</span><br><span class="line">		 * 当StandaloneAppClient的start()方法被调用时，会执行以下代码：endpoint.set(rpcEnv.setupEndpoint(&quot;AppClient&quot;, new ClientEndpoint(rpcEnv)))</span><br><span class="line">		 * rpcEnv.setupEndpoint：在RPC环境中注册ClientEndpoint。</span><br><span class="line">		 * RPC框架的生命周期管理：在RPC端点注册后，RPC框架会自动调用其生命周期方法，包括：</span><br><span class="line">		 * onStart()：在端点启动时调用。</span><br><span class="line">		 * receive()：处理接收到的消息。</span><br><span class="line">		 * onStop()：在端点停止时调用。</span><br><span class="line">		 * 因此，onStart()是在ClientEndpoint注册到RPC环境后，由RPC框架自动调用的。</span><br><span class="line">		 *</span><br><span class="line">		 * 以下是onStart()被调用的完整调用链：</span><br><span class="line">		 * StandaloneSchedulerBackend.start()：-&gt; 调用StandaloneAppClient.start() -&gt;</span><br><span class="line">		 * 调用rpcEnv.setupEndpoint(&quot;AppClient&quot;, new ClientEndpoint(rpcEnv))，注册ClientEndpoint -&gt; 在ClientEndpoint注册后，RPC框架自动调用其onStart()方法。</span><br><span class="line">		 * -&gt; ClientEndpoint.onStart(): 执行初始化逻辑，尝试向Master注册应用程序。</span><br><span class="line">		 *</span><br><span class="line">		 * onStart()的执行逻辑</span><br><span class="line">		 * 在onStart()中，registerWithMaster(1)会尝试向Master注册应用程序。如果注册成功，ClientEndpoint会进入正常运行状态；如果注册失败，则会触发异常处理逻辑：</span><br><span class="line">		 * logWarning：记录警告日志。</span><br><span class="line">		 * markDisconnected：标记为断开连接状态。</span><br><span class="line">		 * stop：停止ClientEndpoint。</span><br><span class="line">		 *</span><br><span class="line">		 * 总结：</span><br><span class="line">		 * onStart()方法是在ClientEndpoint注册到RPC环境后，由RPC框架自动调用的。它的主要作用是初始化ClientEndpoint，并尝试向Master注册应用程序。如果注册失败，ClientEndpoint会进入异常处理流程，记录日志并停止运行。</span><br><span class="line">		 * 通过这种方式，Spark确保了应用程序在启动时能够与Master建立连接，并获取所需的资源。</span><br><span class="line">		 */</span><br><span class="line">    override def onStart(): Unit = &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        registerWithMaster(1)</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        case e: Exception =&gt;</span><br><span class="line">          logWarning(&quot;Failed to connect to master&quot;, e)</span><br><span class="line">          markDisconnected()</span><br><span class="line">          stop()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     *  Register with all masters asynchronously and returns an array `Future`s for cancellation.</span><br><span class="line">     */</span><br><span class="line">    private def tryRegisterAllMasters(): Array[JFuture[_]] = &#123;</span><br><span class="line">//      masterRpcAddresses: 这是一个包含所有 Master 节点 RPC 地址的集合。</span><br><span class="line">//      for ... yield: 对 masterRpcAddresses 中的每个 Master 地址，生成一个异步任务。</span><br><span class="line">      for (masterAddress &lt;- masterRpcAddresses) yield &#123;</span><br><span class="line">        /**</span><br><span class="line">         * registerMasterThreadPool: 这是一个线程池（ExecutorService），用于提交异步任务。</span><br><span class="line">         * submit(new Runnable &#123; ... &#125;): 向线程池提交一个 Runnable 任务，该任务会异步执行 run 方法中的逻辑。</span><br><span class="line">         * 返回值: 每个 submit 调用返回一个 Future，表示异步任务的结果。</span><br><span class="line">         */</span><br><span class="line">        registerMasterThreadPool.submit(new Runnable &#123;</span><br><span class="line">          override def run(): Unit = try &#123;</span><br><span class="line">//            registered.get: 检查 registered（一个 AtomicBoolean）的值。如果为 true，表示应用程序已经成功注册到某个 Master 节点。</span><br><span class="line">//            return: 如果已经注册成功，直接返回，不再执行后续逻辑。</span><br><span class="line">            if (registered.get) &#123;</span><br><span class="line">              return</span><br><span class="line">            &#125;</span><br><span class="line">//            logInfo: 记录一条日志，表示正在尝试连接指定的 Master 节点。</span><br><span class="line">//            masterAddress.toSparkURL: 将 Master 地址转换为可读的 URL 格式。</span><br><span class="line">            logInfo(&quot;Connecting to master &quot; + masterAddress.toSparkURL + &quot;...&quot;)</span><br><span class="line">//            rpcEnv.setupEndpointRef: 通过 Spark 的 RPC 环境 rpcEnv，创建一个指向 Master 节点的 RPC 引用。</span><br><span class="line">//            masterAddress: Master 节点的地址。</span><br><span class="line">//            Master.ENDPOINT_NAME: Master 节点的 RPC 端点名称。</span><br><span class="line">//            masterRef: 返回的 RPC 引用，用于与 Master 节点通信。</span><br><span class="line">            val masterRef = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)</span><br><span class="line">//            masterRef.send: 向 Master 节点发送一条消息。</span><br><span class="line">//            RegisterApplication(appDescription, self): 发送的消息内容，表示注册应用程序的请求。</span><br><span class="line">//            appDescription: 应用程序的描述信息（如应用名称、核心数、内存等）。</span><br><span class="line">//            self: 当前 StandaloneAppClient 的 RPC 引用，用于 Master 节点与应用程序通信。</span><br><span class="line">              /**</span><br><span class="line">               * 向org/apache/spark/deploy/master/Master.scala发送消息，触发master的rpc端点，该端点接收到消息后，触发receive()方法</span><br><span class="line">               * 调用receive()里面的RegisterApplication，然后在通过driver.send(RegisteredApplication(app.id, self))反馈回来，</span><br><span class="line">               * 然后在自动触发了该driver的receive()函数，通过RegisteredApplication函数通过registered.set(true)，将registered更新为true</span><br><span class="line">               */</span><br><span class="line">            masterRef.send(RegisterApplication(appDescription, self))</span><br><span class="line">          &#125; catch &#123;</span><br><span class="line">//            InterruptedException: 如果任务被中断（例如线程池关闭），直接忽略。</span><br><span class="line">            case ie: InterruptedException =&gt; // Cancelled</span><br><span class="line">//              NonFatal(e): 捕获其他非致命异常，并记录警告日志。</span><br><span class="line">            case NonFatal(e) =&gt; logWarning(s&quot;Failed to connect to master $masterAddress&quot;, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Register with all masters asynchronously. It will call `registerWithMaster` every</span><br><span class="line">     * REGISTRATION_TIMEOUT_SECONDS seconds until exceeding REGISTRATION_RETRIES times.</span><br><span class="line">     * Once we connect to a master successfully, all scheduling work and Futures will be cancelled.</span><br><span class="line">     *</span><br><span class="line">     * nthRetry means this is the nth attempt to register with master.</span><br><span class="line">     * nthRetry: Int: 当前重试次数，用于跟踪已经尝试注册的次数。</span><br><span class="line">     */</span><br><span class="line">    private def registerWithMaster(nthRetry: Int): Unit = &#123;</span><br><span class="line">      /**</span><br><span class="line">       * tryRegisterAllMasters(): 调用 tryRegisterAllMasters 函数，尝试向所有已知的 Master 节点注册应用程序。该函数会返回一个 Future 列表，表示每个 Master 节点的注册请求。</span><br><span class="line">       * registerMasterFutures.set(...):将返回的 Future 列表存储到 registerMasterFutures 中。registerMasterFutures 是一个 AtomicReference，用于线程安全地存储和管理注册请求的 Future。</span><br><span class="line">       */</span><br><span class="line">      registerMasterFutures.set(tryRegisterAllMasters())</span><br><span class="line">      /**</span><br><span class="line">       * registrationRetryThread.schedule(...): 使用 registrationRetryThread（一个 ScheduledExecutorService）安排一个定时延时任务。该任务会在 REGISTRATION_TIMEOUT_SECONDS 秒后执行。</span><br><span class="line">       * registrationRetryTimer.set(...): 将定时任务的 ScheduledFuture 存储到 registrationRetryTimer 中。registrationRetryTimer 是一个 AtomicReference，用于线程安全地管理定时任务。</span><br><span class="line">       */</span><br><span class="line">      registrationRetryTimer.set(registrationRetryThread.schedule(new Runnable &#123;</span><br><span class="line">        override def run(): Unit = &#123;</span><br><span class="line">//          registered.get: 检查 registered（一个 AtomicBoolean）的值。如果为 true，表示应用程序已经成功注册到某个 Master 节点。</span><br><span class="line">          if (registered.get) &#123;</span><br><span class="line">//            registerMasterFutures.get.foreach(_.cancel(true)): 如果已经注册成功，取消所有未完成的注册请求。</span><br><span class="line">            registerMasterFutures.get.foreach(_.cancel(true))</span><br><span class="line">//            registerMasterThreadPool.shutdownNow(): 关闭注册线程池，释放资源。</span><br><span class="line">            registerMasterThreadPool.shutdownNow()</span><br><span class="line">//            nthRetry &gt;= REGISTRATION_RETRIES: 检查当前重试次数是否超过了最大重试次数 REGISTRATION_RETRIES。</span><br><span class="line">          &#125; else if (nthRetry &gt;= REGISTRATION_RETRIES) &#123;</span><br><span class="line">//            markDead(&quot;All masters are unresponsive! Giving up.&quot;): 如果超过最大重试次数，调用 markDead 方法，标记应用程序为“死亡”状态，并记录错误信息。</span><br><span class="line">            markDead(&quot;All masters are unresponsive! Giving up.&quot;)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">//            registerMasterFutures.get.foreach(_.cancel(true)): 取消所有未完成的注册请求。</span><br><span class="line">            registerMasterFutures.get.foreach(_.cancel(true))</span><br><span class="line">//            registerWithMaster(nthRetry + 1): 递归调用 registerWithMaster 函数，增加重试次数（nthRetry + 1），继续尝试注册。</span><br><span class="line">            registerWithMaster(nthRetry + 1)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, REGISTRATION_TIMEOUT_SECONDS, TimeUnit.SECONDS))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Send a message to the current master. If we have not yet registered successfully with any</span><br><span class="line">     * master, the message will be dropped.</span><br><span class="line">     */</span><br><span class="line">    private def sendToMaster(message: Any): Unit = &#123;</span><br><span class="line">      master match &#123;</span><br><span class="line">        case Some(masterRef) =&gt; masterRef.send(message)</span><br><span class="line">        case None =&gt; logWarning(s&quot;Drop $message because has not yet connected to master&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private def isPossibleMaster(remoteAddress: RpcAddress): Boolean = &#123;</span><br><span class="line">      masterRpcAddresses.contains(remoteAddress)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 调用时机: 当 RPC 端点接收到消息时，RPC 框架会调用 receive() 方法。</span><br><span class="line">     * 作用: 处理接收到的消息。receive() 是一个消息处理函数，根据消息类型执行相应的逻辑。</span><br><span class="line">     * 特点:</span><br><span class="line">     * receive() 是异步的，每次接收到消息时都会触发。</span><br><span class="line">     * 它不是在上一个函数（如 onStart()）完成后才启动的，而是独立运行的。</span><br><span class="line">     * @return</span><br><span class="line">     */</span><br><span class="line">    override def receive: PartialFunction[Any, Unit] = &#123;</span><br><span class="line">      case RegisteredApplication(appId_, masterRef) =&gt;</span><br><span class="line">        // FIXME How to handle the following cases?</span><br><span class="line">        // 1. A master receives multiple registrations and sends back multiple</span><br><span class="line">        // RegisteredApplications due to an unstable network.</span><br><span class="line">        // 2. Receive multiple RegisteredApplication from different masters because the master is</span><br><span class="line">        // changing.</span><br><span class="line">        appId.set(appId_)</span><br><span class="line">        registered.set(true)</span><br><span class="line">        master = Some(masterRef)</span><br><span class="line">        listener.connected(appId.get)</span><br><span class="line"></span><br><span class="line">      case ApplicationRemoved(message) =&gt;</span><br><span class="line">        markDead(&quot;Master removed our application: %s&quot;.format(message))</span><br><span class="line">        stop()</span><br><span class="line"></span><br><span class="line">      case ExecutorAdded(id: Int, workerId: String, hostPort: String, cores: Int, memory: Int) =&gt;</span><br><span class="line">        val fullId = appId + &quot;/&quot; + id</span><br><span class="line">        logInfo(&quot;Executor added: %s on %s (%s) with %d core(s)&quot;.format(fullId, workerId, hostPort,</span><br><span class="line">          cores))</span><br><span class="line">        listener.executorAdded(fullId, workerId, hostPort, cores, memory)</span><br><span class="line"></span><br><span class="line">      case ExecutorUpdated(id, state, message, exitStatus, workerHost) =&gt;</span><br><span class="line">        val fullId = appId + &quot;/&quot; + id</span><br><span class="line">        val messageText = message.map(s =&gt; &quot; (&quot; + s + &quot;)&quot;).getOrElse(&quot;&quot;)</span><br><span class="line">        logInfo(&quot;Executor updated: %s is now %s%s&quot;.format(fullId, state, messageText))</span><br><span class="line">        if (ExecutorState.isFinished(state)) &#123;</span><br><span class="line">          listener.executorRemoved(fullId, message.getOrElse(&quot;&quot;), exitStatus, workerHost)</span><br><span class="line">        &#125; else if (state == ExecutorState.DECOMMISSIONED) &#123;</span><br><span class="line">          listener.executorDecommissioned(fullId,</span><br><span class="line">            ExecutorDecommissionInfo(message.getOrElse(&quot;&quot;), workerHost))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      case WorkerRemoved(id, host, message) =&gt;</span><br><span class="line">        logInfo(&quot;Master removed worker %s: %s&quot;.format(id, message))</span><br><span class="line">        listener.workerRemoved(id, host, message)</span><br><span class="line"></span><br><span class="line">      case MasterChanged(masterRef, masterWebUiUrl) =&gt;</span><br><span class="line">        logInfo(&quot;Master has changed, new master is at &quot; + masterRef.address.toSparkURL)</span><br><span class="line">//      master: 是 StandaloneAppClient 中的一个变量，用于存储当前 Master 节点的 RPC 引用。</span><br><span class="line">//      Some(masterRef): 将新的 Master 节点的 RPC 引用存储到 master 变量中。</span><br><span class="line">        master = Some(masterRef)</span><br><span class="line">//      alreadyDisconnected: 是一个标志，用于标记 Driver 是否已经与 Master 节点断开连接。</span><br><span class="line">//      false: 重置该标志，表示 Driver 已经重新连接到新的 Master 节点。</span><br><span class="line">        alreadyDisconnected = false</span><br><span class="line">//      masterRef.send: 向新的 Master 节点发送 MasterChangeAcknowledged 消息。</span><br><span class="line">//      MasterChangeAcknowledged(appId.get): 表示 Driver 已经确认 Master 节点的变化，并告知新的 Master 节点自己的 Application ID。</span><br><span class="line">        masterRef.send(MasterChangeAcknowledged(appId.get))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = &#123;</span><br><span class="line">      case StopAppClient =&gt;</span><br><span class="line">        markDead(&quot;Application has been stopped.&quot;)</span><br><span class="line">        sendToMaster(UnregisterApplication(appId.get))</span><br><span class="line">        context.reply(true)</span><br><span class="line">        stop()</span><br><span class="line"></span><br><span class="line">      case r: RequestExecutors =&gt;</span><br><span class="line">        master match &#123;</span><br><span class="line">          case Some(m) =&gt; askAndReplyAsync(m, context, r)</span><br><span class="line">          case None =&gt;</span><br><span class="line">            logWarning(&quot;Attempted to request executors before registering with Master.&quot;)</span><br><span class="line">            context.reply(false)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      case k: KillExecutors =&gt;</span><br><span class="line">        master match &#123;</span><br><span class="line">          case Some(m) =&gt; askAndReplyAsync(m, context, k)</span><br><span class="line">          case None =&gt;</span><br><span class="line">            logWarning(&quot;Attempted to kill executors before registering with Master.&quot;)</span><br><span class="line">            context.reply(false)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private def askAndReplyAsync[T](</span><br><span class="line">        endpointRef: RpcEndpointRef,</span><br><span class="line">        context: RpcCallContext,</span><br><span class="line">        msg: T): Unit = &#123;</span><br><span class="line">      // Ask a message and create a thread to reply with the result.  Allow thread to be</span><br><span class="line">      // interrupted during shutdown, otherwise context must be notified of NonFatal errors.</span><br><span class="line">      endpointRef.ask[Boolean](msg).andThen &#123;</span><br><span class="line">        case Success(b) =&gt; context.reply(b)</span><br><span class="line">        case Failure(ie: InterruptedException) =&gt; // Cancelled</span><br><span class="line">        case Failure(NonFatal(t)) =&gt; context.sendFailure(t)</span><br><span class="line">      &#125;(ThreadUtils.sameThread)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def onDisconnected(address: RpcAddress): Unit = &#123;</span><br><span class="line">      if (master.exists(_.address == address)) &#123;</span><br><span class="line">        logWarning(s&quot;Connection to $address failed; waiting for master to reconnect...&quot;)</span><br><span class="line">        markDisconnected()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def onNetworkError(cause: Throwable, address: RpcAddress): Unit = &#123;</span><br><span class="line">      if (isPossibleMaster(address)) &#123;</span><br><span class="line">        logWarning(s&quot;Could not connect to $address: $cause&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Notify the listener that we disconnected, if we hadn&#x27;t already done so before.</span><br><span class="line">     */</span><br><span class="line">    def markDisconnected(): Unit = &#123;</span><br><span class="line">      if (!alreadyDisconnected) &#123;</span><br><span class="line">        listener.disconnected()</span><br><span class="line">        alreadyDisconnected = true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def markDead(reason: String): Unit = &#123;</span><br><span class="line">      if (!alreadyDead.get) &#123;</span><br><span class="line">        listener.dead(reason)</span><br><span class="line">        alreadyDead.set(true)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">       * onStop() 是在端点生命周期结束时调用的，与 onStart() 和 receive() 的执行无关。</span><br><span class="line">       * 它不是在上一个函数完成后才启动的，而是在端点停止时触发。</span><br><span class="line">       */</span><br><span class="line">    override def onStop(): Unit = &#123;</span><br><span class="line">      if (registrationRetryTimer.get != null) &#123;</span><br><span class="line">        registrationRetryTimer.get.cancel(true)</span><br><span class="line">      &#125;</span><br><span class="line">      registrationRetryThread.shutdownNow()</span><br><span class="line">      registerMasterFutures.get.foreach(_.cancel(true))</span><br><span class="line">      registerMasterThreadPool.shutdownNow()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def start(): Unit = &#123;</span><br><span class="line">		/**</span><br><span class="line">		 * rpcEnv.setupEndpoint:</span><br><span class="line">		 * 作用：在Spark的RPC环境中注册一个新的RPC端点。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * &quot;AppClient&quot;：端点的名称，用于唯一标识该端点。</span><br><span class="line">		 * new ClientEndpoint(rpcEnv)：创建一个ClientEndpoint实例，作为RPC端点的实现。</span><br><span class="line">		 * 返回值：返回一个RpcEndpointRef，表示对该端点的引用。</span><br><span class="line">		 * endpoint.set</span><br><span class="line">		 * 作用：将创建的RPC端点引用保存到endpoint变量中，以便后续使用。</span><br><span class="line">		 * ClientEndpoint:</span><br><span class="line">		 * 作用：ClientEndpoint是一个RPC端点，负责与Spark Standalone集群管理器（Master）通信。它实现了以下功能：</span><br><span class="line">		 * 向Master注册应用程序。</span><br><span class="line">		 * 接收来自Master的消息（如资源分配、Executor状态更新等）。</span><br><span class="line">		 * 处理Master的响应（如注册成功、资源分配等）。</span><br><span class="line">         *</span><br><span class="line">         * onStart(): 在端点注册到 RPC 环境后立即调用。</span><br><span class="line">         * receive(): 在端点运行期间，每次接收到消息时调用。</span><br><span class="line">         * onStop(): 在端点停止时调用。</span><br><span class="line">         * 它们的调用顺序和关系如下：</span><br><span class="line">         * onStart() 和 onStop() 是生命周期方法，分别用于初始化和清理。</span><br><span class="line">         * receive() 是消息处理方法，独立于生命周期方法运行。</span><br><span class="line">		 */</span><br><span class="line">    // Just launch an rpcEndpoint; it will call back into the listener.</span><br><span class="line">    endpoint.set(rpcEnv.setupEndpoint(&quot;AppClient&quot;, new ClientEndpoint(rpcEnv)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def stop(): Unit = &#123;</span><br><span class="line">    if (endpoint.get != null) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        val timeout = RpcUtils.askRpcTimeout(conf)</span><br><span class="line">        timeout.awaitResult(endpoint.get.ask[Boolean](StopAppClient))</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        case e: TimeoutException =&gt;</span><br><span class="line">          logInfo(&quot;Stop request to Master timed out; it may already be shut down.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      endpoint.set(null)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Request executors from the Master by specifying the total number desired,</span><br><span class="line">   * including existing pending and running executors.</span><br><span class="line">   *</span><br><span class="line">   * @return whether the request is acknowledged.</span><br><span class="line">   */</span><br><span class="line">  def requestTotalExecutors(requestedTotal: Int): Future[Boolean] = &#123;</span><br><span class="line">    if (endpoint.get != null &amp;&amp; appId.get != null) &#123;</span><br><span class="line">      endpoint.get.ask[Boolean](RequestExecutors(appId.get, requestedTotal))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      logWarning(&quot;Attempted to request executors before driver fully initialized.&quot;)</span><br><span class="line">      Future.successful(false)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Kill the given list of executors through the Master.</span><br><span class="line">   * @return whether the kill request is acknowledged.</span><br><span class="line">   */</span><br><span class="line">  def killExecutors(executorIds: Seq[String]): Future[Boolean] = &#123;</span><br><span class="line">    if (endpoint.get != null &amp;&amp; appId.get != null) &#123;</span><br><span class="line">      endpoint.get.ask[Boolean](KillExecutors(appId.get, executorIds))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      logWarning(&quot;Attempted to kill executors before driver fully initialized.&quot;)</span><br><span class="line">      Future.successful(false)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-6"><a href="#2-代码解析-6" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>start()函数里面endpoint.set(rpcEnv.setupEndpoint(“AppClient”, new ClientEndpoint(rpcEnv)))在Spark的RPC环境里面注册一个新的RPC端点</li>
<li>注册PRC端点后，会立即调用onStart()函数。receive()函数是在端点运行期间，每次接收到消息时调用。onStop()函数时在端点停止时调用。onStart()和onStop()是生命周期方法，分别用于初始化和清理，receive()是消息处理方法，独立于生命周期方法运行</li>
<li>application注册RPC端点后，调用初始化方法onStart()，</li>
<li>onStart()方法调用registerWithMaster(1)函数开始与活跃的master进行连接</li>
<li>在函数方法调用registerWithMaster里面，调用tryRegisterAllMasters()函数，向已知的master节点尝试建立连接，并存储在registerMasterFutures列表里面</li>
<li>在函数tryRegisterAllMasters()里面，对master列表进行for循环，尝试去建立连接。在standalonemo’s模式下，只要一个master是被激活的，其它都是备用。当和唯一一个被激活的master建立rpc连接后，就会调用Master.scala类里面的case RegisterApplication(description, driver)模式代码</li>
</ul>
<h3 id="9-在Master里面，为已经和Master建立的rpc连接的application进行反馈消息"><a href="#9-在Master里面，为已经和Master建立的rpc连接的application进行反馈消息" class="headerlink" title="9.在Master里面，为已经和Master建立的rpc连接的application进行反馈消息"></a>9.在Master里面，为已经和Master建立的rpc连接的application进行反馈消息</h3><h4 id="1-代码-7"><a href="#1-代码-7" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">    /**</span><br><span class="line">     * 在client模式下，driver信息不会被持久化</span><br><span class="line">     * 在 Client 模式下，Driver 运行在客户端机器上，而不是集群中的某个节点。因此，Driver 的生命周期与客户端机器绑定，而不是与集群绑定。具体原因如下：</span><br><span class="line">     * Client 模式：</span><br><span class="line">     * Driver 运行在客户端机器上，由用户直接控制。</span><br><span class="line">     * 如果客户端机器崩溃或 Driver 进程退出，Application 会失败。</span><br><span class="line">     * 由于 Driver 不在集群中，Master 无法直接管理 Driver 的生命周期。</span><br><span class="line">     *  集群模式 vs Client 模式</span><br><span class="line">     *  集群模式：</span><br><span class="line">     *  Driver 运行在集群中的某个节点上，Master 可以管理 Driver 的生命周期。</span><br><span class="line">     *  如果 Driver 崩溃，Master 可以重新启动 Driver。</span><br><span class="line">     *  在集群模式下，Driver 的信息会被持久化。</span><br><span class="line">     *  Client 模式：</span><br><span class="line">     *  Driver 运行在客户端机器上，Master 无法管理 Driver 的生命周期。</span><br><span class="line">     *  如果 Driver 崩溃，Application 会失败，Master 无法重新启动 Driver。</span><br><span class="line">     *  因此，Driver 的信息不会被持久化。</span><br><span class="line">     */</span><br><span class="line">    case RegisterApplication(description, driver) =&gt;</span><br><span class="line">      // TODO Prevent repeated registrations from some driver</span><br><span class="line">      if (state == RecoveryState.STANDBY) &#123;</span><br><span class="line">        // ignore, don&#x27;t send response</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        logInfo(&quot;Registering app &quot; + description.name)</span><br><span class="line">        val app = createApplication(description, driver)</span><br><span class="line">        registerApplication(app)</span><br><span class="line">        logInfo(&quot;Registered app &quot; + description.name + &quot; with ID &quot; + app.id)</span><br><span class="line">//        persistenceEngine.addApplication(app): 将 Application 的信息存储到 ZooKeeper 中。目的：在 Master 故障恢复后，能够恢复 Application 的状态。</span><br><span class="line">        persistenceEngine.addApplication(app)</span><br><span class="line">//        driver.send(RegisteredApplication(app.id, self)): 向 Driver 发送注册成功的响应。Driver 信息：Driver 的信息（如 driver 对象）不会被存储到 ZooKeeper 中。</span><br><span class="line">        driver.send(RegisteredApplication(app.id, self))</span><br><span class="line">        schedule()</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-7"><a href="#2-代码解析-7" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org.apache.spark.deploy.master.Master#receive</li>
<li>如果该master没有被激活，状态为STANDBY，就直接ignore。</li>
<li>如果该master为alive，那就创建app，生成唯一的appId，并将该app持久化存储在ZooKeeper</li>
<li>schedule()负责调度集群资源，启动等待的 Driver 和 Executor</li>
</ul>
<h3 id="10-application和driver，接收master通过rpc返回的RegisteredApplication信号"><a href="#10-application和driver，接收master通过rpc返回的RegisteredApplication信号" class="headerlink" title="10.application和driver，接收master通过rpc返回的RegisteredApplication信号"></a>10.application和driver，接收master通过rpc返回的RegisteredApplication信号</h3><h4 id="1-代码-8"><a href="#1-代码-8" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">case RegisteredApplication(appId_, masterRef) =&gt;</span><br><span class="line">  // FIXME How to handle the following cases?</span><br><span class="line">  // 1. A master receives multiple registrations and sends back multiple</span><br><span class="line">  // RegisteredApplications due to an unstable network.</span><br><span class="line">  // 2. Receive multiple RegisteredApplication from different masters because the master is</span><br><span class="line">  // changing.</span><br><span class="line">  appId.set(appId_)</span><br><span class="line">  registered.set(true)</span><br><span class="line">  master = Some(masterRef)</span><br><span class="line">  listener.connected(appId.get)</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-8"><a href="#2-代码解析-8" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>将最新的appId放入appId列表里面，更新registered状态为true</li>
<li>添加master引用</li>
</ul>
<h3 id="11-反馈给registerWithMaster函数里面的定时延时任务，可以取消registerMasterFutures里面的所有生命周期"><a href="#11-反馈给registerWithMaster函数里面的定时延时任务，可以取消registerMasterFutures里面的所有生命周期" class="headerlink" title="11.反馈给registerWithMaster函数里面的定时延时任务，可以取消registerMasterFutures里面的所有生命周期"></a>11.反馈给registerWithMaster函数里面的定时延时任务，可以取消registerMasterFutures里面的所有生命周期</h3><h4 id="1-代码-9"><a href="#1-代码-9" class="headerlink" title="(1) 代码"></a>(1) 代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">    /**</span><br><span class="line">     * Register with all masters asynchronously. It will call `registerWithMaster` every</span><br><span class="line">     * REGISTRATION_TIMEOUT_SECONDS seconds until exceeding REGISTRATION_RETRIES times.</span><br><span class="line">     * Once we connect to a master successfully, all scheduling work and Futures will be cancelled.</span><br><span class="line">     *</span><br><span class="line">     * nthRetry means this is the nth attempt to register with master.</span><br><span class="line">     * nthRetry: Int: 当前重试次数，用于跟踪已经尝试注册的次数。</span><br><span class="line">     */</span><br><span class="line">    private def registerWithMaster(nthRetry: Int): Unit = &#123;</span><br><span class="line">      /**</span><br><span class="line">       * tryRegisterAllMasters(): 调用 tryRegisterAllMasters 函数，尝试向所有已知的 Master 节点注册应用程序。该函数会返回一个 Future 列表，表示每个 Master 节点的注册请求。</span><br><span class="line">       * registerMasterFutures.set(...):将返回的 Future 列表存储到 registerMasterFutures 中。registerMasterFutures 是一个 AtomicReference，用于线程安全地存储和管理注册请求的 Future。</span><br><span class="line">       */</span><br><span class="line">      registerMasterFutures.set(tryRegisterAllMasters())</span><br><span class="line">      /**</span><br><span class="line">       * registrationRetryThread.schedule(...): 使用 registrationRetryThread（一个 ScheduledExecutorService）安排一个定时延时任务。该任务会在 REGISTRATION_TIMEOUT_SECONDS 秒后执行。</span><br><span class="line">       * registrationRetryTimer.set(...): 将定时任务的 ScheduledFuture 存储到 registrationRetryTimer 中。registrationRetryTimer 是一个 AtomicReference，用于线程安全地管理定时任务。</span><br><span class="line">       */</span><br><span class="line">      registrationRetryTimer.set(registrationRetryThread.schedule(new Runnable &#123;</span><br><span class="line">        override def run(): Unit = &#123;</span><br><span class="line">//          registered.get: 检查 registered（一个 AtomicBoolean）的值。如果为 true，表示应用程序已经成功注册到某个 Master 节点。</span><br><span class="line">          if (registered.get) &#123;</span><br><span class="line">//            registerMasterFutures.get.foreach(_.cancel(true)): 如果已经注册成功，取消所有未完成的注册请求。</span><br><span class="line">//            这个连接是由 Spark 的 RPC 框架管理的，与 Future 的生命周期无关。Future.cancel(true) 只是取消未完成的异步任务（即未完成的注册请求），并不会影响已经建立的 RPC 连接。</span><br><span class="line">            registerMasterFutures.get.foreach(_.cancel(true))</span><br><span class="line">//            registerMasterThreadPool.shutdownNow(): 关闭注册线程池，释放资源。</span><br><span class="line">            registerMasterThreadPool.shutdownNow()</span><br><span class="line">//            nthRetry &gt;= REGISTRATION_RETRIES: 检查当前重试次数是否超过了最大重试次数 REGISTRATION_RETRIES。</span><br><span class="line">          &#125; else if (nthRetry &gt;= REGISTRATION_RETRIES) &#123;</span><br><span class="line">//            markDead(&quot;All masters are unresponsive! Giving up.&quot;): 如果超过最大重试次数，调用 markDead 方法，标记应用程序为“死亡”状态，并记录错误信息。</span><br><span class="line">            markDead(&quot;All masters are unresponsive! Giving up.&quot;)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">//            registerMasterFutures.get.foreach(_.cancel(true)): 取消所有未完成的注册请求。</span><br><span class="line">            registerMasterFutures.get.foreach(_.cancel(true))</span><br><span class="line">//            registerWithMaster(nthRetry + 1): 递归调用 registerWithMaster 函数，增加重试次数（nthRetry + 1），继续尝试注册。</span><br><span class="line">            registerWithMaster(nthRetry + 1)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, REGISTRATION_TIMEOUT_SECONDS, TimeUnit.SECONDS))</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-代码解析-9"><a href="#2-代码解析-9" class="headerlink" title="(2) 代码解析"></a>(2) 代码解析</h4><ul>
<li>源代码路径：org.apache.spark.deploy.client.StandaloneAppClient.ClientEndpoint#registerWithMaster</li>
<li>该函数会使用tryRegisterAllMasters()函数对所有的master节点进行注册创建rpc端点，并存储在registerMasterFutures列表里面</li>
<li>如果alive的master通过rpc返回RegisteredApplication信息，将registered设置为true</li>
<li>当registered.get为true，就直接使用registerMasterFutures.get.foreach(_.cancel(true))，取消未完成的异步任务。同时registerMasterThreadPool.shutdownNow() 关闭注册线程池，释放资源</li>
<li>application和driver就和alive状态的master正式创建了rpc连接</li>
</ul>
<h2 id="结论总结"><a href="#结论总结" class="headerlink" title="结论总结"></a>结论总结</h2><ul>
<li>使用client模式下提交application的时候，driver运行在客户端机器上</li>
<li>初始化的application，使用endpoint.set(rpcEnv.setupEndpoint(“AppClient”, new ClientEndpoint(rpcEnv)))注册rpc端点</li>
<li>注册rpc端点后，自动启动生命周期函数onStart()</li>
<li>使用tryRegisterAllMasters()函数向各个master节点发送rpc信息</li>
<li>在收到alive的master节点返回的信息后，触发函数receive()里面的RegisteredApplication状态，更新registered为true</li>
<li>在registerWithMaster函数里面，识别到registered为true，就关闭未完成的registerMasterFutures异步任务，关闭注册线程，释放资源</li>
<li>通过上面流程，application和driver已经和master顺利创建rpc连接</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://nrliangxy.github.io/yunshenBlog.github.io">nrliangxy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/">https://nrliangxy.github.io/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/spark-3-3-0/">spark 3.3.0</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/standalone/">standalone</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/rpc/">rpc</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/driver/">driver</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/application/">application</a></div><div class="post-share"><div class="social-share" data-image="/yunshenBlog.github.io/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="spark在standalone模式下，FIFO和FAIR调度模式的对象分析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">spark在standalone模式下，worker与master的rpc通信流程</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，在集群的sbin目录下，启动start-worker.sh脚本，启动了worker端点 worker端点启动后，读取配置后，在spark的rpc通信框架里面注册rpc端点，和master节点进行rpc通信并进行心跳交互 本文就是要详细探讨一下work节点是如何与master节点进行rpc通信  流程梳理1.使用start-worker.sh脚本启动worker节点(1) 在客户端提交命令样例1bash start-worker.sh  (2) 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647  def main(argStrings: Array[String]): Unit = &#123;//    设置一个默认的未捕获异常处理器 SparkUncaughtExceptionHandler。//    exitOnUncaughtException = false...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">spark在standalone模式下，worker与master的rpc通信流程</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，在集群的sbin目录下，启动start-worker.sh脚本，启动了worker端点 worker端点启动后，读取配置后，在spark的rpc通信框架里面注册rpc端点，和master节点进行rpc通信并进行心跳交互 本文就是要详细探讨一下work节点是如何与master节点进行rpc通信  流程梳理1.使用start-worker.sh脚本启动worker节点(1) 在客户端提交命令样例1bash start-worker.sh  (2) 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647  def main(argStrings: Array[String]): Unit = &#123;//    设置一个默认的未捕获异常处理器 SparkUncaughtExceptionHandler。//    exitOnUncaughtException = false...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/21/zookeeper-master-select-leader/" title="spark在standalone模式下，使用zookeeper进行master选举流程剖析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-21</div><div class="info-item-2">spark在standalone模式下，使用zookeeper进行master选举流程剖析</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，为了确保master的高可用性，使用zookeeper来实现对master监控和选举 如果当前leader宕机后，如何实现leader的快速选举和相关worker节点，driver节点的快速恢复 本文要详细探讨一下leader选举和恢复的详细细节  流程梳理1.首次启动spark集群的master节点，使用start-master.sh来启动master节点(1) 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private[deploy] object Master extends Logging &#123;  val SYSTEM_NAME = &quot;sparkMaster&quot;  val ENDPOINT_NAME = &quot;Master&quot;  def...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/" title="spark 在 standalone 模式，Master 是如何调度和启动 Executor"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-18</div><div class="info-item-2">spark 在 standalone 模式，Master 是如何调度和启动 Executor</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数launchExecutor()通知 Worker 和 Driver 要启动一个新的 Executor，后续 Worker 和 Driver 端都是如何进行通信启动和执行 Executor？  流程梳理1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这种方式，Master 告诉 Worker 启动一个新的 Executor 并为其分配资源（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899case LaunchExecutor(masterUrl, appId, execId, appDesc,...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-28</div><div class="info-item-2">spark在standalone模式下，Master如何实现资源的调度和分配</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数schedule()来刷新资源情况，同时启动 Executor 的调度逻辑，为 Application 分配 Executor 资源  流程梳理1，调用scheduler()函数（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960  /**   * Schedule the currently available resources among waiting apps. This method will be called   * every time a new app joins or resource availability changes.   * 调度集群资源，启动等待的 Driver 和 Executor。   * 核心逻辑：   * 检查 Master...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="spark在standalone模式下，FIFO和FAIR调度模式的对象分析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-08</div><div class="info-item-2">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/yunshenBlog.github.io/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/yunshenBlog.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">nrliangxy</div><div class="author-info-description"></div><div class="site-data"><a href="/yunshenBlog.github.io/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/yunshenBlog.github.io/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/yunshenBlog.github.io/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">流程梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%9C%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%9D%A2%E6%8F%90%E4%BA%A4application"><span class="toc-number">2.1.</span> <span class="toc-text">1.在客户端服务器上面提交application</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%9C%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8F%90%E4%BA%A4%E5%91%BD%E4%BB%A4%E6%A0%B7%E4%BE%8B"><span class="toc-number">2.1.1.</span> <span class="toc-text">(1) 在客户端提交命令样例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81"><span class="toc-number">2.1.2.</span> <span class="toc-text">(2) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.1.3.</span> <span class="toc-text">(3) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%80%9A%E8%BF%87%E8%B0%83%E7%94%A8getOrCreate-%E6%96%B9%E6%B3%95%EF%BC%8C%E8%8E%B7%E5%8F%96%E6%88%96%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AASparkSession%E5%AE%9E%E4%BE%8B"><span class="toc-number">2.2.</span> <span class="toc-text">2.通过调用getOrCreate()方法，获取或创建一个SparkSession实例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81"><span class="toc-number">2.2.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.2.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%80%9A%E8%BF%87%E8%B0%83%E7%94%A8SparkContext-getOrCreate-sparkConf-%E8%8E%B7%E5%8F%96%E6%88%96%E5%88%9B%E5%BB%BASparkContext"><span class="toc-number">2.3.</span> <span class="toc-text">3.通过调用SparkContext.getOrCreate(sparkConf)获取或创建SparkContext</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-1"><span class="toc-number">2.3.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-1"><span class="toc-number">2.3.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-SparkContext%E7%B1%BB%E8%BF%9B%E8%A1%8C%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E8%BF%90%E8%A1%8C%E9%BB%98%E8%AE%A4%E5%88%9D%E5%A7%8B%E5%8C%96%E4%BB%A3%E7%A0%81"><span class="toc-number">2.4.</span> <span class="toc-text">4.SparkContext类进行初始化，运行默认初始化代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.4.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-2"><span class="toc-number">2.4.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%B0%86StandaloneSchedulerBackend%E5%AF%B9%E8%B1%A1%E4%BB%98%E7%BB%99%E7%B1%BBTaskSchedulerImpl%E7%9A%84backend%E5%8F%82%E6%95%B0"><span class="toc-number">2.5.</span> <span class="toc-text">5.将StandaloneSchedulerBackend对象付给类TaskSchedulerImpl的backend参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-3"><span class="toc-number">2.5.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-3"><span class="toc-number">2.5.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E9%80%9A%E8%BF%87-taskScheduler-start-%E8%B0%83%E7%94%A8TaskSchedulerImpl%E7%B1%BB%E7%9A%84start-%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.</span> <span class="toc-text">6.通过_taskScheduler.start()调用TaskSchedulerImpl类的start()函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-4"><span class="toc-number">2.6.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-4"><span class="toc-number">2.6.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%AD%A3%E5%BC%8F%E5%88%9B%E5%BB%BAapplication%E5%92%8Cdriver%E5%AF%B9%E8%B1%A1%EF%BC%8C%E5%90%8C%E6%97%B6%E5%92%8Cmaster%E5%88%9B%E5%BB%BArpc%E8%BF%9E%E6%8E%A5"><span class="toc-number">2.7.</span> <span class="toc-text">7.正式创建application和driver对象，同时和master创建rpc连接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-5"><span class="toc-number">2.7.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-5"><span class="toc-number">2.7.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%9C%A8StandaloneAppClient%E9%87%8C%E9%9D%A2%EF%BC%8C%E5%9C%A8Spark%E7%9A%84RPC%E7%8E%AF%E5%A2%83%E9%87%8C%E9%9D%A2%E6%B3%A8%E5%86%8C%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84RPC%E7%AB%AF%E7%82%B9%E3%80%82%E8%87%AA%E5%8A%A8%E5%BC%80%E5%90%AFrpc%E5%91%A8%E6%9C%9F%E5%87%BD%E6%95%B0%E7%9A%84%E8%B0%83%E7%94%A8"><span class="toc-number">2.8.</span> <span class="toc-text">8.在StandaloneAppClient里面，在Spark的RPC环境里面注册一个新的RPC端点。自动开启rpc周期函数的调用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-6"><span class="toc-number">2.8.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-6"><span class="toc-number">2.8.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E5%9C%A8Master%E9%87%8C%E9%9D%A2%EF%BC%8C%E4%B8%BA%E5%B7%B2%E7%BB%8F%E5%92%8CMaster%E5%BB%BA%E7%AB%8B%E7%9A%84rpc%E8%BF%9E%E6%8E%A5%E7%9A%84application%E8%BF%9B%E8%A1%8C%E5%8F%8D%E9%A6%88%E6%B6%88%E6%81%AF"><span class="toc-number">2.9.</span> <span class="toc-text">9.在Master里面，为已经和Master建立的rpc连接的application进行反馈消息</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-7"><span class="toc-number">2.9.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-7"><span class="toc-number">2.9.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-application%E5%92%8Cdriver%EF%BC%8C%E6%8E%A5%E6%94%B6master%E9%80%9A%E8%BF%87rpc%E8%BF%94%E5%9B%9E%E7%9A%84RegisteredApplication%E4%BF%A1%E5%8F%B7"><span class="toc-number">2.10.</span> <span class="toc-text">10.application和driver，接收master通过rpc返回的RegisteredApplication信号</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-8"><span class="toc-number">2.10.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-8"><span class="toc-number">2.10.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E5%8F%8D%E9%A6%88%E7%BB%99registerWithMaster%E5%87%BD%E6%95%B0%E9%87%8C%E9%9D%A2%E7%9A%84%E5%AE%9A%E6%97%B6%E5%BB%B6%E6%97%B6%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%8F%96%E6%B6%88registerMasterFutures%E9%87%8C%E9%9D%A2%E7%9A%84%E6%89%80%E6%9C%89%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F"><span class="toc-number">2.11.</span> <span class="toc-text">11.反馈给registerWithMaster函数里面的定时延时任务，可以取消registerMasterFutures里面的所有生命周期</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81-9"><span class="toc-number">2.11.1.</span> <span class="toc-text">(1) 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-9"><span class="toc-number">2.11.2.</span> <span class="toc-text">(2) 代码解析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">结论总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/" title="spark 在 standalone 模式，Master 是如何调度和启动 Executor">spark 在 standalone 模式，Master 是如何调度和启动 Executor</a><time datetime="2025-04-17T22:53:11.000Z" title="Created 2025-04-18 06:53:11">2025-04-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配">spark在standalone模式下，Master如何实现资源的调度和分配</a><time datetime="2025-03-28T01:57:10.000Z" title="Created 2025-03-28 09:57:10">2025-03-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/21/zookeeper-master-select-leader/" title="spark在standalone模式下，使用zookeeper进行master选举流程剖析">spark在standalone模式下，使用zookeeper进行master选举流程剖析</a><time datetime="2025-03-21T03:57:09.000Z" title="Created 2025-03-21 11:57:09">2025-03-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程">spark在standalone模式下，worker与master的rpc通信流程</a><time datetime="2025-03-06T01:13:14.000Z" title="Created 2025-03-06 09:13:14">2025-03-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/" title="spark在standalone模式下，driver/application与master的rpc通信流程">spark在standalone模式下，driver/application与master的rpc通信流程</a><time datetime="2025-02-25T12:27:00.000Z" title="Created 2025-02-25 20:27:00">2025-02-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By nrliangxy</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/yunshenBlog.github.io/js/utils.js"></script><script src="/yunshenBlog.github.io/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>