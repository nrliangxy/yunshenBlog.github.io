<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>spark 在 standalone 模式，Master 是如何调度和启动 Executor | 只在此山中，云深不知处</title><meta name="author" content="nrliangxy"><meta name="copyright" content="nrliangxy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数launchExecutor()通知 Worker 和 Driver 要启动一个新的 Executor，后续 Worker 和 Driver 端都是如何进行通信启动和执行 Executor？  流程梳理1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这">
<meta property="og:type" content="article">
<meta property="og:title" content="spark 在 standalone 模式，Master 是如何调度和启动 Executor">
<meta property="og:url" content="https://nrliangxy.github.io/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/index.html">
<meta property="og:site_name" content="只在此山中，云深不知处">
<meta property="og:description" content="&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数launchExecutor()通知 Worker 和 Driver 要启动一个新的 Executor，后续 Worker 和 Driver 端都是如何进行通信启动和执行 Executor？  流程梳理1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-04-17T22:53:11.000Z">
<meta property="article:modified_time" content="2025-04-21T23:33:50.738Z">
<meta property="article:author" content="nrliangxy">
<meta property="article:tag" content="spark 3.3.0">
<meta property="article:tag" content="standalone">
<meta property="article:tag" content="Executor">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "spark 在 standalone 模式，Master 是如何调度和启动 Executor",
  "url": "https://nrliangxy.github.io/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/",
  "image": "https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png",
  "datePublished": "2025-04-17T22:53:11.000Z",
  "dateModified": "2025-04-21T23:33:50.738Z",
  "author": [
    {
      "@type": "Person",
      "name": "nrliangxy",
      "url": "https://nrliangxy.github.io/yunshenBlog.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/yunshenBlog.github.io/img/favicon.png"><link rel="canonical" href="https://nrliangxy.github.io/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/yunshenBlog.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/yunshenBlog.github.io/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'spark 在 standalone 模式，Master 是如何调度和启动 Executor',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/yunshenBlog.github.io/"><span class="site-name">只在此山中，云深不知处</span></a><a class="nav-page-title" href="/yunshenBlog.github.io/"><span class="site-name">spark 在 standalone 模式，Master 是如何调度和启动 Executor</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">spark 在 standalone 模式，Master 是如何调度和启动 Executor</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-04-17T22:53:11.000Z" title="Created 2025-04-18 06:53:11">2025-04-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-04-21T23:33:50.738Z" title="Updated 2025-04-22 07:33:50">2025-04-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/">大数据计算</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/%E6%89%B9%E5%A4%84%E7%90%86/">批处理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>&emsp;&emsp;</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li>spark集群在 standalone 模式下，Master 通过函数launchExecutor()通知 Worker 和 Driver 要启动一个新的 Executor，后续 Worker 和 Driver 端都是如何进行通信启动和执行 Executor？</li>
</ul>
<h2 id="流程梳理"><a href="#流程梳理" class="headerlink" title="流程梳理"></a>流程梳理</h2><h3 id="1，消息会发送到-Worker-的-endpoint，这是-Worker-接收消息的端点。通过这种方式，Master-告诉-Worker-启动一个新的-Executor-并为其分配资源"><a href="#1，消息会发送到-Worker-的-endpoint，这是-Worker-接收消息的端点。通过这种方式，Master-告诉-Worker-启动一个新的-Executor-并为其分配资源" class="headerlink" title="1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这种方式，Master 告诉 Worker 启动一个新的 Executor 并为其分配资源"></a>1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这种方式，Master 告诉 Worker 启动一个新的 Executor 并为其分配资源</h3><h4 id="（1）代码"><a href="#（1）代码" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">case LaunchExecutor(masterUrl, appId, execId, appDesc, cores_, memory_, resources_) =&gt;</span><br><span class="line">//      Master 有效性检查：验证消息是否来自当前活跃的 Master（防止网络分区或旧 Master 的残留消息）</span><br><span class="line">      if (masterUrl != activeMasterUrl) &#123;</span><br><span class="line">        logWarning(&quot;Invalid Master (&quot; + masterUrl + &quot;) attempted to launch executor.&quot;)</span><br><span class="line">//        Worker 状态检查：检查 Worker 是否处于退役状态（decommissioned），若正在退役则拒绝启动 Executor</span><br><span class="line">      &#125; else if (decommissioned) &#123;</span><br><span class="line">        logWarning(&quot;Asked to launch an executor while decommissioned. Not launching executor.&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">//          记录 INFO 级别日志，标识启动的 Executor 和应用名称（如 app-20231001000000-0001/0）</span><br><span class="line">          logInfo(&quot;Asked to launch executor %s/%d for %s&quot;.format(appId, execId, appDesc.name))</span><br><span class="line"></span><br><span class="line">          // Create the executor&#x27;s working directory</span><br><span class="line">//          在 Worker 的 workDir 下创建 Executor 专属目录（路径格式：workDir/appId/execId）</span><br><span class="line">          val executorDir = new File(workDir, appId + &quot;/&quot; + execId)</span><br><span class="line">          if (!executorDir.mkdirs()) &#123;</span><br><span class="line">//            若目录创建失败（如权限不足、磁盘已满），抛出 IOException，终止后续流程</span><br><span class="line">            throw new IOException(&quot;Failed to create directory &quot; + executorDir)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          // Create local dirs for the executor. These are passed to the executor via the</span><br><span class="line">          // SPARK_EXECUTOR_DIRS environment variable, and deleted by the Worker when the</span><br><span class="line">          // application finishes.</span><br><span class="line">          val appLocalDirs = appDirectories.getOrElse(appId, &#123;</span><br><span class="line">//            获取本地根目录：通过 Utils.getOrCreateLocalRootDirs(conf) 获取配置的本地存储目录（如 spark.local.dir 或系统临时目录）</span><br><span class="line">            val localRootDirs = Utils.getOrCreateLocalRootDirs(conf)</span><br><span class="line">            val dirs = localRootDirs.flatMap &#123; dir =&gt;</span><br><span class="line">              try &#123;</span><br><span class="line">//                创建子目录：在每个根目录下创建以 executor 为前缀的子目录（如 /tmp/spark-executor-1234）</span><br><span class="line">                val appDir = Utils.createDirectory(dir, namePrefix = &quot;executor&quot;)</span><br><span class="line">//                权限设置：调用 Utils.chmod700 设置目录权限为 rwx------（仅所有者可访问）</span><br><span class="line">                Utils.chmod700(appDir)</span><br><span class="line">                Some(appDir.getAbsolutePath())</span><br><span class="line">              &#125; catch &#123;</span><br><span class="line">                case e: IOException =&gt;</span><br><span class="line">                  logWarning(s&quot;$&#123;e.getMessage&#125;. Ignoring this directory.&quot;)</span><br><span class="line">                  None</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;.toSeq</span><br><span class="line">            if (dirs.isEmpty) &#123;</span><br><span class="line">              throw new IOException(&quot;No subfolder can be created in &quot; +</span><br><span class="line">                s&quot;$&#123;localRootDirs.mkString(&quot;,&quot;)&#125;.&quot;)</span><br><span class="line">            &#125;</span><br><span class="line">            dirs</span><br><span class="line">          &#125;)</span><br><span class="line">//          缓存目录列表：将目录列表存入 appDirectories 映射（键为 appId），供后续 Executor 复用</span><br><span class="line">          appDirectories(appId) = appLocalDirs</span><br><span class="line">          /**</span><br><span class="line">           * appId, execId：标识 Executor 归属</span><br><span class="line">           * appDesc.copy(...)：更新应用启动命令的 SSL 配置（如启用 HTTPS）</span><br><span class="line">           * cores_, memory_, resources_：资源配额</span><br><span class="line">           * self：Worker 的 RPC 端点引用（用于 Executor 状态回调）</span><br><span class="line">           * webUi 相关参数：Worker Web UI 的协议、端口、主机信息</span><br><span class="line">           * executorDir：Executor 工作目录路径</span><br><span class="line">           * appLocalDirs：本地存储目录列表（通过 SPARK_EXECUTOR_DIRS 环境变量传递给 Executor）</span><br><span class="line">           * ExecutorState.LAUNCHING：初始状态标记</span><br><span class="line">           */</span><br><span class="line">          val manager = new ExecutorRunner(</span><br><span class="line">            appId,</span><br><span class="line">            execId,</span><br><span class="line">            appDesc.copy(command = Worker.maybeUpdateSSLSettings(appDesc.command, conf)),</span><br><span class="line">            cores_,</span><br><span class="line">            memory_,</span><br><span class="line">            self,</span><br><span class="line">            workerId,</span><br><span class="line">            webUi.scheme,</span><br><span class="line">            host,</span><br><span class="line">            webUi.boundPort,</span><br><span class="line">            publicAddress,</span><br><span class="line">            sparkHome,</span><br><span class="line">            executorDir,</span><br><span class="line">            workerUri,</span><br><span class="line">            conf,</span><br><span class="line">            appLocalDirs,</span><br><span class="line">            ExecutorState.LAUNCHING,</span><br><span class="line">            resources_)</span><br><span class="line">//          将 ExecutorRunner 实例存入 executors 映射表（键为 appId/execId）</span><br><span class="line">          executors(appId + &quot;/&quot; + execId) = manager</span><br><span class="line">//          调用 start() 方法启动 ExecutorRunner，其内部会通过 ProcessBuilder 启动 JVM 进程执行 org.apache.spark.executor.CoarseGrainedExecutorBackend</span><br><span class="line">          manager.start()</span><br><span class="line">//          累加 Worker 已分配的 CPU 核心数（coresUsed）</span><br><span class="line">          coresUsed += cores_</span><br><span class="line">//          累加已分配的内存（memoryUsed）</span><br><span class="line">          memoryUsed += memory_</span><br><span class="line">//          调用 addResourcesUsed 更新其他资源（如 GPU）的使用量</span><br><span class="line">          addResourcesUsed(resources_)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: Exception =&gt;</span><br><span class="line">            logError(s&quot;Failed to launch executor $appId/$execId for $&#123;appDesc.name&#125;.&quot;, e)</span><br><span class="line">//            若 ExecutorRunner 已创建，调用 kill() 终止进程并从 executors 映射中移除</span><br><span class="line">            if (executors.contains(appId + &quot;/&quot; + execId)) &#123;</span><br><span class="line">              executors(appId + &quot;/&quot; + execId).kill()</span><br><span class="line">              executors -= appId + &quot;/&quot; + execId</span><br><span class="line">            &#125;</span><br><span class="line">//            通过 syncExecutorStateWithMaster 向 Master 报告 Executor 状态为 FAILED，并附带异常信息</span><br><span class="line">            syncExecutorStateWithMaster(ExecutorStateChanged(appId, execId, ExecutorState.FAILED,</span><br><span class="line">              Some(e.toString), None))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析"><a href="#（2）代码解析" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>代码路径为：org.apache.spark.deploy.worker.Worker#receive</li>
<li>在 Worker 的 workDir 下创建 Executor 专属目录（路径格式：workDir&#x2F;appId&#x2F;execId）</li>
<li>调用 start() 方法启动 ExecutorRunner，其内部会通过 ProcessBuilder 启动 JVM 进程执行 org.apache.spark.executor.CoarseGrainedExecutorBackend</li>
</ul>
<h3 id="2，调用-ExecutorRunner-函数里面的-start-方法"><a href="#2，调用-ExecutorRunner-函数里面的-start-方法" class="headerlink" title="2，调用 ExecutorRunner() 函数里面的 start() 方法"></a>2，调用 ExecutorRunner() 函数里面的 start() 方法</h3><h4 id="（1）代码-1"><a href="#（1）代码-1" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line">   private[worker] def start(): Unit = &#123;</span><br><span class="line">    workerThread = new Thread(&quot;ExecutorRunner for &quot; + fullId) &#123;</span><br><span class="line">      override def run(): Unit = &#123; fetchAndRunExecutor() &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    workerThread.start()</span><br><span class="line">    // Shutdown hook that kills actors on shutdown.</span><br><span class="line">    shutdownHook = ShutdownHookManager.addShutdownHook &#123; () =&gt;</span><br><span class="line">      // It&#x27;s possible that we arrive here before calling `fetchAndRunExecutor`, then `state` will</span><br><span class="line">      // be `ExecutorState.LAUNCHING`. In this case, we should set `state` to `FAILED`.</span><br><span class="line">      if (state == ExecutorState.LAUNCHING || state == ExecutorState.RUNNING) &#123;</span><br><span class="line">        state = ExecutorState.FAILED</span><br><span class="line">      &#125;</span><br><span class="line">      killProcess(Some(&quot;Worker shutting down&quot;)) &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  /**</span><br><span class="line">   * Download and run the executor described in our ApplicationDescription</span><br><span class="line">   */</span><br><span class="line">  private def fetchAndRunExecutor(): Unit = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      /**</span><br><span class="line">       * 1. 函数入口与资源准备</span><br><span class="line">       * 作用：</span><br><span class="line">       *  调用 prepareResourcesFile 方法，生成 Executor 所需的资源文件（如用户通过 --files 提交的文件），返回资源文件的路径（可选）。</span><br><span class="line">       * 参数：</span><br><span class="line">       *  SPARK_EXECUTOR_PREFIX：资源文件名前缀（固定为 &quot;executor&quot;）。</span><br><span class="line">       *  resources：用户通过 spark-submit --files 提交的资源列表（例如 List(&quot;data.txt&quot;, &quot;config.json&quot;)）。</span><br><span class="line">       *  executorDir：Executor 的工作目录路径（例如 /tmp/spark-worker/app-20231001-001/executor-1）。</span><br><span class="line">       * 返回值：</span><br><span class="line">       *  若资源列表非空，返回 Some(File)，否则 None。例如，资源文件路径可能为 /tmp/spark-worker/.../executor-resources.json。</span><br><span class="line">       */</span><br><span class="line">      val resourceFileOpt = prepareResourcesFile(SPARK_EXECUTOR_PREFIX, resources, executorDir)</span><br><span class="line">      // Launch the process</span><br><span class="line">//			启动executor的核心启动命令</span><br><span class="line">      /**</span><br><span class="line">       * 2. 构造启动命令参数</span><br><span class="line">       * 作用：</span><br><span class="line">       *  合并基础命令参数和资源文件参数。</span><br><span class="line">       * 变量值：</span><br><span class="line">       *  appDesc.command.arguments：从应用程序描述中获取的基础命令参数（例如 Seq(&quot;--class&quot;, &quot;org.example.Main&quot;, &quot;app.jar&quot;)）。</span><br><span class="line">       *  resourceFileOpt.map(...)：若存在资源文件，添加 --resourcesFile &lt;路径&gt; 参数（例如 Seq(&quot;--resourcesFile&quot;, &quot;/tmp/executor-resources.json&quot;)）。</span><br><span class="line">       * 结果：</span><br><span class="line">       *  arguments 可能是 Seq(&quot;--class&quot;, &quot;org.example.Main&quot;, &quot;app.jar&quot;, &quot;--resourcesFile&quot;, &quot;...&quot;)。</span><br><span class="line">       */</span><br><span class="line">      val arguments = appDesc.command.arguments ++ resourceFileOpt.map(f =&gt;</span><br><span class="line">        Seq(&quot;--resourcesFile&quot;, f.getAbsolutePath)).getOrElse(Seq.empty)</span><br><span class="line">      /**</span><br><span class="line">       * 3. 替换 Java 虚拟机参数占位符</span><br><span class="line">       * 作用：</span><br><span class="line">       *  替换 Java 虚拟机参数中的占位符 &#123;&#123;APP_ID&#125;&#125; 和 &#123;&#123;EXECUTOR_ID&#125;&#125; 为实际值。</span><br><span class="line">       * 变量值：</span><br><span class="line">       *  appDesc.command.javaOpts：用户指定的 JVM 参数（例如 Seq(&quot;-Dapp.id=&#123;&#123;APP_ID&#125;&#125;&quot;, &quot;-Xmx4g&quot;)）。</span><br><span class="line">       *  appId：应用程序 ID（例如 &quot;app-20231001-001&quot;）。</span><br><span class="line">       *  execId：Executor ID（例如 1）。</span><br><span class="line">       * 结果：</span><br><span class="line">       *  subsOpts 变为 Seq(&quot;-Dapp.id=app-20231001-001&quot;, &quot;-Xmx4g&quot;)。</span><br><span class="line">       */</span><br><span class="line">      val subsOpts = appDesc.command.javaOpts.map &#123;</span><br><span class="line">        Utils.substituteAppNExecIds(_, appId, execId.toString)</span><br><span class="line">      &#125;</span><br><span class="line">      /**</span><br><span class="line">       * 4. 构造最终命令</span><br><span class="line">       * 作用：</span><br><span class="line">       *  复制原始命令对象，替换参数和 JVM 选项为处理后的值。</span><br><span class="line">       * 变量值：</span><br><span class="line">       *  appDesc.command：原始命令对象（包含主类、JAR 路径等）。</span><br><span class="line">       *  arguments：合并后的命令行参数。</span><br><span class="line">       *  subsOpts：替换占位符后的 JVM 参数。</span><br><span class="line">       * 结果：</span><br><span class="line">       *  生成一个完整的 Command 对象，用于启动 Executor 进程。</span><br><span class="line">       */</span><br><span class="line">      val subsCommand = appDesc.command.copy(arguments = arguments, javaOpts = subsOpts)</span><br><span class="line">      /**</span><br><span class="line">       * 5. 构建进程启动器</span><br><span class="line">       * 作用：</span><br><span class="line">       *  使用 CommandUtils 工具类构造 ProcessBuilder 对象，用于启动 JVM 进程。</span><br><span class="line">       * 参数：</span><br><span class="line">       *  subsCommand：处理后的命令对象。</span><br><span class="line">       *  SecurityManager：安全管理器，控制资源访问权限。</span><br><span class="line">       *  memory：Executor 的内存限制（例如 4096 MB）。</span><br><span class="line">       *  sparkHome：Spark 安装目录（例如 /opt/spark）。</span><br><span class="line">       *  substituteVariables：是否替换环境变量占位符（默认为 true）。</span><br><span class="line">       * 结果：</span><br><span class="line">       *  生成一个配置好的 ProcessBuilder，用于启动 CoarseGrainedExecutorBackend 进程。</span><br><span class="line">       */</span><br><span class="line">      val builder = CommandUtils.buildProcessBuilder(subsCommand, new SecurityManager(conf),</span><br><span class="line">        memory, sparkHome.getAbsolutePath, substituteVariables)</span><br><span class="line">      val command = builder.command()</span><br><span class="line">      val redactedCommand = Utils.redactCommandLineArgs(conf, command.asScala.toSeq)</span><br><span class="line">        .mkString(&quot;\&quot;&quot;, &quot;\&quot; \&quot;&quot;, &quot;\&quot;&quot;)</span><br><span class="line">      logInfo(s&quot;Launch command: $redactedCommand&quot;)</span><br><span class="line">      /**</span><br><span class="line">       * 作用：</span><br><span class="line">       *  工作目录：设置进程的工作目录为 executorDir（例如 /tmp/spark-worker/.../executor-1）。</span><br><span class="line">       *  本地目录：将 appLocalDirs（Executor 的本地存储目录列表）通过环境变量 SPARK_EXECUTOR_DIRS 传递（例如 &quot;/tmp/spark-local-1:/tmp/spark-local-2&quot;）。</span><br><span class="line">       *  避免 Scala 父进程：设置 SPARK_LAUNCH_WITH_SCALA=0，防止在 Spark Shell 中启动多余的 Scala 进程。</span><br><span class="line">       */</span><br><span class="line">      builder.directory(executorDir)</span><br><span class="line">      builder.environment.put(&quot;SPARK_EXECUTOR_DIRS&quot;, appLocalDirs.mkString(File.pathSeparator))</span><br><span class="line">      // In case we are running this from within the Spark Shell, avoid creating a &quot;scala&quot;</span><br><span class="line">      // parent process for the executor command</span><br><span class="line">      builder.environment.put(&quot;SPARK_LAUNCH_WITH_SCALA&quot;, &quot;0&quot;)</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">       * 作用：</span><br><span class="line">       *  生成标准输出和错误输出的 Web UI 访问链接，供用户查看日志。</span><br><span class="line">       *  变量值：</span><br><span class="line">       *  workerId：Worker 节点的 ID（例如 worker-20231001-001）。</span><br><span class="line">       *  webUiScheme：Web UI 的协议（http:// 或 https://）。</span><br><span class="line">       *  publicAddress：Worker 的公网 IP（例如 192.168.1.100）。</span><br><span class="line">       *  webUiPort：Worker 的 Web UI 端口（例如 8081）。</span><br><span class="line">       *</span><br><span class="line">       *  结果：</span><br><span class="line">       *  SPARK_LOG_URL_STDOUT 可能为 http://192.168.1.100:8081/logPage/?appId=app-001&amp;executorId=1&amp;logType=stdout。</span><br><span class="line">       */</span><br><span class="line">      // Add webUI log urls</span><br><span class="line">      val baseUrl =</span><br><span class="line">        if (conf.get(UI_REVERSE_PROXY)) &#123;</span><br><span class="line">          conf.get(UI_REVERSE_PROXY_URL.key, &quot;&quot;).stripSuffix(&quot;/&quot;) +</span><br><span class="line">            s&quot;/proxy/$workerId/logPage/?appId=$appId&amp;executorId=$execId&amp;logType=&quot;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          s&quot;$webUiScheme$publicAddress:$webUiPort/logPage/?appId=$appId&amp;executorId=$execId&amp;logType=&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      builder.environment.put(&quot;SPARK_LOG_URL_STDERR&quot;, s&quot;$&#123;baseUrl&#125;stderr&quot;)</span><br><span class="line">      builder.environment.put(&quot;SPARK_LOG_URL_STDOUT&quot;, s&quot;$&#123;baseUrl&#125;stdout&quot;)</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">       * 作用：</span><br><span class="line">       *  启动 Executor 进程（CoarseGrainedExecutorBackend）。</span><br><span class="line">       *  将标准输出和错误输出重定向到 stdout 和 stderr 文件。</span><br><span class="line">       *</span><br><span class="line">       *  变量值：</span><br><span class="line">       *  redactedCommand：脱敏后的命令行参数（隐藏敏感信息如密码）。</span><br><span class="line">       *  stdout 和 stderr 文件路径（例如 /tmp/.../executor-1/stdout）。</span><br><span class="line">       */</span><br><span class="line">      process = builder.start()</span><br><span class="line">      val header = &quot;Spark Executor Command: %s\n%s\n\n&quot;.format(</span><br><span class="line">        redactedCommand, &quot;=&quot; * 40)</span><br><span class="line"></span><br><span class="line">      // Redirect its stdout and stderr to files</span><br><span class="line">      val stdout = new File(executorDir, &quot;stdout&quot;)</span><br><span class="line">      stdoutAppender = FileAppender(process.getInputStream, stdout, conf, true)</span><br><span class="line"></span><br><span class="line">      val stderr = new File(executorDir, &quot;stderr&quot;)</span><br><span class="line">      Files.write(header, stderr, StandardCharsets.UTF_8)</span><br><span class="line">      stderrAppender = FileAppender(process.getErrorStream, stderr, conf, true)</span><br><span class="line">      /**</span><br><span class="line">       * 作用：</span><br><span class="line">       *  将 Executor 状态设为 RUNNING，通知 Worker。</span><br><span class="line">       *  阻塞等待进程退出，获取退出码。</span><br><span class="line">       *  更新状态为 EXITED，通知 Worker 退出原因和码。</span><br><span class="line">       *</span><br><span class="line">       * 变量值：</span><br><span class="line">       *  exitCode：进程退出码（例如 0 表示正常退出，137 表示被终止）。</span><br><span class="line">       *</span><br><span class="line">       *  1. Executor 进程的生命周期</span><br><span class="line">       *  Executor 进程（即 CoarseGrainedExecutorBackend）是长期运行的，其核心职责是接收 Driver 分配的任务并执行。只要应用程序在运行，Executor 进程会一直存活，直到以下情况发生：</span><br><span class="line">       * Driver 主动关闭 Executor（如应用程序结束、动态资源调整）。</span><br><span class="line">       * Executor 自身异常崩溃（如 OOM、网络断开、代码错误）。</span><br><span class="line">       * 2. process.waitFor() 的作用</span><br><span class="line">       * 该方法是同步阻塞的，但阻塞的 不是等待任务完成，而是 等待 Executor 进程本身的终止。</span><br><span class="line">       * 如果 Executor 正常启动并运行，waitFor() 会一直阻塞，直到 Driver 或外部因素（如资源回收）终止进程。</span><br><span class="line">       * 如果 Executor 启动失败（如 JVM 参数错误），waitFor() 会立即返回，进程快速退出。</span><br><span class="line">       */</span><br><span class="line">      state = ExecutorState.RUNNING</span><br><span class="line">      worker.send(ExecutorStateChanged(appId, execId, state, None, None))</span><br><span class="line">      // Wait for it to exit; executor may exit with code 0 (when driver instructs it to shutdown)</span><br><span class="line">      // or with nonzero exit code</span><br><span class="line">//      waitFor() 是等待 进程终止</span><br><span class="line">//      Executor 启动后，状态立即标记为 RUNNING（进程存在）。</span><br><span class="line">//      waitFor() 阻塞直到进程终止（正常或异常），然后标记为 EXITED</span><br><span class="line">      val exitCode = process.waitFor()</span><br><span class="line">      state = ExecutorState.EXITED</span><br><span class="line">      val message = &quot;Command exited with code &quot; + exitCode</span><br><span class="line">      worker.send(ExecutorStateChanged(appId, execId, state, Some(message), Some(exitCode)))</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      /**</span><br><span class="line">       * 作用：</span><br><span class="line">       *  中断异常：标记 Executor 为 KILLED，终止进程。</span><br><span class="line">       *  其他异常：标记为 FAILED，记录错误信息并终止进程。</span><br><span class="line">       * 日志示例：</span><br><span class="line">       *  INFO ExecutorRunner: Runner thread for executor app-001/1 interrupted</span><br><span class="line">       *  ERROR ExecutorRunner: Error running executor - java.io.IOException: Cannot run program...</span><br><span class="line">       */</span><br><span class="line">      case interrupted: InterruptedException =&gt;</span><br><span class="line">        logInfo(&quot;Runner thread for executor &quot; + fullId + &quot; interrupted&quot;)</span><br><span class="line">        state = ExecutorState.KILLED</span><br><span class="line">        killProcess(None)</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        logError(&quot;Error running executor&quot;, e)</span><br><span class="line">        state = ExecutorState.FAILED</span><br><span class="line">        killProcess(Some(e.toString))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析-1"><a href="#（2）代码解析-1" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>代码路径为：org.apache.spark.deploy.worker.ExecutorRunner#fetchAndRunExecutor</li>
<li>构造启动的命令参数和最终的命令</li>
<li>使用 builder.start() 启动 Executor 进程（CoarseGrainedExecutorBackend）</li>
</ul>
<h3 id="3，启动新的-Executor-进程（CoarseGrainedExecutorBackend）具体细节"><a href="#3，启动新的-Executor-进程（CoarseGrainedExecutorBackend）具体细节" class="headerlink" title="3，启动新的 Executor 进程（CoarseGrainedExecutorBackend）具体细节"></a>3，启动新的 Executor 进程（CoarseGrainedExecutorBackend）具体细节</h3><h4 id="（1）代码-2"><a href="#（1）代码-2" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"> * contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"> * this work for additional information regarding copyright ownership.</span><br><span class="line"> * The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"> * (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"> * the License.  You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> *    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> * See the License for the specific language governing permissions and</span><br><span class="line"> * limitations under the License.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">package org.apache.spark.executor</span><br><span class="line"></span><br><span class="line">import java.net.URL</span><br><span class="line">import java.nio.ByteBuffer</span><br><span class="line">import java.util.Locale</span><br><span class="line">import java.util.concurrent.atomic.AtomicBoolean</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable</span><br><span class="line">import scala.util.&#123;Failure, Success&#125;</span><br><span class="line">import scala.util.control.NonFatal</span><br><span class="line"></span><br><span class="line">import io.netty.util.internal.PlatformDependent</span><br><span class="line">import org.json4s.DefaultFormats</span><br><span class="line"></span><br><span class="line">import org.apache.spark._</span><br><span class="line">import org.apache.spark.TaskState.TaskState</span><br><span class="line">import org.apache.spark.deploy.SparkHadoopUtil</span><br><span class="line">import org.apache.spark.deploy.worker.WorkerWatcher</span><br><span class="line">import org.apache.spark.internal.Logging</span><br><span class="line">import org.apache.spark.internal.config._</span><br><span class="line">import org.apache.spark.resource.ResourceInformation</span><br><span class="line">import org.apache.spark.resource.ResourceProfile</span><br><span class="line">import org.apache.spark.resource.ResourceProfile._</span><br><span class="line">import org.apache.spark.resource.ResourceUtils._</span><br><span class="line">import org.apache.spark.rpc._</span><br><span class="line">import org.apache.spark.scheduler.&#123;ExecutorLossMessage, ExecutorLossReason, TaskDescription&#125;</span><br><span class="line">import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._</span><br><span class="line">import org.apache.spark.util.&#123;ChildFirstURLClassLoader, MutableURLClassLoader, SignalUtils, ThreadUtils, Utils&#125;</span><br><span class="line"></span><br><span class="line">private[spark] class CoarseGrainedExecutorBackend(</span><br><span class="line">    override val rpcEnv: RpcEnv,</span><br><span class="line">    driverUrl: String,</span><br><span class="line">    executorId: String,</span><br><span class="line">    bindAddress: String,</span><br><span class="line">    hostname: String,</span><br><span class="line">    cores: Int,</span><br><span class="line">    env: SparkEnv,</span><br><span class="line">    resourcesFileOpt: Option[String],</span><br><span class="line">    resourceProfile: ResourceProfile)</span><br><span class="line">  extends IsolatedRpcEndpoint with ExecutorBackend with Logging &#123;</span><br><span class="line"></span><br><span class="line">  import CoarseGrainedExecutorBackend._</span><br><span class="line"></span><br><span class="line">  private implicit val formats = DefaultFormats</span><br><span class="line"></span><br><span class="line">  private[spark] val stopping = new AtomicBoolean(false)</span><br><span class="line">  var executor: Executor = null</span><br><span class="line">  @volatile var driver: Option[RpcEndpointRef] = None</span><br><span class="line"></span><br><span class="line">  private var _resources = Map.empty[String, ResourceInformation]</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Map each taskId to the information about the resource allocated to it, Please refer to</span><br><span class="line">   * [[ResourceInformation]] for specifics.</span><br><span class="line">   * Exposed for testing only.</span><br><span class="line">   */</span><br><span class="line">  private[executor] val taskResources = new mutable.HashMap[Long, Map[String, ResourceInformation]]</span><br><span class="line"></span><br><span class="line">  private var decommissioned = false</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * 核心流程总结</span><br><span class="line">   * 优雅下线准备：注册信号处理器，支持通过外部信号触发安全下线。</span><br><span class="line">   * 环境校验：确保直接内存满足远程数据传输需求。</span><br><span class="line">   * 资源加载：解析分配的硬件资源文件，验证资源合法性。</span><br><span class="line">   * Driver 注册：建立与 Driver 的双向通信，上报 Executor 资源信息，完成注册流程。</span><br><span class="line">   * 状态同步：通过 RPC 消息机制，确保 Driver 知晓 Executor 的存活状态。</span><br><span class="line">   * 关键设计要点</span><br><span class="line">   * 异步非阻塞：使用 flatMap 和 onComplete 实现异步注册，避免阻塞 RPC 线程。</span><br><span class="line">   * 资源隔离：通过 _resources 字段隔离不同 Executor 的硬件资源，支持异构计算。</span><br><span class="line">   * 容错机制：注册失败时立即终止进程，防止无效 Executor 占用集群资源。</span><br><span class="line">   * 生命周期管理：RegisteredExecutor 消息触发后续初始化，确保 Executor 完全就绪后才接收任务。</span><br><span class="line">   */</span><br><span class="line">  override def onStart(): Unit = &#123;</span><br><span class="line">//    配置检查：若启用了 Executor 优雅下线功能（spark.decommission.enabled=true），获取配置的触发信号（如 SIGPWR）。</span><br><span class="line">    if (env.conf.get(DECOMMISSION_ENABLED)) &#123;</span><br><span class="line">//      信号注册：注册该信号的处理器，当收到信号时，向自身发送 ExecutorDecommissionSigReceived 消息，触发优雅下线流程（如释放资源、停止任务等）</span><br><span class="line">      val signal = env.conf.get(EXECUTOR_DECOMMISSION_SIGNAL)</span><br><span class="line">      logInfo(s&quot;Registering SIG$signal handler to trigger decommissioning.&quot;)</span><br><span class="line">      SignalUtils.register(signal, s&quot;Failed to register SIG$signal handler - disabling&quot; +</span><br><span class="line">        s&quot; executor decommission feature.&quot;) (self.askSync[Boolean](ExecutorDecommissionSigReceived))</span><br><span class="line">    &#125;</span><br><span class="line">//    输出日志，显示当前 Executor 正在连接的 Driver 地址（用于调试和监控）。</span><br><span class="line">    logInfo(&quot;Connecting to driver: &quot; + driverUrl)</span><br><span class="line">    try &#123;</span><br><span class="line">//      内存检查：若 Netty 使用直接内存（Direct Buffer）模式，且最大直接内存容量小于配置的 spark.maxRemoteBlockSizeFetchToMem（默认 1GB），抛出异常。</span><br><span class="line">//      设计意义：确保 Executor 能够处理大型远程数据块（避免因内存不足导致 OOM）</span><br><span class="line">      if (PlatformDependent.directBufferPreferred() &amp;&amp;</span><br><span class="line">          PlatformDependent.maxDirectMemory() &lt; env.conf.get(MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM)) &#123;</span><br><span class="line">        throw new SparkException(s&quot;Netty direct memory should at least be bigger than &quot; +</span><br><span class="line">          s&quot;&#x27;$&#123;MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM.key&#125;&#x27;, but got &quot; +</span><br><span class="line">          s&quot;$&#123;PlatformDependent.maxDirectMemory()&#125; bytes &lt; &quot; +</span><br><span class="line">          s&quot;$&#123;env.conf.get(MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM)&#125;&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">//      资源解析：通过 parseOrFindResources 解析资源文件（如 resourcesFileOpt 指定的文件路径），获取 Executor 分配到的硬件资源（如 GPU、FPGA 等）。</span><br><span class="line">      _resources = parseOrFindResources(resourcesFileOpt)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">//      异常处理：若解析失败（如资源文件损坏），调用 exitExecutor 终止进程，并向 Driver 报告失败状态。</span><br><span class="line">      case NonFatal(e) =&gt;</span><br><span class="line">        exitExecutor(1, &quot;Unable to create executor due to &quot; + e.getMessage, e)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * asyncSetupEndpointRefByURI 返回一个 Future[RpcEndpointRef]，表示异步获取 Driver 的 RPC 引用。</span><br><span class="line">     * 非阻塞地建立与 Driver 的连接，避免阻塞当前线程。</span><br><span class="line">     * flatMap 用于将多个异步操作串联。当前一个 Future 完成后，将其结果传递给下一个操作</span><br><span class="line">     * 保存 Driver 的引用 (driver = Some(ref))</span><br><span class="line">     * 绑定 ExecutorBackend 到环境 (env.executorBackend = Option(this))</span><br><span class="line">     * 发送注册请求 (ref.ask)</span><br><span class="line">     *</span><br><span class="line">     * onComplete 接受一个偏函数，处理 Future 的完成状态（成功或失败）</span><br><span class="line">     * 成功：向自身发送 RegisteredExecutor 消息，触发后续初始化。</span><br><span class="line">     * 失败：终止 Executor 进程并记录错误。</span><br><span class="line">     *</span><br><span class="line">     * ThreadUtils.sameThread</span><br><span class="line">     * 指定回调的执行上下文（线程池）。</span><br><span class="line">     * 强制 flatMap 和 onComplete 在 当前线程（即 RPC 线程）执行，避免线程切换开销。</span><br><span class="line">     *</span><br><span class="line">     * 关键设计思想</span><br><span class="line">     *  异步非阻塞：通过 Future 避免线程阻塞，提高吞吐量。</span><br><span class="line">     *  链式操作：使用 flatMap 串联依赖的异步操作。</span><br><span class="line">     *  线程控制：指定 sameThread 避免不必要的上下文切换。</span><br><span class="line">     *  容错处理：通过 onComplete 统一处理成功/失败逻辑。</span><br><span class="line">     */</span><br><span class="line">    rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap &#123; ref =&gt;</span><br><span class="line">      // This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span><br><span class="line">      // 获取 Driver 的 RPC 端点引用</span><br><span class="line">      driver = Some(ref)</span><br><span class="line">      // 将当前 Backend 绑定到 SparkEnv</span><br><span class="line">      env.executorBackend = Option(this)</span><br><span class="line">      // 向 Driver 发送注册请求</span><br><span class="line">      ref.ask[Boolean](RegisterExecutor(executorId,</span><br><span class="line">        self,    // Executor 自身的 RPC 端点</span><br><span class="line">        hostname,</span><br><span class="line">        cores,</span><br><span class="line">        extractLogUrls,  // 日志 URL（用于 Web UI）</span><br><span class="line">        extractAttributes,  // Executor 属性（如节点标签）</span><br><span class="line">        _resources,   // 分配的资源（如 GPU）</span><br><span class="line">        resourceProfile.id   // 资源配置文件 ID</span><br><span class="line">      ))</span><br><span class="line">    &#125;(ThreadUtils.sameThread).onComplete &#123;</span><br><span class="line">//      成功：向自身发送 RegisteredExecutor 消息，触发 Executor 的完全启动（如初始化 Task 执行器）。</span><br><span class="line">      case Success(_) =&gt;</span><br><span class="line">        self.send(RegisteredExecutor)</span><br><span class="line">//        失败：调用 exitExecutor 终止进程，并记录错误原因（如网络不可达、Driver 已下线）</span><br><span class="line">      case Failure(e) =&gt;</span><br><span class="line">        exitExecutor(1, s&quot;Cannot register with driver: $driverUrl&quot;, e, notifyDriver = false)</span><br><span class="line">    &#125;(ThreadUtils.sameThread)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create a classLoader for use for resource discovery. The user could provide a class</span><br><span class="line">   * as a substitute for the default one so we have to be able to load it from a user specified</span><br><span class="line">   * jar.</span><br><span class="line">   */</span><br><span class="line">  private def createClassLoader(): MutableURLClassLoader = &#123;</span><br><span class="line">    val currentLoader = Utils.getContextOrSparkClassLoader</span><br><span class="line">    val urls = getUserClassPath.toArray</span><br><span class="line">    if (env.conf.get(EXECUTOR_USER_CLASS_PATH_FIRST)) &#123;</span><br><span class="line">      new ChildFirstURLClassLoader(urls, currentLoader)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new MutableURLClassLoader(urls, currentLoader)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // visible for testing</span><br><span class="line">  def parseOrFindResources(resourcesFileOpt: Option[String]): Map[String, ResourceInformation] = &#123;</span><br><span class="line">    // use a classloader that includes the user classpath in case they specified a class for</span><br><span class="line">    // resource discovery</span><br><span class="line">    val urlClassLoader = createClassLoader()</span><br><span class="line">    logDebug(s&quot;Resource profile id is: $&#123;resourceProfile.id&#125;&quot;)</span><br><span class="line">    Utils.withContextClassLoader(urlClassLoader) &#123;</span><br><span class="line">      val resources = getOrDiscoverAllResourcesForResourceProfile(</span><br><span class="line">        resourcesFileOpt,</span><br><span class="line">        SPARK_EXECUTOR_PREFIX,</span><br><span class="line">        resourceProfile,</span><br><span class="line">        env.conf)</span><br><span class="line">      logResourceInfo(SPARK_EXECUTOR_PREFIX, resources)</span><br><span class="line">      resources</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getUserClassPath: Seq[URL] = Nil</span><br><span class="line"></span><br><span class="line">  def extractLogUrls: Map[String, String] = &#123;</span><br><span class="line">    val prefix = &quot;SPARK_LOG_URL_&quot;</span><br><span class="line">    sys.env.filterKeys(_.startsWith(prefix))</span><br><span class="line">      .map(e =&gt; (e._1.substring(prefix.length).toLowerCase(Locale.ROOT), e._2)).toMap</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def extractAttributes: Map[String, String] = &#123;</span><br><span class="line">    val prefix = &quot;SPARK_EXECUTOR_ATTRIBUTE_&quot;</span><br><span class="line">    sys.env.filterKeys(_.startsWith(prefix))</span><br><span class="line">      .map(e =&gt; (e._1.substring(prefix.length).toUpperCase(Locale.ROOT), e._2)).toMap</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def notifyDriverAboutPushCompletion(shuffleId: Int, shuffleMergeId: Int, mapIndex: Int): Unit = &#123;</span><br><span class="line">    val msg = ShufflePushCompletion(shuffleId, shuffleMergeId, mapIndex)</span><br><span class="line">    driver.foreach(_.send(msg))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def receive: PartialFunction[Any, Unit] = &#123;</span><br><span class="line">    case RegisteredExecutor =&gt;</span><br><span class="line">      // 1. 日志记录注册成功</span><br><span class="line">      logInfo(&quot;Successfully registered with driver&quot;)</span><br><span class="line">      try &#123;</span><br><span class="line">        // 2. 创建实际的任务执行器（Executor）</span><br><span class="line">        executor = new Executor(</span><br><span class="line">          executorId,  // Executor 的唯一标识（如 &quot;0&quot;）</span><br><span class="line">          hostname,  // Executor 所在节点的主机名</span><br><span class="line">          env,  // SparkEnv（包含 RpcEnv、序列化器、BlockManager 等）</span><br><span class="line">          getUserClassPath,  // 用户类路径（用于加载用户代码的 JAR 包和依赖）</span><br><span class="line">          isLocal = false,   // 标记是否为本地模式（此处为集群模式）</span><br><span class="line">          resources = _resources  // 分配的硬件资源（如 GPU、FPGA）</span><br><span class="line">        )</span><br><span class="line">//          接收方：org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint.receive</span><br><span class="line">        // 3. 通知 Driver Executor 已启动</span><br><span class="line">//        driver.get：Driver 的 RPC 端点引用（通过 onStart() 中的注册流程获取）</span><br><span class="line">//        LaunchedExecutor 消息：向 Driver 发送此消息，告知其 Executor 已成功创建并准备好接收任务。</span><br><span class="line">//        后续流程：Driver 收到 LaunchedExecutor 后，会将该 Executor 加入资源池，后续将向其分配 Task。</span><br><span class="line">        driver.get.send(LaunchedExecutor(executorId))</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        // 3. 通知 Driver Executor 已启动</span><br><span class="line">        case NonFatal(e) =&gt;</span><br><span class="line">          exitExecutor(1, &quot;Unable to create executor due to &quot; + e.getMessage, e)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    case LaunchTask(data) =&gt;</span><br><span class="line">      if (executor == null) &#123;</span><br><span class="line">        exitExecutor(1, &quot;Received LaunchTask command but executor was null&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        val taskDesc = TaskDescription.decode(data.value)</span><br><span class="line">        logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)</span><br><span class="line">        taskResources(taskDesc.taskId) = taskDesc.resources</span><br><span class="line">        executor.launchTask(this, taskDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    case KillTask(taskId, _, interruptThread, reason) =&gt;</span><br><span class="line">      if (executor == null) &#123;</span><br><span class="line">        exitExecutor(1, &quot;Received KillTask command but executor was null&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        executor.killTask(taskId, interruptThread, reason)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    case StopExecutor =&gt;</span><br><span class="line">      stopping.set(true)</span><br><span class="line">      logInfo(&quot;Driver commanded a shutdown&quot;)</span><br><span class="line">      // Cannot shutdown here because an ack may need to be sent back to the caller. So send</span><br><span class="line">      // a message to self to actually do the shutdown.</span><br><span class="line">      self.send(Shutdown)</span><br><span class="line"></span><br><span class="line">    case Shutdown =&gt;</span><br><span class="line">      stopping.set(true)</span><br><span class="line">      new Thread(&quot;CoarseGrainedExecutorBackend-stop-executor&quot;) &#123;</span><br><span class="line">        override def run(): Unit = &#123;</span><br><span class="line">          // `executor` can be null if there&#x27;s any error in `CoarseGrainedExecutorBackend.onStart`</span><br><span class="line">          // or fail to create `Executor`.</span><br><span class="line">          if (executor == null) &#123;</span><br><span class="line">            System.exit(1)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            // executor.stop() will call `SparkEnv.stop()` which waits until RpcEnv stops totally.</span><br><span class="line">            // However, if `executor.stop()` runs in some thread of RpcEnv, RpcEnv won&#x27;t be able to</span><br><span class="line">            // stop until `executor.stop()` returns, which becomes a dead-lock (See SPARK-14180).</span><br><span class="line">            // Therefore, we put this line in a new thread.</span><br><span class="line">            executor.stop()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line"></span><br><span class="line">    case UpdateDelegationTokens(tokenBytes) =&gt;</span><br><span class="line">      logInfo(s&quot;Received tokens of $&#123;tokenBytes.length&#125; bytes&quot;)</span><br><span class="line">      SparkHadoopUtil.get.addDelegationTokens(tokenBytes, env.conf)</span><br><span class="line"></span><br><span class="line">    case DecommissionExecutor =&gt;</span><br><span class="line">      decommissionSelf()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = &#123;</span><br><span class="line">    case ExecutorDecommissionSigReceived =&gt;</span><br><span class="line">      var driverNotified = false</span><br><span class="line">      try &#123;</span><br><span class="line">        driver.foreach &#123; driverRef =&gt;</span><br><span class="line">          // Tell driver that we are starting decommissioning so it stops trying to schedule us</span><br><span class="line">          driverNotified = driverRef.askSync[Boolean](ExecutorDecommissioning(executorId))</span><br><span class="line">          if (driverNotified) decommissionSelf()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        case e: Exception =&gt;</span><br><span class="line">          if (driverNotified) &#123;</span><br><span class="line">            logError(&quot;Fail to decommission self (but driver has been notified).&quot;, e)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            logError(&quot;Fail to tell driver that we are starting decommissioning&quot;, e)</span><br><span class="line">          &#125;</span><br><span class="line">          decommissioned = false</span><br><span class="line">      &#125;</span><br><span class="line">      context.reply(decommissioned)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def onDisconnected(remoteAddress: RpcAddress): Unit = &#123;</span><br><span class="line">    if (stopping.get()) &#123;</span><br><span class="line">      logInfo(s&quot;Driver from $remoteAddress disconnected during shutdown&quot;)</span><br><span class="line">    &#125; else if (driver.exists(_.address == remoteAddress)) &#123;</span><br><span class="line">      exitExecutor(1, s&quot;Driver $remoteAddress disassociated! Shutting down.&quot;, null,</span><br><span class="line">        notifyDriver = false)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      logWarning(s&quot;An unknown ($remoteAddress) driver disconnected.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit = &#123;</span><br><span class="line">    val resources = taskResources.getOrElse(taskId, Map.empty[String, ResourceInformation])</span><br><span class="line">    val msg = StatusUpdate(executorId, taskId, state, data, resources)</span><br><span class="line">    if (TaskState.isFinished(state)) &#123;</span><br><span class="line">      taskResources.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">    driver match &#123;</span><br><span class="line">      case Some(driverRef) =&gt; driverRef.send(msg)</span><br><span class="line">      case None =&gt; logWarning(s&quot;Drop $msg because has not yet connected to driver&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * This function can be overloaded by other child classes to handle</span><br><span class="line">   * executor exits differently. For e.g. when an executor goes down,</span><br><span class="line">   * back-end may not want to take the parent process down.</span><br><span class="line">   */</span><br><span class="line">  protected def exitExecutor(code: Int,</span><br><span class="line">                             reason: String,</span><br><span class="line">                             throwable: Throwable = null,</span><br><span class="line">                             notifyDriver: Boolean = true) = &#123;</span><br><span class="line">    if (stopping.compareAndSet(false, true)) &#123;</span><br><span class="line">      val message = &quot;Executor self-exiting due to : &quot; + reason</span><br><span class="line">      if (throwable != null) &#123;</span><br><span class="line">        logError(message, throwable)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        if (code == 0) &#123;</span><br><span class="line">          logInfo(message)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          logError(message)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if (notifyDriver &amp;&amp; driver.nonEmpty) &#123;</span><br><span class="line">        driver.get.send(RemoveExecutor(executorId, new ExecutorLossReason(reason)))</span><br><span class="line">      &#125;</span><br><span class="line">      self.send(Shutdown)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      logInfo(&quot;Skip exiting executor since it&#x27;s been already asked to exit before.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def decommissionSelf(): Unit = &#123;</span><br><span class="line">    if (!env.conf.get(DECOMMISSION_ENABLED)) &#123;</span><br><span class="line">      logWarning(s&quot;Receive decommission request, but decommission feature is disabled.&quot;)</span><br><span class="line">      return</span><br><span class="line">    &#125; else if (decommissioned) &#123;</span><br><span class="line">      logWarning(s&quot;Executor $executorId already started decommissioning.&quot;)</span><br><span class="line">      return</span><br><span class="line">    &#125;</span><br><span class="line">    val msg = s&quot;Decommission executor $executorId.&quot;</span><br><span class="line">    logInfo(msg)</span><br><span class="line">    try &#123;</span><br><span class="line">      decommissioned = true</span><br><span class="line">      val migrationEnabled = env.conf.get(STORAGE_DECOMMISSION_ENABLED) &amp;&amp;</span><br><span class="line">        (env.conf.get(STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED) ||</span><br><span class="line">          env.conf.get(STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED))</span><br><span class="line">      if (migrationEnabled) &#123;</span><br><span class="line">        env.blockManager.decommissionBlockManager()</span><br><span class="line">      &#125; else if (env.conf.get(STORAGE_DECOMMISSION_ENABLED)) &#123;</span><br><span class="line">        logError(s&quot;Storage decommissioning attempted but neither &quot; +</span><br><span class="line">          s&quot;$&#123;STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED.key&#125; or &quot; +</span><br><span class="line">          s&quot;$&#123;STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED.key&#125; is enabled &quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      if (executor != null) &#123;</span><br><span class="line">        executor.decommission()</span><br><span class="line">      &#125;</span><br><span class="line">      // Shutdown the executor once all tasks are gone &amp; any configured migrations completed.</span><br><span class="line">      // Detecting migrations completion doesn&#x27;t need to be perfect and we want to minimize the</span><br><span class="line">      // overhead for executors that are not in decommissioning state as overall that will be</span><br><span class="line">      // more of the executors. For example, this will not catch a block which is already in</span><br><span class="line">      // the process of being put from a remote executor before migration starts. This trade-off</span><br><span class="line">      // is viewed as acceptable to minimize introduction of any new locking structures in critical</span><br><span class="line">      // code paths.</span><br><span class="line"></span><br><span class="line">      val shutdownThread = new Thread(&quot;wait-for-blocks-to-migrate&quot;) &#123;</span><br><span class="line">        override def run(): Unit = &#123;</span><br><span class="line">          var lastTaskRunningTime = System.nanoTime()</span><br><span class="line">          val sleep_time = 1000 // 1s</span><br><span class="line">          // This config is internal and only used by unit tests to force an executor</span><br><span class="line">          // to hang around for longer when decommissioned.</span><br><span class="line">          val initialSleepMillis = env.conf.getInt(</span><br><span class="line">            &quot;spark.test.executor.decommission.initial.sleep.millis&quot;, sleep_time)</span><br><span class="line">          if (initialSleepMillis &gt; 0) &#123;</span><br><span class="line">            Thread.sleep(initialSleepMillis)</span><br><span class="line">          &#125;</span><br><span class="line">          while (true) &#123;</span><br><span class="line">            logInfo(&quot;Checking to see if we can shutdown.&quot;)</span><br><span class="line">            if (executor == null || executor.numRunningTasks == 0) &#123;</span><br><span class="line">              if (migrationEnabled) &#123;</span><br><span class="line">                logInfo(&quot;No running tasks, checking migrations&quot;)</span><br><span class="line">                val (migrationTime, allBlocksMigrated) = env.blockManager.lastMigrationInfo()</span><br><span class="line">                // We can only trust allBlocksMigrated boolean value if there were no tasks running</span><br><span class="line">                // since the start of computing it.</span><br><span class="line">                if (allBlocksMigrated &amp;&amp; (migrationTime &gt; lastTaskRunningTime)) &#123;</span><br><span class="line">                  logInfo(&quot;No running tasks, all blocks migrated, stopping.&quot;)</span><br><span class="line">                  exitExecutor(0, ExecutorLossMessage.decommissionFinished, notifyDriver = true)</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                  logInfo(&quot;All blocks not yet migrated.&quot;)</span><br><span class="line">                &#125;</span><br><span class="line">              &#125; else &#123;</span><br><span class="line">                logInfo(&quot;No running tasks, no block migration configured, stopping.&quot;)</span><br><span class="line">                exitExecutor(0, ExecutorLossMessage.decommissionFinished, notifyDriver = true)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              logInfo(s&quot;Blocked from shutdown by $&#123;executor.numRunningTasks&#125; running tasks&quot;)</span><br><span class="line">              // If there is a running task it could store blocks, so make sure we wait for a</span><br><span class="line">              // migration loop to complete after the last task is done.</span><br><span class="line">              // Note: this is only advanced if there is a running task, if there</span><br><span class="line">              // is no running task but the blocks are not done migrating this does not</span><br><span class="line">              // move forward.</span><br><span class="line">              lastTaskRunningTime = System.nanoTime()</span><br><span class="line">            &#125;</span><br><span class="line">            Thread.sleep(sleep_time)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      shutdownThread.setDaemon(true)</span><br><span class="line">      shutdownThread.start()</span><br><span class="line"></span><br><span class="line">      logInfo(&quot;Will exit when finished decommissioning&quot;)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        decommissioned = false</span><br><span class="line">        logError(&quot;Unexpected error while decommissioning self&quot;, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private[spark] object CoarseGrainedExecutorBackend extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  // Message used internally to start the executor when the driver successfully accepted the</span><br><span class="line">  // registration request.</span><br><span class="line">  case object RegisteredExecutor</span><br><span class="line"></span><br><span class="line">  case class Arguments(</span><br><span class="line">      driverUrl: String,</span><br><span class="line">      executorId: String,</span><br><span class="line">      bindAddress: String,</span><br><span class="line">      hostname: String,</span><br><span class="line">      cores: Int,</span><br><span class="line">      appId: String,</span><br><span class="line">      workerUrl: Option[String],</span><br><span class="line">      resourcesFileOpt: Option[String],</span><br><span class="line">      resourceProfileId: Int)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val createFn: (RpcEnv, Arguments, SparkEnv, ResourceProfile) =&gt;</span><br><span class="line">      CoarseGrainedExecutorBackend = &#123; case (rpcEnv, arguments, env, resourceProfile) =&gt;</span><br><span class="line">      new CoarseGrainedExecutorBackend(rpcEnv, arguments.driverUrl, arguments.executorId,</span><br><span class="line">        arguments.bindAddress, arguments.hostname, arguments.cores,</span><br><span class="line">        env, arguments.resourcesFileOpt, resourceProfile)</span><br><span class="line">    &#125;</span><br><span class="line">    run(parseArguments(args, this.getClass.getCanonicalName.stripSuffix(&quot;$&quot;)), createFn)</span><br><span class="line">    System.exit(0)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def run(</span><br><span class="line">      arguments: Arguments,</span><br><span class="line">      backendCreateFn: (RpcEnv, Arguments, SparkEnv, ResourceProfile) =&gt;</span><br><span class="line">        CoarseGrainedExecutorBackend): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    Utils.initDaemon(log)</span><br><span class="line">//    在安全集群中，以 spark 用户身份运行后续代码，避免权限问题（例如访问 HDFS 文件）</span><br><span class="line">    SparkHadoopUtil.get.runAsSparkUser &#123; () =&gt;</span><br><span class="line">      // Debug code</span><br><span class="line">      Utils.checkHost(arguments.hostname)</span><br><span class="line"></span><br><span class="line">      // Bootstrap to fetch the driver&#x27;s Spark properties.</span><br><span class="line">      val executorConf = new SparkConf</span><br><span class="line">      /**</span><br><span class="line">       * 创建一个临时的 RPC 环境 fetcher，命名为 driverPropsFetcher。</span><br><span class="line">       * clientMode = true 表示此 RPC 客户端仅用于短期通信（获取配置后立即关闭）。</span><br><span class="line">       * numUsableCores = 0 表示不占用 CPU 核心（仅用于配置获取，无需处理计算任务）。</span><br><span class="line">       */</span><br><span class="line">      val fetcher = RpcEnv.create(</span><br><span class="line">        &quot;driverPropsFetcher&quot;,</span><br><span class="line">        arguments.bindAddress,</span><br><span class="line">        arguments.hostname,</span><br><span class="line">        -1,</span><br><span class="line">        executorConf,</span><br><span class="line">        new SecurityManager(executorConf),</span><br><span class="line">        numUsableCores = 0,</span><br><span class="line">        clientMode = true)</span><br><span class="line"></span><br><span class="line">      var driver: RpcEndpointRef = null</span><br><span class="line">      /**</span><br><span class="line">       * 尝试最多 3 次连接 Driver 的 RPC 端点（通过 arguments.driverUrl）。</span><br><span class="line">       * 若最后一次尝试仍失败，抛出异常终止 Executor 启动。</span><br><span class="line">       */</span><br><span class="line">      val nTries = 3</span><br><span class="line">      for (i &lt;- 0 until nTries if driver == null) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          driver = fetcher.setupEndpointRefByURI(arguments.driverUrl)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: Throwable =&gt; if (i == nTries - 1) &#123;</span><br><span class="line">            throw e</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      /**</span><br><span class="line">       * 向 Driver 发送同步请求 RetrieveSparkAppConfig，获取应用的完整配置（包括动态分配的资源和安全配置）。</span><br><span class="line">       * 合并 spark.app.id 到配置属性中。</span><br><span class="line">       * 关闭临时 RPC 环境 fetcher（配置已获取，不再需要此连接）。</span><br><span class="line">       */</span><br><span class="line">      val cfg = driver.askSync[SparkAppConfig](RetrieveSparkAppConfig(arguments.resourceProfileId))</span><br><span class="line">      val props = cfg.sparkProperties ++ Seq[(String, String)]((&quot;spark.app.id&quot;, arguments.appId))</span><br><span class="line">      fetcher.shutdown()</span><br><span class="line">      /**</span><br><span class="line">       * 将 Driver 返回的配置加载到 driverConf。</span><br><span class="line">       * 特殊处理 ExecutorStartupConf 类配置（如 SSL 相关参数），避免覆盖本地配置。</span><br><span class="line">       */</span><br><span class="line">      // Create SparkEnv using properties we fetched from the driver.</span><br><span class="line">      val driverConf = new SparkConf()</span><br><span class="line">      for ((key, value) &lt;- props) &#123;</span><br><span class="line">        // this is required for SSL in standalone mode</span><br><span class="line">        if (SparkConf.isExecutorStartupConf(key)) &#123;</span><br><span class="line">          driverConf.setIfMissing(key, value)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          driverConf.set(key, value)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">       * 在安全集群中，将 Hadoop 委托令牌（Delegation Tokens）添加到配置，使 Executor 能访问 HDFS 等受保护资源。</span><br><span class="line">       */</span><br><span class="line">      cfg.hadoopDelegationCreds.foreach &#123; tokens =&gt;</span><br><span class="line">        SparkHadoopUtil.get.addDelegationTokens(tokens, driverConf)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">       * 创建 Executor 的运行时环境 SparkEnv，包含以下核心组件：</span><br><span class="line">       * RpcEnv：Executor 的 RPC 通信环境。</span><br><span class="line">       * BlockManager：管理数据块存储。</span><br><span class="line">       * Serializer：任务数据的序列化组件。</span><br><span class="line">       * SecurityManager：安全管理器（处理 SSL 和认证）。</span><br><span class="line">       */</span><br><span class="line">      driverConf.set(EXECUTOR_ID, arguments.executorId)</span><br><span class="line">      val env = SparkEnv.createExecutorEnv(driverConf, arguments.executorId, arguments.bindAddress,</span><br><span class="line">        arguments.hostname, arguments.cores, cfg.ioEncryptionKey, isLocal = false)</span><br><span class="line">      // Set the application attemptId in the BlockStoreClient if available.</span><br><span class="line">      /**</span><br><span class="line">       * 如果应用启用了重试机制（如 YARN 的 Application Attempt），将尝试 ID 设置到块存储客户端，确保数据隔离。</span><br><span class="line">       */</span><br><span class="line">      val appAttemptId = env.conf.get(APP_ATTEMPT_ID)</span><br><span class="line">      appAttemptId.foreach(attemptId =&gt;</span><br><span class="line">        env.blockManager.blockStoreClient.setAppAttemptId(attemptId)</span><br><span class="line">      )</span><br><span class="line">      /**</span><br><span class="line">       * 通过工厂函数 backendCreateFn 创建 CoarseGrainedExecutorBackend 实例。</span><br><span class="line">       * 将此实例注册到 RpcEnv，命名为 Executor，使其能接收 Driver 的 RPC 消息（如 LaunchTask）。</span><br><span class="line">       */</span><br><span class="line">      val backend = backendCreateFn(env.rpcEnv, arguments, env, cfg.resourceProfile)</span><br><span class="line">      env.rpcEnv.setupEndpoint(&quot;Executor&quot;, backend)</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">       * 创建 WorkerWatcher 监听 Worker 的 RPC 端点。</span><br><span class="line">       * 如果 Worker 进程终止，WorkerWatcher 会触发 Executor 的优雅关闭。</span><br><span class="line">       */</span><br><span class="line">      arguments.workerUrl.foreach &#123; url =&gt;</span><br><span class="line">        env.rpcEnv.setupEndpoint(&quot;WorkerWatcher&quot;,</span><br><span class="line">          new WorkerWatcher(env.rpcEnv, url, isChildProcessStopping = backend.stopping))</span><br><span class="line">      &#125;</span><br><span class="line">//      阻塞当前线程，等待 RPC 环境终止（即 Executor 进程退出）</span><br><span class="line">      env.rpcEnv.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def parseArguments(args: Array[String], classNameForEntry: String): Arguments = &#123;</span><br><span class="line">    var driverUrl: String = null</span><br><span class="line">    var executorId: String = null</span><br><span class="line">    var bindAddress: String = null</span><br><span class="line">    var hostname: String = null</span><br><span class="line">    var cores: Int = 0</span><br><span class="line">    var resourcesFileOpt: Option[String] = None</span><br><span class="line">    var appId: String = null</span><br><span class="line">    var workerUrl: Option[String] = None</span><br><span class="line">    var resourceProfileId: Int = DEFAULT_RESOURCE_PROFILE_ID</span><br><span class="line"></span><br><span class="line">    var argv = args.toList</span><br><span class="line">    while (!argv.isEmpty) &#123;</span><br><span class="line">      argv match &#123;</span><br><span class="line">        case (&quot;--driver-url&quot;) :: value :: tail =&gt;</span><br><span class="line">          driverUrl = value</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--executor-id&quot;) :: value :: tail =&gt;</span><br><span class="line">          executorId = value</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--bind-address&quot;) :: value :: tail =&gt;</span><br><span class="line">          bindAddress = value</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--hostname&quot;) :: value :: tail =&gt;</span><br><span class="line">          hostname = value</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--cores&quot;) :: value :: tail =&gt;</span><br><span class="line">          cores = value.toInt</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--resourcesFile&quot;) :: value :: tail =&gt;</span><br><span class="line">          resourcesFileOpt = Some(value)</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--app-id&quot;) :: value :: tail =&gt;</span><br><span class="line">          appId = value</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--worker-url&quot;) :: value :: tail =&gt;</span><br><span class="line">          // Worker url is used in spark standalone mode to enforce fate-sharing with worker</span><br><span class="line">          workerUrl = Some(value)</span><br><span class="line">          argv = tail</span><br><span class="line">        case (&quot;--resourceProfileId&quot;) :: value :: tail =&gt;</span><br><span class="line">          resourceProfileId = value.toInt</span><br><span class="line">          argv = tail</span><br><span class="line">        case Nil =&gt;</span><br><span class="line">        case tail =&gt;</span><br><span class="line">          // scalastyle:off println</span><br><span class="line">          System.err.println(s&quot;Unrecognized options: $&#123;tail.mkString(&quot; &quot;)&#125;&quot;)</span><br><span class="line">          // scalastyle:on println</span><br><span class="line">          printUsageAndExit(classNameForEntry)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (hostname == null) &#123;</span><br><span class="line">      hostname = Utils.localHostName()</span><br><span class="line">      log.info(s&quot;Executor hostname is not provided, will use &#x27;$hostname&#x27; to advertise itself&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (driverUrl == null || executorId == null || cores &lt;= 0 || appId == null) &#123;</span><br><span class="line">      printUsageAndExit(classNameForEntry)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (bindAddress == null) &#123;</span><br><span class="line">      bindAddress = hostname</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Arguments(driverUrl, executorId, bindAddress, hostname, cores, appId, workerUrl,</span><br><span class="line">      resourcesFileOpt, resourceProfileId)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def printUsageAndExit(classNameForEntry: String): Unit = &#123;</span><br><span class="line">    // scalastyle:off println</span><br><span class="line">    System.err.println(</span><br><span class="line">      s&quot;&quot;&quot;</span><br><span class="line">      |Usage: $classNameForEntry [options]</span><br><span class="line">      |</span><br><span class="line">      | Options are:</span><br><span class="line">      |   --driver-url &lt;driverUrl&gt;</span><br><span class="line">      |   --executor-id &lt;executorId&gt;</span><br><span class="line">      |   --bind-address &lt;bindAddress&gt;</span><br><span class="line">      |   --hostname &lt;hostname&gt;</span><br><span class="line">      |   --cores &lt;cores&gt;</span><br><span class="line">      |   --resourcesFile &lt;fileWithJSONResourceInformation&gt;</span><br><span class="line">      |   --app-id &lt;appid&gt;</span><br><span class="line">      |   --worker-url &lt;workerUrl&gt;</span><br><span class="line">      |   --resourceProfileId &lt;id&gt;</span><br><span class="line">      |&quot;&quot;&quot;.stripMargin)</span><br><span class="line">    // scalastyle:on println</span><br><span class="line">    System.exit(1)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析-2"><a href="#（2）代码解析-2" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>main 函数里面，变量 createFn 为工厂函数</li>
<li>在 main 函数里面，调用 run 函数<ul>
<li>通过工厂函数 backendCreateFn 创建 CoarseGrainedExecutorBackend 实例</li>
<li>将此实例注册到 RpcEnv，命名为 Executor，使其能接收 Driver 的 RPC 消息（如 LaunchTask），同时也触发了 CoarseGrainedExecutorBackend 类里面的 onStart() 函数</li>
<li>创建 WorkerWatcher 监听 Worker 的 RPC 端点</li>
<li>如果 Worker 进程终止，WorkerWatcher 会触发 Executor 的优雅关闭</li>
</ul>
</li>
<li>调用 CoarseGrainedExecutorBackend 类里面 onStart() 函数<ul>
<li>通过 parseOrFindResources 解析资源文件（如 resourcesFileOpt 指定的文件路径），获取 Executor 分配到的硬件资源（如 GPU、FPGA 等）</li>
<li>asyncSetupEndpointRefByURI：返回一个 Future[RpcEndpointRef]，表示异步获取 Driver 的 RPC 引用，非阻塞地建立与 Driver 的连接，避免阻塞当前线程</li>
<li>flatMap：用于将多个异步操作串联，当前一个 Future 完成后，将其结果传递给下一个操作</li>
<li>onComplete：接受一个偏函数，处理 Future 的完成状态（成功或失败）。如果成功，向自身发送 RegisteredExecutor 消息，触发后续初始化。如果失败，终止 Executor 进程并记录错误</li>
<li>ThreadUtils.sameThread：指定回调的执行上下文（线程池），强制 flatMap 和 onComplete 在 当前线程（即 RPC 线程）执行，避免线程切换开销</li>
<li>向 Driver 发送注册请求成功后，self.send(RegisteredExecutor) 向自身发送 RegisteredExecutor 消息，触发 Executor 的完全启动（如初始化 Task 执行器）</li>
</ul>
</li>
<li>触发 case RegisteredExecutor 方法，创建实际的任务执行器（Executor），通知 Driver Executor 已经启动</li>
<li>LaunchedExecutor 消息，是向 Driver 发送消息，告知 Executor 已经成功创建并准备好接收任务</li>
<li>接收 LaunchedExecutor 消息的源代码路径为：org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#receive</li>
</ul>
<h3 id="4，详细分析-DriverEndpoint-接收到-LaunchedExecutor-消息后的执行逻辑"><a href="#4，详细分析-DriverEndpoint-接收到-LaunchedExecutor-消息后的执行逻辑" class="headerlink" title="4，详细分析 DriverEndpoint 接收到 LaunchedExecutor 消息后的执行逻辑"></a>4，详细分析 DriverEndpoint 接收到 LaunchedExecutor 消息后的执行逻辑</h3><h4 id="（1）代码-3"><a href="#（1）代码-3" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line">      /**</span><br><span class="line">       * 触发场景：</span><br><span class="line">       * 当 Driver 收到 LaunchedExecutor(executorId) 消息时，表示某个 Executor 已成功启动并向 Driver 注册。此时需要：</span><br><span class="line">       * 1，重置该 Executor 的空闲资源（CPU 核心数），标记为“完全可用”。</span><br><span class="line">       * 2，立即触发任务调度，将待处理的任务分配给该 Executor。</span><br><span class="line">       */</span><br><span class="line">      case LaunchedExecutor(executorId) =&gt;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * executorDataMap.get(executorId).foreach &#123; ... &#125;</span><br><span class="line">         * 从 executorDataMap 中获取该 Executor 的运行时数据（ExecutorData），并将其 freeCores（空闲核心数）重置为 totalCores（总核心数）。</span><br><span class="line">         *  目的：确保 Executor 的所有核心都可以被调度器重新分配任务（例如 Executor 重启后恢复资源）。</span><br><span class="line">         *     1. executorDataMap 的结构</span><br><span class="line">         *     类型：Map[String, ExecutorData]</span><br><span class="line">         *     键为 executorId（字符串），值为 ExecutorData 对象，包含 Executor 的运行时信息。</span><br><span class="line">         *     示例：</span><br><span class="line">         *     val executorDataMap = Map(</span><br><span class="line">         *     &quot;executor-1&quot; -&gt; ExecutorData(totalCores=4, freeCores=0),</span><br><span class="line">         *     &quot;executor-2&quot; -&gt; ExecutorData(totalCores=4, freeCores=4)</span><br><span class="line">         *     )</span><br><span class="line">         *     2. get 方法返回 Option[ExecutorData]</span><br><span class="line">         *     executorDataMap.get(executorId) 返回 Option[ExecutorData]：</span><br><span class="line">         *     若存在该 executorId，返回 Some(ExecutorData)。</span><br><span class="line">         *     若不存在，返回 None。</span><br><span class="line">         *     示例：</span><br><span class="line">         *     executorDataMap.get(&quot;executor-1&quot;) // 返回 Some(ExecutorData(4, 0))</span><br><span class="line">         *     executorDataMap.get(&quot;executor-3&quot;) // 返回 None</span><br><span class="line">         *     3. foreach 方法的作用</span><br><span class="line">         *     Option.foreach 是 Scala 中 Option 类型的方法，其行为如下：</span><br><span class="line">         *     若 Option 是 Some(value)，则对 value 执行代码块。</span><br><span class="line">         *     若 Option 是 None，则不执行任何操作。</span><br><span class="line">         *     语法等价于：</span><br><span class="line">         *     executorDataMap.get(executorId) match &#123;</span><br><span class="line">         *     case Some(data) =&gt; data.freeCores = data.totalCores</span><br><span class="line">         *     case None       =&gt; // 忽略</span><br><span class="line">         *     &#125;</span><br><span class="line">         *     4. 完整语法解释</span><br><span class="line">         *     步骤：</span><br><span class="line">         *      通过 executorId 从 executorDataMap 中查找对应的 ExecutorData。</span><br><span class="line">         *      如果找到（Some(data)），执行代码块，将 freeCores 重置为 totalCores。</span><br><span class="line">         *      如果未找到（None），不做任何操作（可能因 Executor 已被移除或未正确注册）。</span><br><span class="line">         *</span><br><span class="line">         * 代码执行过程</span><br><span class="line">         *  收到 LaunchedExecutor(&quot;executor-1&quot;) 消息</span><br><span class="line">         *  Driver 确认该 Executor 已启动并注册。</span><br><span class="line">         *</span><br><span class="line">         *  从 executorDataMap 获取数据</span><br><span class="line">         *  executorDataMap.get(&quot;executor-1&quot;) 返回 Some(ExecutorData(4, 0))。</span><br><span class="line">         *</span><br><span class="line">         *  执行 foreach 代码块</span><br><span class="line">         *  将 freeCores 从 0 重置为 4，表示所有核心可用。</span><br><span class="line">         *</span><br><span class="line">         *  调用 makeOffers(&quot;executor-1&quot;)</span><br><span class="line">         *  调度器立即检查该 Executor 的空闲核心（4 核），并分配任务。</span><br><span class="line">         */</span><br><span class="line">        executorDataMap.get(executorId).foreach &#123; data =&gt;</span><br><span class="line">          data.freeCores = data.totalCores</span><br><span class="line">        &#125;</span><br><span class="line">        makeOffers(executorId)</span><br><span class="line">    private def makeOffers(executorId: String): Unit = &#123;</span><br><span class="line">      // Make sure no executor is killed while some task is launching on it</span><br><span class="line">      val taskDescs = withLock &#123;</span><br><span class="line">        // Filter out executors under killing</span><br><span class="line">        if (isExecutorActive(executorId)) &#123;</span><br><span class="line">          /**</span><br><span class="line">           * executorData：</span><br><span class="line">           *  从 executorDataMap 中获取该 Executor 的运行时数据（如资源信息）。</span><br><span class="line">           *  WorkerOffer 参数：</span><br><span class="line">           *  executorId：Executor 的 ID。</span><br><span class="line">           *  executorHost：Executor 所在主机名。</span><br><span class="line">           *  freeCores：Executor 当前可用的 CPU 核心数。</span><br><span class="line">           *  executorAddress.hostPort：Executor 的地址（用于通信）。</span><br><span class="line">           *  resourcesInfo：Executor 的可用资源（如 GPU、FPGA 等自定义资源）。</span><br><span class="line">           *  resourceProfileId：资源配置文件 ID，匹配任务资源需求。</span><br><span class="line">           *</span><br><span class="line">           *  作用：</span><br><span class="line">           *  将 Executor 的可用资源封装为 WorkerOffer，供调度器分配任务。</span><br><span class="line">           */</span><br><span class="line">          val executorData = executorDataMap(executorId)</span><br><span class="line">          val workOffers = IndexedSeq(</span><br><span class="line">            new WorkerOffer(executorId, executorData.executorHost, executorData.freeCores,</span><br><span class="line">              Some(executorData.executorAddress.hostPort),</span><br><span class="line">              executorData.resourcesInfo.map &#123; case (rName, rInfo) =&gt;</span><br><span class="line">                (rName, rInfo.availableAddrs.toBuffer)</span><br><span class="line">              &#125;, executorData.resourceProfileId))</span><br><span class="line"></span><br><span class="line">          /**</span><br><span class="line">           * scheduler：</span><br><span class="line">           *  指向 TaskSchedulerImpl，负责实际的任务调度。</span><br><span class="line">           *</span><br><span class="line">           *  resourceOffers 参数：</span><br><span class="line">           *</span><br><span class="line">           *  workOffers：资源提供列表（此处仅包含一个 Executor 的资源）。</span><br><span class="line">           *</span><br><span class="line">           *  false：表示非批量模式（可能用于调试或特殊调度场景）。</span><br><span class="line">           *</span><br><span class="line">           *  返回值：</span><br><span class="line">           *  taskDescs 是 Seq[TaskDescription]，表示需要在该 Executor 上启动的任务列表。</span><br><span class="line">           *</span><br><span class="line">           *  内部逻辑：</span><br><span class="line">           *  调度器根据任务优先级、数据本地性、资源需求等条件，从待调度队列中选择合适的任务，生成任务描述。</span><br><span class="line">           */</span><br><span class="line">          scheduler.resourceOffers(workOffers, false)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          Seq.empty</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      if (taskDescs.nonEmpty) &#123;</span><br><span class="line">        /**</span><br><span class="line">         * launchTasks：</span><br><span class="line">         *  将任务描述列表发送给对应 Executor 执行。</span><br><span class="line">         *</span><br><span class="line">         *  流程：</span><br><span class="line">         *</span><br><span class="line">         *  序列化任务：将 TaskDescription 序列化为字节流。</span><br><span class="line">         *</span><br><span class="line">         *  发送 RPC 消息：通过 CoarseGrainedExecutorBackend 的 RPC 端点发送 LaunchTask 消息。</span><br><span class="line">         *</span><br><span class="line">         *  Executor 执行任务：Executor 接收消息后反序列化任务并执行。</span><br><span class="line">         *</span><br><span class="line">         *</span><br><span class="line">         */</span><br><span class="line">        launchTasks(taskDescs)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = &#123;</span><br><span class="line">      for (task &lt;- tasks.flatten) &#123;</span><br><span class="line">        /**</span><br><span class="line">         * TaskDescription(</span><br><span class="line">         *  taskId = 1234,</span><br><span class="line">         *  executorId = &quot;exec-1&quot;,</span><br><span class="line">         *  index = 0,</span><br><span class="line">         *  attemptNumber = 1,</span><br><span class="line">         *  name = &quot;Map Task 0&quot;,</span><br><span class="line">         *  serializedTask = ByteBuffer.wrap(taskData)</span><br><span class="line">         *  )</span><br><span class="line">         */</span><br><span class="line">        val serializedTask = TaskDescription.encode(task)</span><br><span class="line">        if (serializedTask.limit() &gt;= maxRpcMessageSize) &#123;</span><br><span class="line">          Option(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach &#123; taskSetMgr =&gt;</span><br><span class="line">            try &#123;</span><br><span class="line">              var msg = &quot;Serialized task %s:%d was %d bytes, which exceeds max allowed: &quot; +</span><br><span class="line">                s&quot;$&#123;RPC_MESSAGE_MAX_SIZE.key&#125; (%d bytes). Consider increasing &quot; +</span><br><span class="line">                s&quot;$&#123;RPC_MESSAGE_MAX_SIZE.key&#125; or using broadcast variables for large values.&quot;</span><br><span class="line">              msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize)</span><br><span class="line">              taskSetMgr.abort(msg)</span><br><span class="line">            &#125; catch &#123;</span><br><span class="line">              case e: Exception =&gt; logError(&quot;Exception in error callback&quot;, e)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">//          根据任务分配的 executorId，从 executorDataMap 中获取对应 Executor 的状态信息</span><br><span class="line">          val executorData = executorDataMap(task.executorId)</span><br><span class="line">          // Do resources allocation here. The allocated resources will get released after the task</span><br><span class="line">          // finishes.</span><br><span class="line">          val rpId = executorData.resourceProfileId</span><br><span class="line">          val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)</span><br><span class="line">          val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)</span><br><span class="line">//          减少 Executor 的空闲 CPU 核数</span><br><span class="line">          executorData.freeCores -= taskCpus</span><br><span class="line">//          为任务分配所需的资源（如 GPU 地址），并更新资源的使用情况</span><br><span class="line">          task.resources.foreach &#123; case (rName, rInfo) =&gt;</span><br><span class="line">            assert(executorData.resourcesInfo.contains(rName))</span><br><span class="line">            executorData.resourcesInfo(rName).acquire(rInfo.addresses)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          logDebug(s&quot;Launching task $&#123;task.taskId&#125; on executor id: $&#123;task.executorId&#125; hostname: &quot; +</span><br><span class="line">            s&quot;$&#123;executorData.executorHost&#125;.&quot;)</span><br><span class="line">//          使用 RPC 调用将 LaunchTask 消息发送到目标 Executor，其中包含序列化的任务数据。</span><br><span class="line">//          接收方org.apache.spark.executor.CoarseGrainedExecutorBackend.receive</span><br><span class="line">          executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析-3"><a href="#（2）代码解析-3" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>当 Driver 收到 LaunchedExecutor(executorId) 消息时，表示某个 Executor 已成功启动并向 Driver 注册</li>
<li>重置该 Executor 的空闲资源（CPU 核心数），标记为“完全可用”</li>
<li>立即触发任务调度，将待处理的任务分配给该 Executor</li>
<li>触发 makeOffers 函数，将 Executor 的可用资源封装为 WorkerOffer，供调度器分配任务</li>
<li>调用 TaskSchedulerImpl 类里面的 resourceOffers 函数，调度器根据任务优先级、数据本地性、资源需求等条件，从待调度队列中选择合适的任务，生成任务描述</li>
<li>如果生成的任务描述 taskDescs 不为空，调用 launchTasks 函数<ul>
<li>executorData：根据任务分配的 executorId，从 executorDataMap 中获取对应 Executor 的状态信息</li>
<li>rpId：获取 Executor 的资源配置文件 ID</li>
<li>prof：根据 ID 获取具体的 ResourceProfile（定义 CPU、GPU 等资源需求）</li>
<li>taskCpus：从配置文件中读取任务所需的 CPU 核数（默认 1）</li>
<li>减少 Executor 的空闲 CPU 核数</li>
<li>遍历任务所需的资源（如 GPU），检查 Executor 是否具备该资源，并调用 acquire 方法分配具体资源地址</li>
<li>使用 RPC 调用将 LaunchTask 消息发送到目标 Executor，其中包含序列化的任务数据，接收方为 org.apache.spark.executor.CoarseGrainedExecutorBackend.receive</li>
</ul>
</li>
</ul>
<h3 id="5，详细分析-CoarseGrainedExecutorBackend-接收-LaunchTask-后的执行逻辑"><a href="#5，详细分析-CoarseGrainedExecutorBackend-接收-LaunchTask-后的执行逻辑" class="headerlink" title="5，详细分析 CoarseGrainedExecutorBackend 接收 LaunchTask 后的执行逻辑"></a>5，详细分析 CoarseGrainedExecutorBackend 接收 LaunchTask 后的执行逻辑</h3><h4 id="（1）代码-4"><a href="#（1）代码-4" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">    case LaunchTask(data) =&gt;</span><br><span class="line">      if (executor == null) &#123;</span><br><span class="line">        exitExecutor(1, &quot;Received LaunchTask command but executor was null&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        val taskDesc = TaskDescription.decode(data.value)</span><br><span class="line">        logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)</span><br><span class="line">        taskResources(taskDesc.taskId) = taskDesc.resources</span><br><span class="line">        executor.launchTask(this, taskDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = &#123;</span><br><span class="line">    val taskId = taskDescription.taskId</span><br><span class="line">//    创建 TaskRunner 对象</span><br><span class="line">    val tr = createTaskRunner(context, taskDescription)</span><br><span class="line">//     将 TaskRunner 注册到 runningTasks（用于跟踪运行中的任务）</span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">    val killMark = killMarks.get(taskId)</span><br><span class="line">    if (killMark != null) &#123;</span><br><span class="line">      tr.kill(killMark._1, killMark._2)</span><br><span class="line">      killMarks.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">//    提交到线程池执行</span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">    if (decommissioned) &#123;</span><br><span class="line">      log.error(s&quot;Launching a task while in decommissioned state.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析-4"><a href="#（2）代码解析-4" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>case LaunchTask 代码路径：org.apache.spark.executor.CoarseGrainedExecutorBackend#receive</li>
<li>launchTask 代码路径：org.apache.spark.executor.Executor#launchTask</li>
<li>taskDesc：将二进制数据反序列化为 TaskDescription 对象</li>
<li>使用 taskResources 记录任务资源信息,当任务完成或失败时，通过这些记录释放资源，确保资源被正确回收</li>
<li>executor.launchTask 创建 TaskRunner 对象，封装任务执行的逻辑，将 TaskRunner 提交到线程池（ThreadPoolExecutor）中执行<ul>
<li>tr 为创建 TaskRunner 对象</li>
<li>将 TaskRunner 注册到 runningTasks（用于跟踪运行中的任务）</li>
<li>killMark：ConcurrentHashMap[Long, (Boolean, String)]，记录被外部请求终止的任务及其原因</li>
<li>ThreadPoolExecutor，核心线程数由 spark.executor.cores 控制（默认1个线程&#x2F;CPU核心），将 TaskRunner 提交到线程池异步执行，实现任务并行化。线程池是调用 TaskRunner 实例里面的 run() 方法</li>
</ul>
</li>
</ul>
<h3 id="6，从-Master-发送-launchExecutor-消息到-launchTask-消息启动用户代码整体流程图"><a href="#6，从-Master-发送-launchExecutor-消息到-launchTask-消息启动用户代码整体流程图" class="headerlink" title="6，从 Master 发送 launchExecutor 消息到 launchTask 消息启动用户代码整体流程图"></a>6，从 Master 发送 launchExecutor 消息到 launchTask 消息启动用户代码整体流程图</h3><p><img src="/yunshenBlog.github.io/images/master_to_executor_task_success.svg" alt="SVG图片"></p>
<p><img src="/yunshenBlog.github.io/images/master_to_executor_task_failed.svg" alt="SVG图片"></p>
<h3 id="7，RPC消息关键路径流程图"><a href="#7，RPC消息关键路径流程图" class="headerlink" title="7，RPC消息关键路径流程图"></a>7，RPC消息关键路径流程图</h3><table>
<thead>
<tr>
<th>步骤</th>
<th>RPC消息</th>
<th>发送方</th>
<th>接收方</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>发送 <code>LaunchExecutor</code></td>
<td>Master</td>
<td>Worker</td>
<td>通知 Worker 启动 Executor，携带资源需求（CPU、内存、GPU 等）</td>
</tr>
<tr>
<td>2</td>
<td>-</td>
<td>Woker</td>
<td>ExecutorRunner</td>
<td>Woker 创建 ExecutorRunner，启动子进程 CoarseGrainedExecutorBackend</td>
</tr>
<tr>
<td>3</td>
<td>发送 <code>RegisterExecutor</code></td>
<td>CoarseGrainedExecutorBackend</td>
<td>DriverEndpoint</td>
<td>Executor 向 Driver 注册，上报资源信息（Executor ID，主机名，核心数等）</td>
</tr>
<tr>
<td>4</td>
<td>发送 <code>RegistedExecutor</code></td>
<td>CoarseGrainedExecutorBackend</td>
<td>CoarseGrainedExecutorBackend</td>
<td>Driver 确认注册成功，允许 Executor 接收任务，Executor 向自身发送 LaunchedExecutor 消息</td>
</tr>
<tr>
<td>5</td>
<td>发送 <code>LaunchedExecutor</code></td>
<td>CoarseGrainedExecutorBackend</td>
<td>DriverEndpoint</td>
<td>Executor 通知 Driver 已准备就绪，等待任务分配</td>
</tr>
<tr>
<td>6</td>
<td>发送 <code>LaunchTask</code></td>
<td>DriverEndpoint</td>
<td>CoarseGrainedExecutorBackend</td>
<td>Driver 分配任务到 Executor，携带序列化的任务数据（用户代码逻辑）</td>
</tr>
<tr>
<td>7</td>
<td>-</td>
<td>CoarseGrainedExecutorBackend</td>
<td>Executor</td>
<td>Executor 调用 <code>launchTask</code>，提交任务到线程池执行</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://nrliangxy.github.io/yunshenBlog.github.io">nrliangxy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://nrliangxy.github.io/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/">https://nrliangxy.github.io/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/spark-3-3-0/">spark 3.3.0</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/standalone/">standalone</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/Executor/">Executor</a></div><div class="post-share"><div class="social-share" data-image="/yunshenBlog.github.io/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">spark在standalone模式下，Master如何实现资源的调度和分配</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数schedule()来刷新资源情况，同时启动 Executor 的调度逻辑，为 Application 分配 Executor 资源  流程梳理1，调用scheduler()函数（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960  /**   * Schedule the currently available resources among waiting apps. This method will be called   * every time a new app joins or resource availability changes.   * 调度集群资源，启动等待的 Driver 和 Executor。   * 核心逻辑：   * 检查 Master...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/05/20/process-after-submit-app/" title="spark在standalone模式下，提交代码后的整个处理流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">spark在standalone模式下，提交代码后的整个处理流程</div></div><div class="info-2"><div class="info-item-1">背景 spark集群在 standalone 模式下，用户提交 app 后，spark 集群是如何进行读取代码，解析代码，执行代码的？这一篇文章就详细介绍一下整个流程  流程梳理1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args)...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/yunshenBlog.github.io/2025/05/20/process-after-submit-app/" title="spark在standalone模式下，提交代码后的整个处理流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-20</div><div class="info-item-2">spark在standalone模式下，提交代码后的整个处理流程</div></div><div class="info-2"><div class="info-item-1">背景 spark集群在 standalone 模式下，用户提交 app 后，spark 集群是如何进行读取代码，解析代码，执行代码的？这一篇文章就详细介绍一下整个流程  流程梳理1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args)...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-28</div><div class="info-item-2">spark在standalone模式下，Master如何实现资源的调度和分配</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数schedule()来刷新资源情况，同时启动 Executor 的调度逻辑，为 Application 分配 Executor 资源  流程梳理1，调用scheduler()函数（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960  /**   * Schedule the currently available resources among waiting apps. This method will be called   * every time a new app joins or resource availability changes.   * 调度集群资源，启动等待的 Driver 和 Executor。   * 核心逻辑：   * 检查 Master...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/" title="spark在standalone模式下，driver&#x2F;application与master的rpc通信流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-25</div><div class="info-item-2">spark在standalone模式下，driver&#x2F;application与master的rpc通信流程</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，在使用client模式下提交application的时候，driver运行在客户端机器上 如果客户端机器崩溃或者driver进程退出，application会失败 由于driver不在集群中，master无法直接管理driver的生命周期 本次只讨论在standalone模式下，使用client模式向master提交application的时候，driver和application与master的通信流程  流程梳理1.在客户端服务器上面提交application(1) 在客户端提交命令样例12345spark-submit --master spark://&lt;master-ip&gt;:&lt;master-port&gt; \  --deploy-mode client \  --class &lt;main-class&gt; \  &lt;application-jar&gt; \  &lt;application-args&gt;  (2)...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/21/zookeeper-master-select-leader/" title="spark在standalone模式下，使用zookeeper进行master选举流程剖析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-21</div><div class="info-item-2">spark在standalone模式下，使用zookeeper进行master选举流程剖析</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，为了确保master的高可用性，使用zookeeper来实现对master监控和选举 如果当前leader宕机后，如何实现leader的快速选举和相关worker节点，driver节点的快速恢复 本文要详细探讨一下leader选举和恢复的详细细节  流程梳理1.首次启动spark集群的master节点，使用start-master.sh来启动master节点(1) 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private[deploy] object Master extends Logging &#123;  val SYSTEM_NAME = &quot;sparkMaster&quot;  val ENDPOINT_NAME = &quot;Master&quot;  def...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">spark在standalone模式下，worker与master的rpc通信流程</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，在集群的sbin目录下，启动start-worker.sh脚本，启动了worker端点 worker端点启动后，读取配置后，在spark的rpc通信框架里面注册rpc端点，和master节点进行rpc通信并进行心跳交互 本文就是要详细探讨一下work节点是如何与master节点进行rpc通信  流程梳理1.使用start-worker.sh脚本启动worker节点(1) 在客户端提交命令样例1bash start-worker.sh  (2) 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647  def main(argStrings: Array[String]): Unit = &#123;//    设置一个默认的未捕获异常处理器 SparkUncaughtExceptionHandler。//    exitOnUncaughtException = false...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="spark在standalone模式下，FIFO和FAIR调度模式的对象分析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-08</div><div class="info-item-2">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/yunshenBlog.github.io/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/yunshenBlog.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">nrliangxy</div><div class="author-info-description"></div><div class="site-data"><a href="/yunshenBlog.github.io/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/yunshenBlog.github.io/tags/"><div class="headline">Tags</div><div class="length-num">12</div></a><a href="/yunshenBlog.github.io/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">流程梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%BC%8C%E6%B6%88%E6%81%AF%E4%BC%9A%E5%8F%91%E9%80%81%E5%88%B0-Worker-%E7%9A%84-endpoint%EF%BC%8C%E8%BF%99%E6%98%AF-Worker-%E6%8E%A5%E6%94%B6%E6%B6%88%E6%81%AF%E7%9A%84%E7%AB%AF%E7%82%B9%E3%80%82%E9%80%9A%E8%BF%87%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%8CMaster-%E5%91%8A%E8%AF%89-Worker-%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84-Executor-%E5%B9%B6%E4%B8%BA%E5%85%B6%E5%88%86%E9%85%8D%E8%B5%84%E6%BA%90"><span class="toc-number">2.1.</span> <span class="toc-text">1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这种方式，Master 告诉 Worker 启动一个新的 Executor 并为其分配资源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81"><span class="toc-number">2.1.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.1.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%EF%BC%8C%E8%B0%83%E7%94%A8-ExecutorRunner-%E5%87%BD%E6%95%B0%E9%87%8C%E9%9D%A2%E7%9A%84-start-%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">2，调用 ExecutorRunner() 函数里面的 start() 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-1"><span class="toc-number">2.2.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%EF%BC%8C%E5%90%AF%E5%8A%A8%E6%96%B0%E7%9A%84-Executor-%E8%BF%9B%E7%A8%8B%EF%BC%88CoarseGrainedExecutorBackend%EF%BC%89%E5%85%B7%E4%BD%93%E7%BB%86%E8%8A%82"><span class="toc-number">2.3.</span> <span class="toc-text">3，启动新的 Executor 进程（CoarseGrainedExecutorBackend）具体细节</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.3.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-2"><span class="toc-number">2.3.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%EF%BC%8C%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90-DriverEndpoint-%E6%8E%A5%E6%94%B6%E5%88%B0-LaunchedExecutor-%E6%B6%88%E6%81%AF%E5%90%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91"><span class="toc-number">2.4.</span> <span class="toc-text">4，详细分析 DriverEndpoint 接收到 LaunchedExecutor 消息后的执行逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-3"><span class="toc-number">2.4.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-3"><span class="toc-number">2.4.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%EF%BC%8C%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90-CoarseGrainedExecutorBackend-%E6%8E%A5%E6%94%B6-LaunchTask-%E5%90%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91"><span class="toc-number">2.5.</span> <span class="toc-text">5，详细分析 CoarseGrainedExecutorBackend 接收 LaunchTask 后的执行逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-4"><span class="toc-number">2.5.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-4"><span class="toc-number">2.5.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%EF%BC%8C%E4%BB%8E-Master-%E5%8F%91%E9%80%81-launchExecutor-%E6%B6%88%E6%81%AF%E5%88%B0-launchTask-%E6%B6%88%E6%81%AF%E5%90%AF%E5%8A%A8%E7%94%A8%E6%88%B7%E4%BB%A3%E7%A0%81%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.6.</span> <span class="toc-text">6，从 Master 发送 launchExecutor 消息到 launchTask 消息启动用户代码整体流程图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%EF%BC%8CRPC%E6%B6%88%E6%81%AF%E5%85%B3%E9%94%AE%E8%B7%AF%E5%BE%84%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.7.</span> <span class="toc-text">7，RPC消息关键路径流程图</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/05/20/process-after-submit-app/" title="spark在standalone模式下，提交代码后的整个处理流程">spark在standalone模式下，提交代码后的整个处理流程</a><time datetime="2025-05-19T23:25:51.000Z" title="Created 2025-05-20 07:25:51">2025-05-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/" title="spark 在 standalone 模式，Master 是如何调度和启动 Executor">spark 在 standalone 模式，Master 是如何调度和启动 Executor</a><time datetime="2025-04-17T22:53:11.000Z" title="Created 2025-04-18 06:53:11">2025-04-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配">spark在standalone模式下，Master如何实现资源的调度和分配</a><time datetime="2025-03-28T01:57:10.000Z" title="Created 2025-03-28 09:57:10">2025-03-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/21/zookeeper-master-select-leader/" title="spark在standalone模式下，使用zookeeper进行master选举流程剖析">spark在standalone模式下，使用zookeeper进行master选举流程剖析</a><time datetime="2025-03-21T03:57:09.000Z" title="Created 2025-03-21 11:57:09">2025-03-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程">spark在standalone模式下，worker与master的rpc通信流程</a><time datetime="2025-03-06T01:13:14.000Z" title="Created 2025-03-06 09:13:14">2025-03-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By nrliangxy</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/yunshenBlog.github.io/js/utils.js"></script><script src="/yunshenBlog.github.io/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>