<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>spark在standalone模式下，提交代码后的整个处理流程 | 只在此山中，云深不知处</title><meta name="author" content="nrliangxy"><meta name="copyright" content="nrliangxy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="背景 spark集群在 standalone 模式下，用户提交 app 后，spark 集群是如何进行读取代码，解析代码，执行代码的？这一篇文章就详细介绍一下整个流程  流程梳理1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args) 提交代码（1）代码1234567891011121314151617181920212223242526">
<meta property="og:type" content="article">
<meta property="og:title" content="spark在standalone模式下，提交代码后的整个处理流程">
<meta property="og:url" content="https://nrliangxy.github.io/yunshenBlog.github.io/2025/05/20/process-after-submit-app/index.html">
<meta property="og:site_name" content="只在此山中，云深不知处">
<meta property="og:description" content="背景 spark集群在 standalone 模式下，用户提交 app 后，spark 集群是如何进行读取代码，解析代码，执行代码的？这一篇文章就详细介绍一下整个流程  流程梳理1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args) 提交代码（1）代码1234567891011121314151617181920212223242526">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-05-19T23:25:51.000Z">
<meta property="article:modified_time" content="2025-05-21T13:35:44.867Z">
<meta property="article:author" content="nrliangxy">
<meta property="article:tag" content="spark 3.3.0">
<meta property="article:tag" content="standalone">
<meta property="article:tag" content="Executor">
<meta property="article:tag" content="submit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "spark在standalone模式下，提交代码后的整个处理流程",
  "url": "https://nrliangxy.github.io/yunshenBlog.github.io/2025/05/20/process-after-submit-app/",
  "image": "https://nrliangxy.github.io/yunshenBlog.github.io/img/butterfly-icon.png",
  "datePublished": "2025-05-19T23:25:51.000Z",
  "dateModified": "2025-05-21T13:35:44.867Z",
  "author": [
    {
      "@type": "Person",
      "name": "nrliangxy",
      "url": "https://nrliangxy.github.io/yunshenBlog.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/yunshenBlog.github.io/img/favicon.png"><link rel="canonical" href="https://nrliangxy.github.io/yunshenBlog.github.io/2025/05/20/process-after-submit-app/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/yunshenBlog.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/yunshenBlog.github.io/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'spark在standalone模式下，提交代码后的整个处理流程',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/yunshenBlog.github.io/"><span class="site-name">只在此山中，云深不知处</span></a><a class="nav-page-title" href="/yunshenBlog.github.io/"><span class="site-name">spark在standalone模式下，提交代码后的整个处理流程</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">spark在standalone模式下，提交代码后的整个处理流程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-05-19T23:25:51.000Z" title="Created 2025-05-20 07:25:51">2025-05-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-05-21T13:35:44.867Z" title="Updated 2025-05-21 21:35:44">2025-05-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/">大数据计算</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/yunshenBlog.github.io/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/%E6%89%B9%E5%A4%84%E7%90%86/">批处理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li>spark集群在 standalone 模式下，用户提交 app 后，spark 集群是如何进行读取代码，解析代码，执行代码的？这一篇文章就详细介绍一下整个流程</li>
</ul>
<h2 id="流程梳理"><a href="#流程梳理" class="headerlink" title="流程梳理"></a>流程梳理</h2><h3 id="1，通过-object-SparkSubmit-的-main-函数里面的-submit-doSubmit-args-提交代码"><a href="#1，通过-object-SparkSubmit-的-main-函数里面的-submit-doSubmit-args-提交代码" class="headerlink" title="1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args) 提交代码"></a>1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args) 提交代码</h3><h4 id="（1）代码"><a href="#（1）代码" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br></pre></td><td class="code"><pre><span class="line">  def doSubmit(args: Array[String]): Unit = &#123;</span><br><span class="line">    // Initialize logging if it hasn&#x27;t been done yet. Keep track of whether logging needs to</span><br><span class="line">    // be reset before the application starts.</span><br><span class="line">//    作用：初始化日志系统。如果日志系统尚未初始化，则进行初始化。</span><br><span class="line">//    true：表示需要初始化日志。silent = true：表示在初始化日志时不输出日志信息。</span><br><span class="line">//    uninitLog 是一个布尔值，表示日志系统是否需要重置。</span><br><span class="line">    val uninitLog = initializeLogIfNecessary(true, silent = true)</span><br><span class="line">//    作用：解析命令行参数，将其转换为 SparkSubmitArguments 对象。</span><br><span class="line">//    parseArguments 方法：将命令行参数解析为 SparkSubmitArguments 对象，包含任务提交所需的所有配置信息（如主类、JAR 包、资源分配等）。</span><br><span class="line">    val appArgs = parseArguments(args)</span><br><span class="line">//    作用：如果设置了 verbose 参数（即 --verbose），则打印解析后的参数信息。</span><br><span class="line">    if (appArgs.verbose) &#123;</span><br><span class="line">//      logInfo：输出日志信息。</span><br><span class="line">      logInfo(appArgs.toString)</span><br><span class="line">    &#125;</span><br><span class="line">    appArgs.action match &#123;</span><br><span class="line">//    根据 appArgs 中的配置信息，准备任务提交所需的环境（如类路径、资源分配等）。</span><br><span class="line">//    调用 runMain 方法启动任务的主类。如果任务提交成功，返回任务的状态信息。</span><br><span class="line">      case SparkSubmitAction.SUBMIT =&gt; submit(appArgs, uninitLog)</span><br><span class="line">//    作用：杀死指定的 Spark 任务。</span><br><span class="line">//    根据 appArgs 中的任务 ID，向集群管理器发送杀死任务的请求。</span><br><span class="line">//    如果任务成功被杀死，返回成功信息；否则返回错误信息。</span><br><span class="line">      /**</span><br><span class="line">       * spark-submit \</span><br><span class="line">       *  --kill app-20231010123456-0001 \</span><br><span class="line">       *  --master spark://&lt;master-host&gt;:&lt;port&gt;</span><br><span class="line">       */</span><br><span class="line">      case SparkSubmitAction.KILL =&gt; kill(appArgs)</span><br><span class="line">//    作用：查询指定 Spark 任务的状态。</span><br><span class="line">//    requestStatus 方法：根据 appArgs 中的任务 ID，向集群管理器发送查询任务状态的请求。</span><br><span class="line">//    返回任务的状态信息（如运行中、已完成、失败等）。</span><br><span class="line">      /**</span><br><span class="line">       * spark-submit \</span><br><span class="line">       *  --status app-20231010123456-0001 \</span><br><span class="line">       *  --master spark://&lt;master-host&gt;:&lt;port&gt;</span><br><span class="line">       */</span><br><span class="line">      case SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs)</span><br><span class="line">//    作用：打印 Spark 的版本信息。</span><br><span class="line">      case SparkSubmitAction.PRINT_VERSION =&gt; printVersion()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Submit the application using the provided parameters, ensuring to first wrap</span><br><span class="line">   * in a doAs when --proxy-user is specified.</span><br><span class="line">   */</span><br><span class="line">  @tailrec</span><br><span class="line">  private def submit(args: SparkSubmitArguments, uninitLog: Boolean): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    def doRunMain(): Unit = &#123;</span><br><span class="line">      if (args.proxyUser != null) &#123;</span><br><span class="line">        val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser,</span><br><span class="line">          UserGroupInformation.getCurrentUser())</span><br><span class="line">        try &#123;</span><br><span class="line">          proxyUser.doAs(new PrivilegedExceptionAction[Unit]() &#123;</span><br><span class="line">            override def run(): Unit = &#123;</span><br><span class="line">              runMain(args, uninitLog)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: Exception =&gt;</span><br><span class="line">            // Hadoop&#x27;s AuthorizationException suppresses the exception&#x27;s stack trace, which</span><br><span class="line">            // makes the message printed to the output by the JVM not very helpful. Instead,</span><br><span class="line">            // detect exceptions with empty stack traces here, and treat them differently.</span><br><span class="line">            if (e.getStackTrace().length == 0) &#123;</span><br><span class="line">              error(s&quot;ERROR: $&#123;e.getClass().getName()&#125;: $&#123;e.getMessage()&#125;&quot;)</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              throw e</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        runMain(args, uninitLog)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // In standalone cluster mode, there are two submission gateways:</span><br><span class="line">    //   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper</span><br><span class="line">    //   (2) The new REST-based gateway introduced in Spark 1.3</span><br><span class="line">    // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span><br><span class="line">    // to use the legacy gateway if the master endpoint turns out to be not a REST server.</span><br><span class="line">    if (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        logInfo(&quot;Running Spark using the REST application submission protocol.&quot;)</span><br><span class="line">        doRunMain()</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        // Fail over to use the legacy submission gateway</span><br><span class="line">        case e: SubmitRestConnectionException =&gt;</span><br><span class="line">          logWarning(s&quot;Master endpoint $&#123;args.master&#125; was not a REST server. &quot; +</span><br><span class="line">            &quot;Falling back to legacy submission gateway instead.&quot;)</span><br><span class="line">          args.useRest = false</span><br><span class="line">          submit(args, false)</span><br><span class="line">      &#125;</span><br><span class="line">    // In all other modes, just run the main class as prepared</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      doRunMain()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Submit the application using the provided parameters, ensuring to first wrap</span><br><span class="line">   * in a doAs when --proxy-user is specified.</span><br><span class="line">   */</span><br><span class="line">  @tailrec</span><br><span class="line">  private def submit(args: SparkSubmitArguments, uninitLog: Boolean): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    def doRunMain(): Unit = &#123;</span><br><span class="line">      if (args.proxyUser != null) &#123;</span><br><span class="line">        val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser,</span><br><span class="line">          UserGroupInformation.getCurrentUser())</span><br><span class="line">        try &#123;</span><br><span class="line">          proxyUser.doAs(new PrivilegedExceptionAction[Unit]() &#123;</span><br><span class="line">            override def run(): Unit = &#123;</span><br><span class="line">              runMain(args, uninitLog)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: Exception =&gt;</span><br><span class="line">            // Hadoop&#x27;s AuthorizationException suppresses the exception&#x27;s stack trace, which</span><br><span class="line">            // makes the message printed to the output by the JVM not very helpful. Instead,</span><br><span class="line">            // detect exceptions with empty stack traces here, and treat them differently.</span><br><span class="line">            if (e.getStackTrace().length == 0) &#123;</span><br><span class="line">              error(s&quot;ERROR: $&#123;e.getClass().getName()&#125;: $&#123;e.getMessage()&#125;&quot;)</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              throw e</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        runMain(args, uninitLog)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // In standalone cluster mode, there are two submission gateways:</span><br><span class="line">    //   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper</span><br><span class="line">    //   (2) The new REST-based gateway introduced in Spark 1.3</span><br><span class="line">    // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span><br><span class="line">    // to use the legacy gateway if the master endpoint turns out to be not a REST server.</span><br><span class="line">    if (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        logInfo(&quot;Running Spark using the REST application submission protocol.&quot;)</span><br><span class="line">        doRunMain()</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        // Fail over to use the legacy submission gateway</span><br><span class="line">        case e: SubmitRestConnectionException =&gt;</span><br><span class="line">          logWarning(s&quot;Master endpoint $&#123;args.master&#125; was not a REST server. &quot; +</span><br><span class="line">            &quot;Falling back to legacy submission gateway instead.&quot;)</span><br><span class="line">          args.useRest = false</span><br><span class="line">          submit(args, false)</span><br><span class="line">      &#125;</span><br><span class="line">    // In all other modes, just run the main class as prepared</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      doRunMain()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  /**</span><br><span class="line">   * Run the main method of the child class using the submit arguments.</span><br><span class="line">   *</span><br><span class="line">   * This runs in two steps. First, we prepare the launch environment by setting up</span><br><span class="line">   * the appropriate classpath, system properties, and application arguments for</span><br><span class="line">   * running the child main class based on the cluster manager and the deploy mode.</span><br><span class="line">   * Second, we use this launch environment to invoke the main method of the child</span><br><span class="line">   * main class.</span><br><span class="line">   *</span><br><span class="line">   * Note that this main class will not be the one provided by the user if we&#x27;re</span><br><span class="line">   * running cluster deploy mode or python applications.</span><br><span class="line">   * args: SparkSubmitArguments：任务提交的参数，包含主类、JAR 包、资源配置等信息。</span><br><span class="line">   * uninitLog: Boolean：是否需要重置日志系统。</span><br><span class="line">   */</span><br><span class="line">  private def runMain(args: SparkSubmitArguments, uninitLog: Boolean): Unit = &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 作用：调用 prepareSubmitEnvironment 方法，准备任务运行所需的环境。</span><br><span class="line">     * 返回值：</span><br><span class="line">     * childArgs：任务的命令行参数。</span><br><span class="line">     * childClasspath：任务的类路径（包括 JAR 包和依赖）。</span><br><span class="line">     * sparkConf：任务的 Spark 配置。</span><br><span class="line">     * childMainClass：任务的主类。</span><br><span class="line">     */</span><br><span class="line">    val (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</span><br><span class="line">    // Let the main class re-initialize the logging system once it starts.</span><br><span class="line">//    作用：如果 uninitLog 为 true，则重置日志系统。</span><br><span class="line">//    原因：在任务启动时，主类可能会重新初始化日志系统，因此需要先重置。</span><br><span class="line">    if (uninitLog) &#123;</span><br><span class="line">      Logging.uninitialize()</span><br><span class="line">    &#125;</span><br><span class="line">//    作用：如果启用了 verbose 模式，打印任务的主类、参数、配置和类路径信息。</span><br><span class="line">//    Utils.redact：对敏感信息（如密码）进行脱敏处理。</span><br><span class="line">    if (args.verbose) &#123;</span><br><span class="line">      logInfo(s&quot;Main class:\n$childMainClass&quot;)</span><br><span class="line">      logInfo(s&quot;Arguments:\n$&#123;childArgs.mkString(&quot;\n&quot;)&#125;&quot;)</span><br><span class="line">      // sysProps may contain sensitive information, so redact before printing</span><br><span class="line">      logInfo(s&quot;Spark config:\n$&#123;Utils.redact(sparkConf.getAll.toMap).sorted.mkString(&quot;\n&quot;)&#125;&quot;)</span><br><span class="line">      logInfo(s&quot;Classpath elements:\n$&#123;childClasspath.mkString(&quot;\n&quot;)&#125;&quot;)</span><br><span class="line">      logInfo(&quot;\n&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">//  作用：</span><br><span class="line">//  获取任务的类加载器（loader）。</span><br><span class="line">//  将任务的类路径（childClasspath）中的 JAR 包添加到类加载器中。</span><br><span class="line">    val loader = getSubmitClassLoader(sparkConf)</span><br><span class="line">    for (jar &lt;- childClasspath) &#123;</span><br><span class="line">      addJarToClasspath(jar, loader)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var mainClass: Class[_] = null</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">//      作用：加载任务的主类（childMainClass）。</span><br><span class="line">      mainClass = Utils.classForName(childMainClass)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: ClassNotFoundException =&gt;</span><br><span class="line">        logError(s&quot;Failed to load class $childMainClass.&quot;)</span><br><span class="line">        if (childMainClass.contains(&quot;thriftserver&quot;)) &#123;</span><br><span class="line">          logInfo(s&quot;Failed to load main class $childMainClass.&quot;)</span><br><span class="line">          logInfo(&quot;You need to build Spark with -Phive and -Phive-thriftserver.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">        throw new SparkUserAppException(CLASS_NOT_FOUND_EXIT_STATUS)</span><br><span class="line">      case e: NoClassDefFoundError =&gt;</span><br><span class="line">        logError(s&quot;Failed to load $childMainClass: $&#123;e.getMessage()&#125;&quot;)</span><br><span class="line">        if (e.getMessage.contains(&quot;org/apache/hadoop/hive&quot;)) &#123;</span><br><span class="line">          logInfo(s&quot;Failed to load hive class.&quot;)</span><br><span class="line">          logInfo(&quot;You need to build Spark with -Phive and -Phive-thriftserver.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">        throw new SparkUserAppException(CLASS_NOT_FOUND_EXIT_STATUS)</span><br><span class="line">    &#125;</span><br><span class="line">//    如果主类实现了 SparkApplication 接口，则直接实例化主类。否则，将主类包装为 JavaMainApplication 实例。</span><br><span class="line">    /**</span><br><span class="line">     *  主类实现了 SparkApplication 接口,直接调用主类的 start 方法。</span><br><span class="line">     *  样例：</span><br><span class="line">     * class MySparkApp extends SparkApplication &#123;</span><br><span class="line">     *  override def start(args: Array[String], conf: SparkConf): Unit = &#123;</span><br><span class="line">     *  // 初始化 SparkContext</span><br><span class="line">     *  val sc = new SparkContext(conf)</span><br><span class="line">     *  // 任务逻辑</span><br><span class="line">     *  val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))</span><br><span class="line">     *  println(rdd.reduce(_ + _))</span><br><span class="line">     *  // 关闭 SparkContext</span><br><span class="line">     *  sc.stop()</span><br><span class="line">     *  &#125;</span><br><span class="line">     *  &#125;</span><br><span class="line">     *  如果主类未实现 SparkApplication 接口，使用 JavaMainApplication 包装类调用主类的 main 方法。</span><br><span class="line">     *  样例：</span><br><span class="line">     * object MySparkApp &#123;</span><br><span class="line">     *  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">     *  // 初始化 SparkConf</span><br><span class="line">     *  val conf = new SparkConf().setAppName(&quot;My Spark App&quot;)</span><br><span class="line">     *  // 初始化 SparkContext</span><br><span class="line">     *  val sc = new SparkContext(conf)</span><br><span class="line">     *  // 任务逻辑</span><br><span class="line">     *  val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))</span><br><span class="line">     *  println(rdd.reduce(_ + _))</span><br><span class="line">     *  // 关闭 SparkContext</span><br><span class="line">     *  sc.stop()</span><br><span class="line">     *  &#125;</span><br><span class="line">     *  &#125;</span><br><span class="line">     */</span><br><span class="line">    val app: SparkApplication = if (classOf[SparkApplication].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">      mainClass.getConstructor().newInstance().asInstanceOf[SparkApplication]</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new JavaMainApplication(mainClass)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @tailrec</span><br><span class="line">    def findCause(t: Throwable): Throwable = t match &#123;</span><br><span class="line">      case e: UndeclaredThrowableException =&gt;</span><br><span class="line">        if (e.getCause() != null) findCause(e.getCause()) else e</span><br><span class="line">      case e: InvocationTargetException =&gt;</span><br><span class="line">        if (e.getCause() != null) findCause(e.getCause()) else e</span><br><span class="line">      case e: Throwable =&gt;</span><br><span class="line">        e</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">//      作用：调用 app.start 方法启动任务。如果任务抛出异常，递归查找根本原因并抛出。如果任务运行在 Kubernetes 模式下，任务结束后关闭 SparkContext。</span><br><span class="line">      app.start(childArgs.toArray, sparkConf)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case t: Throwable =&gt;</span><br><span class="line">        throw findCause(t)</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      if (args.master.startsWith(&quot;k8s&quot;) &amp;&amp; !isShell(args.primaryResource) &amp;&amp;</span><br><span class="line">          !isSqlShell(args.mainClass) &amp;&amp; !isThriftServer(args.mainClass)) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          SparkContext.getActive.foreach(_.stop())</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: Throwable =&gt; logError(s&quot;Failed to close SparkContext: $e&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Implementation of SparkApplication that wraps a standard Java class with a &quot;main&quot; method.</span><br><span class="line"> *</span><br><span class="line"> * Configuration is propagated to the application via system properties, so running multiple</span><br><span class="line"> * of these in the same JVM may lead to undefined behavior due to configuration leaks.</span><br><span class="line"> */</span><br><span class="line">private[deploy] class JavaMainApplication(klass: Class[_]) extends SparkApplication &#123;</span><br><span class="line"></span><br><span class="line">  override def start(args: Array[String], conf: SparkConf): Unit = &#123;</span><br><span class="line">    val mainMethod = klass.getMethod(&quot;main&quot;, new Array[String](0).getClass)</span><br><span class="line">    if (!Modifier.isStatic(mainMethod.getModifiers)) &#123;</span><br><span class="line">      throw new IllegalStateException(&quot;The main method in the given main class must be static&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val sysProps = conf.getAll.toMap</span><br><span class="line">    sysProps.foreach &#123; case (k, v) =&gt;</span><br><span class="line">      sys.props(k) = v</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    mainMethod.invoke(null, args)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析"><a href="#（2）代码解析" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>代码路径为：org.apache.spark.deploy.SparkSubmit#runMain</li>
<li>入口函数路径为：org.apache.spark.deploy.SparkSubmit#submit</li>
<li>mainClass：加载任务的主类（childMainClass）</li>
<li>client 模式下，app 为 new JavaMainApplication(mainClass)</li>
<li>app.start 调用 JavaMainApplication 对象的 start() 方法，启动用户提交的代码的 main 函数，初始化 SparkContext</li>
</ul>
<h4 id="（3）app-任务提交过程中，函数调用流程图"><a href="#（3）app-任务提交过程中，函数调用流程图" class="headerlink" title="（3）app 任务提交过程中，函数调用流程图"></a>（3）app 任务提交过程中，函数调用流程图</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">┌──────────────────────────┐</span><br><span class="line">│          main()          │ 入口方法（SparkSubmit.scala）</span><br><span class="line">└────────────┬─────────────┘</span><br><span class="line">             │</span><br><span class="line">             ▼</span><br><span class="line">┌──────────────────────────┐</span><br><span class="line">│        submit()          │ 解析参数，选择执行模式</span><br><span class="line">└────────────┬─────────────┘</span><br><span class="line">             │</span><br><span class="line">             ▼</span><br><span class="line">┌──────────────────────────┐</span><br><span class="line">│  parseArguments()        │ 解析命令行参数，生成 SparkSubmitArguments 对象</span><br><span class="line">└────────────┬─────────────┘</span><br><span class="line">             │</span><br><span class="line">             ▼</span><br><span class="line">┌──────────────────────────┐</span><br><span class="line">│  prepareSubmitEnvironment() │ 准备环境：确定主类、依赖、部署模式（Client/Cluster）</span><br><span class="line">│  - 根据参数选择 childMainClass  │ （如 Client、YarnClusterApplication 或用户类）</span><br><span class="line">└────────────┬─────────────┘</span><br><span class="line">             │</span><br><span class="line">             ▼</span><br><span class="line">┌──────────────────────────┐</span><br><span class="line">│        runMain()         │ 反射加载主类并调用其入口方法</span><br><span class="line">│  - 创建类加载器          │</span><br><span class="line">│  - 实例化 app（childMainClass）│</span><br><span class="line">│  - 调用 app.start()      │</span><br><span class="line">└────────────┬─────────────┘</span><br><span class="line">             │</span><br><span class="line">             ▼</span><br><span class="line">┌──────────────────────────┐</span><br><span class="line">│     app.start()          │ 根据 app 类型执行不同逻辑：</span><br><span class="line">│                          │</span><br><span class="line">├──────────────────────────┤</span><br><span class="line">│ 1. 集群模式（Cluster Mode）  │</span><br><span class="line">│   - 例如：StandaloneClient      │</span><br><span class="line">│   - 源码位置：StandaloneAppClient.scala │</span><br><span class="line">│   - 提交请求到集群，由集群启动 Driver  │</span><br><span class="line">│                         │</span><br><span class="line">├──────────────────────────┤</span><br><span class="line">│ 2. 客户端模式（Client Mode） │</span><br><span class="line">│   - app = JavaMainApplication │</span><br><span class="line">│   - 反射调用用户类的 main()   │</span><br><span class="line">└──────────────────────────┘</span><br></pre></td></tr></table></figure>
<h3 id="2，启动用户提交代码里面-SparkContext-初始化"><a href="#2，启动用户提交代码里面-SparkContext-初始化" class="headerlink" title="2，启动用户提交代码里面 SparkContext() 初始化"></a>2，启动用户提交代码里面 SparkContext() 初始化</h3><h4 id="（1）代码-1"><a href="#（1）代码-1" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">        val (sched, ts) = SparkContext.createTaskScheduler(this, master)</span><br><span class="line">        _schedulerBackend = sched</span><br><span class="line">        _taskScheduler = ts</span><br><span class="line">        //    通过DAGScheduler将job分解为stage，然后分解为taskset</span><br><span class="line">        _dagScheduler = new DAGScheduler(this)</span><br><span class="line">        _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)</span><br><span class="line"></span><br><span class="line">        val _executorMetricsSource =</span><br><span class="line">          if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) &#123;</span><br><span class="line">            Some(new ExecutorMetricsSource)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            None</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        // create and start the heartbeater for collecting memory metrics</span><br><span class="line">        _heartbeater = new Heartbeater(</span><br><span class="line">          () =&gt; SparkContext.this.reportHeartBeat(_executorMetricsSource),</span><br><span class="line">          &quot;driver-heartbeater&quot;,</span><br><span class="line">          conf.get(EXECUTOR_HEARTBEAT_INTERVAL))</span><br><span class="line">        _heartbeater.start()</span><br><span class="line"></span><br><span class="line">        // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&#x27;s</span><br><span class="line">        // constructor</span><br><span class="line">        _taskScheduler.start()</span><br><span class="line"></span><br><span class="line">- SparkContext() 进行初始化启动的时候，默认初始化启动</span><br><span class="line">- 初始化 DAGScheduler，创建心跳连接</span><br><span class="line">- \_taskScheduler.start() 创建 spark 应用描述信息，并与集群建立 rpc 通信</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">      case SPARK_REGEX(sparkUrl) =&gt;</span><br><span class="line">//        生成</span><br><span class="line">        val scheduler = new TaskSchedulerImpl(sc)</span><br><span class="line">        val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)</span><br><span class="line">        //      standalone模式下执行</span><br><span class="line">        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">	/**</span><br><span class="line">	 * 推测执行（Speculative Execution）的背景：</span><br><span class="line">	 * 推测执行是 Spark 的一种优化机制，用于解决“长尾任务”问题。在某些情况下，某些任务的执行时间可能远远超过其他任务（例如由于数据倾斜或节点性能问题）。</span><br><span class="line">	 * 推测执行会为这些“慢任务”启动一个备份任务（Speculative Task），并在多个节点上同时执行。最先完成的任务结果会被采用，从而加快整体任务的完成速度。</span><br><span class="line">	 */</span><br><span class="line">  override def start(): Unit = &#123;</span><br><span class="line">//		启动任务调度器的后端（backend），准备接收资源分配和任务调度请求。</span><br><span class="line">//    val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)</span><br><span class="line">    backend.start()</span><br><span class="line"></span><br><span class="line">    if (!isLocal &amp;&amp; conf.get(SPECULATION_ENABLED)) &#123;</span><br><span class="line">      logInfo(&quot;Starting speculative execution thread&quot;)</span><br><span class="line">//      作用：启动一个定时任务，定期检查是否有需要推测执行的任务。</span><br><span class="line">//			speculationScheduler:这是一个 ScheduledExecutorService 实例，用于调度定时任务。 它负责定期执行推测执行的检查逻辑。</span><br><span class="line">//			scheduleWithFixedDelay:这是一个定时调度方法，用于以固定的延迟时间重复执行任务。</span><br><span class="line">//			参数说明：第一个参数：要执行的任务（一个 Runnable 或函数）。</span><br><span class="line">//			第二个参数：初始延迟时间（SPECULATION_INTERVAL_MS）。</span><br><span class="line">//			第三个参数：每次执行任务的间隔时间（SPECULATION_INTERVAL_MS）。</span><br><span class="line">//			第四个参数：时间单位（TimeUnit.MILLISECONDS 表示毫秒）。</span><br><span class="line">//			() =&gt; Utils.tryOrStopSparkContext(sc) &#123; checkSpeculatableTasks() &#125;：这是一个匿名函数，作为定时任务的执行逻辑。Utils.tryOrStopSparkContext(sc)：捕获任务执行过程中的异常，并在发生异常时停止 SparkContext。checkSpeculatableTasks()：检查是否有需要推测执行的任务。</span><br><span class="line">//			SPECULATION_INTERVAL_MS：这是推测执行检查的时间间隔，默认值为 100 毫秒（可以通过 spark.speculation.interval 配置）。</span><br><span class="line">      speculationScheduler.scheduleWithFixedDelay(</span><br><span class="line">        () =&gt; Utils.tryOrStopSparkContext(sc) &#123; checkSpeculatableTasks() &#125;,</span><br><span class="line">        SPECULATION_INTERVAL_MS, SPECULATION_INTERVAL_MS, TimeUnit.MILLISECONDS)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><span class="line">	/**</span><br><span class="line">	 * start()函数负责初始化并启动Standalone模式下的调度后端，包括创建Executor启动命令、设置资源需求、启动客户端、等待注册并更新应用状态，</span><br><span class="line">	 * 确保Spark应用顺利运行。</span><br><span class="line">	 */</span><br><span class="line">  override def start(): Unit = &#123;</span><br><span class="line">//    调用 CoarseGrainedSchedulerBackend 的 start</span><br><span class="line">//    StandaloneSchedulerBackend 在启动时，会调用其父类 CoarseGrainedSchedulerBackend 的 start() 方法，该方法会向 Master 注册 Driver。</span><br><span class="line">    super.start()</span><br><span class="line"></span><br><span class="line">    // SPARK-21159. The scheduler backend should only try to connect to the launcher when in client</span><br><span class="line">    // mode. In cluster mode, the code that submits the application to the Master needs to connect</span><br><span class="line">    // to the launcher instead.</span><br><span class="line">		/**</span><br><span class="line">		 * Client模式：Driver进程运行在提交应用的客户端机器上。</span><br><span class="line">		 * Cluster模式：Driver进程运行在集群中的某个节点上（由集群管理器启动）。</span><br><span class="line">		 * launcherBackend是LauncherBackend的实例，用于与Spark应用的启动器（如spark-submit）通信，报告应用的状态</span><br><span class="line">		 * （如SUBMITTED、RUNNING、FINISHED等）。connect()的作用是建立与启动器的连接，以便在应用运行期间发送状态更新。</span><br><span class="line">		 * Client模式：</span><br><span class="line">		 * Driver运行在客户端机器上，与启动器（如spark-submit）在同一环境中。</span><br><span class="line">		 * 启动器需要实时获取应用的状态（如是否启动成功、是否运行中、是否完成等），因此需要通过launcherBackend.connect()建立连接，</span><br><span class="line">		 * 以便将状态信息发送回启动器。</span><br><span class="line">		 * 如果未建立连接，启动器无法感知应用的状态变化，可能导致用户无法及时了解应用的运行情况。</span><br><span class="line">		 * Cluster模式：</span><br><span class="line">		 * Driver运行在集群中的某个节点上，与启动器（如spark-submit）不在同一环境中。</span><br><span class="line">		 * 启动器在提交应用后，通常不会继续监控应用的状态（因为Driver已经在集群中运行），因此不需要通过launcherBackend.connect()建立连接。</span><br><span class="line">		 * 集群管理器（如Standalone Master、YARN ResourceManager等）会负责监控应用的状态，并将状态信息反馈给用户。</span><br><span class="line">		 *</span><br><span class="line">		 * Client模式：Driver运行在客户端，启动器需要实时获取应用状态，因此需要调用launcherBackend.connect()建立连接。</span><br><span class="line">		 * Cluster模式：Driver运行在集群中，启动器不直接监控应用状态，因此不需要调用launcherBackend.connect()。</span><br><span class="line">		 */</span><br><span class="line">    if (sc.deployMode == &quot;client&quot;) &#123;</span><br><span class="line">      launcherBackend.connect()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // The endpoint for executors to talk to us</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：构建Driver的RPC地址，Executor需要通过该地址与Driver通信。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * sc.conf.get(config.DRIVER_HOST_ADDRESS)：从配置中获取Driver的主机地址。</span><br><span class="line">		 * sc.conf.get(config.DRIVER_PORT)：从配置中获取Driver的端口。</span><br><span class="line">		 * CoarseGrainedSchedulerBackend.ENDPOINT_NAME：Driver的RPC端点名称（固定为CoarseGrainedScheduler）。</span><br><span class="line">		 * 返回值：driverUrl是一个字符串，格式为spark://&lt;driver_host&gt;:&lt;driver_port&gt;，Executor会使用该URL连接到Driver。</span><br><span class="line">		 */</span><br><span class="line">    val driverUrl = RpcEndpointAddress(</span><br><span class="line">      sc.conf.get(config.DRIVER_HOST_ADDRESS),</span><br><span class="line">      sc.conf.get(config.DRIVER_PORT),</span><br><span class="line">      CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：定义启动Executor时需要的命令行参数。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * --driver-url：Driver的RPC地址，Executor通过该地址与Driver通信</span><br><span class="line">		 * --executor-id：Executor的唯一ID，由Worker节点动态生成并替换&#123;&#123;EXECUTOR_ID&#125;&#125;</span><br><span class="line">		 * --hostname：Executor运行的主机名，由Worker节点动态生成并替换&#123;&#123;HOSTNAME&#125;&#125;</span><br><span class="line">		 * --cores：Executor分配的CPU核数，由Worker节点动态生成并替换&#123;&#123;CORES&#125;&#125;</span><br><span class="line">		 * --app-id：Spark应用的唯一ID，由Worker节点动态生成并替换&#123;&#123;APP_ID&#125;&#125;</span><br><span class="line">		 * --worker-url：Worker节点的RPC地址，由Worker节点动态生成并替换&#123;&#123;WORKER_URL&#125;&#125;</span><br><span class="line">		 * 说明：这些参数中的占位符（如&#123;&#123;EXECUTOR_ID&#125;&#125;）会在Executor启动时由Worker节点替换为实际值。</span><br><span class="line">		 */</span><br><span class="line">    val args = Seq(</span><br><span class="line">      &quot;--driver-url&quot;, driverUrl,</span><br><span class="line">      &quot;--executor-id&quot;, &quot;&#123;&#123;EXECUTOR_ID&#125;&#125;&quot;,</span><br><span class="line">      &quot;--hostname&quot;, &quot;&#123;&#123;HOSTNAME&#125;&#125;&quot;,</span><br><span class="line">      &quot;--cores&quot;, &quot;&#123;&#123;CORES&#125;&#125;&quot;,</span><br><span class="line">      &quot;--app-id&quot;, &quot;&#123;&#123;APP_ID&#125;&#125;&quot;,</span><br><span class="line">      &quot;--worker-url&quot;, &quot;&#123;&#123;WORKER_URL&#125;&#125;&quot;)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：从配置中获取额外的JVM参数，用于启动Executor进程。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * config.EXECUTOR_JAVA_OPTIONS：配置项spark.executor.extraJavaOptions，用于设置Executor的JVM参数。</span><br><span class="line">		 * Utils.splitCommandString：将字符串形式的JVM参数拆分为一个字符串序列。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有额外的JVM参数。</span><br><span class="line">		 */</span><br><span class="line">    val extraJavaOpts = sc.conf.get(config.EXECUTOR_JAVA_OPTIONS)</span><br><span class="line">      .map(Utils.splitCommandString).getOrElse(Seq.empty)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：从配置中获取Executor的类路径。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * config.EXECUTOR_CLASS_PATH：配置项spark.executor.extraClassPath，用于设置Executor的额外类路径。</span><br><span class="line">		 * split(java.io.File.pathSeparator)：将类路径字符串按路径分隔符拆分为多个路径。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有类路径条目。</span><br><span class="line">		 */</span><br><span class="line">    val classPathEntries = sc.conf.get(config.EXECUTOR_CLASS_PATH)</span><br><span class="line">      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：从配置中获取Executor的库路径。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * config.EXECUTOR_LIBRARY_PATH：配置项spark.executor.extraLibraryPath，用于设置Executor的额外库路径。</span><br><span class="line">		 * split(java.io.File.pathSeparator)：将库路径字符串按路径分隔符拆分为多个路径。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有库路径条目。</span><br><span class="line">		 */</span><br><span class="line">    val libraryPathEntries = sc.conf.get(config.EXECUTOR_LIBRARY_PATH)</span><br><span class="line">      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)</span><br><span class="line">    // When testing, expose the parent class path to the child. This is processed by</span><br><span class="line">    // compute-classpath.&#123;cmd,sh&#125; and makes all needed jars available to child processes</span><br><span class="line">    // when the assembly is built with the &quot;*-provided&quot; profiles enabled.</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：在测试环境下，将父进程的类路径传递给子进程（Executor）。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * IS_TESTING.key：判断是否处于测试环境的配置项。</span><br><span class="line">		 * sys.props(&quot;java.class.path&quot;)：获取当前JVM进程的类路径。</span><br><span class="line">		 * 返回值：如果是测试环境，返回父进程的类路径；否则返回空列表。</span><br><span class="line">		 */</span><br><span class="line">    val testingClassPath =</span><br><span class="line">      if (sys.props.contains(IS_TESTING.key)) &#123;</span><br><span class="line">        sys.props(&quot;java.class.path&quot;).split(java.io.File.pathSeparator).toSeq</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        Nil</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    // Start executors with a few necessary configs for registering with the scheduler</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：构建Executor的完整JVM参数。</span><br><span class="line">		 * 参数：</span><br><span class="line">		 * Utils.sparkJavaOpts：从配置中提取与Executor启动相关的JVM参数。</span><br><span class="line">		 * SparkConf.isExecutorStartupConf：过滤出与Executor启动相关的配置项。</span><br><span class="line">		 * extraJavaOpts：额外的JVM参数。</span><br><span class="line">		 * 返回值：返回一个字符串序列（Seq[String]），包含所有JVM参数。</span><br><span class="line">		 */</span><br><span class="line">    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)</span><br><span class="line">    val javaOpts = sparkJavaOpts ++ extraJavaOpts</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：定义启动Executor进程的命令。</span><br><span class="line">		 * &quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;：Executor的主类。</span><br><span class="line">		 * args：传递给Executor的参数</span><br><span class="line">		 * sc.executorEnvs：Executor的环境变量。</span><br><span class="line">		 * classPathEntries ++ testingClassPath：Executor的类路径</span><br><span class="line">		 * libraryPathEntries：Executor的库路径。</span><br><span class="line">		 * javaOpts：JVM选项。</span><br><span class="line">		 */</span><br><span class="line">    val command = Command(&quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;,</span><br><span class="line">      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：获取Spark应用的Web UI URL，用于监控和调试。</span><br><span class="line">		 */</span><br><span class="line">    val webUrl = sc.ui.map(_.webUrl).getOrElse(&quot;&quot;)</span><br><span class="line">//		从配置中获取每个Executor的CPU核数。</span><br><span class="line">    val coresPerExecutor = conf.getOption(config.EXECUTOR_CORES.key).map(_.toInt)</span><br><span class="line">    // If we&#x27;re using dynamic allocation, set our initial executor limit to 0 for now.</span><br><span class="line">    // ExecutorAllocationManager will send the real initial limit to the Master later.</span><br><span class="line">		/**</span><br><span class="line">		 * 作用：如果启用了动态资源分配，初始Executor限制设为0，后续由ExecutorAllocationManager调整。</span><br><span class="line">		 */</span><br><span class="line">    val initialExecutorLimit =</span><br><span class="line">      if (Utils.isDynamicAllocationEnabled(conf)) &#123;</span><br><span class="line">        Some(0)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        None</span><br><span class="line">      &#125;</span><br><span class="line">//		解析Executor的资源需求，如GPU、内存等。</span><br><span class="line">    val executorResourceReqs = ResourceUtils.parseResourceRequirements(conf,</span><br><span class="line">      config.SPARK_EXECUTOR_PREFIX)</span><br><span class="line">//  如果未指定，默认为 0</span><br><span class="line">    val priority = conf.getInt(&quot;spark.submit.priority&quot;, 0)</span><br><span class="line">		/**</span><br><span class="line">		 * 描述Spark应用的基本信息。</span><br><span class="line">		 * sc.appName：应用名称。</span><br><span class="line">		 * maxCores：最大CPU核数。</span><br><span class="line">		 * sc.executorMemory：Executor内存。</span><br><span class="line">		 * command：启动Executor的命令。</span><br><span class="line">		 * webUrl：Web UI URL。</span><br><span class="line">		 * sc.eventLogDir：事件日志目录。</span><br><span class="line">		 * sc.eventLogCodec：事件日志编码。</span><br><span class="line">		 * coresPerExecutor：每个Executor的核数。</span><br><span class="line">		 * initialExecutorLimit：初始Executor限制。</span><br><span class="line">		 * executorResourceReqs：Executor资源需求。</span><br><span class="line">		 * priority：应用优先级。</span><br><span class="line">		 */</span><br><span class="line">    val appDesc = ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,</span><br><span class="line">      webUrl, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit,</span><br><span class="line">      resourceReqsPerExecutor = executorResourceReqs, priority = priority)</span><br><span class="line"></span><br><span class="line">		/**</span><br><span class="line">		 * 创建与集群管理器通信的客户端。</span><br><span class="line">		 * sc.env.rpcEnv：RPC环境。</span><br><span class="line">		 * masters：集群管理器地址。</span><br><span class="line">		 * appDesc：应用描述。</span><br><span class="line">		 * this：当前StandaloneSchedulerBackend实例。</span><br><span class="line">		 * conf：配置对象。</span><br><span class="line">		 */</span><br><span class="line">    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)</span><br><span class="line">//		启动客户端，开始与集群管理器通信。</span><br><span class="line">    client.start()</span><br><span class="line">//		将应用状态设置为SUBMITTED，表示应用已提交。</span><br><span class="line">    launcherBackend.setState(SparkAppHandle.State.SUBMITTED)</span><br><span class="line">//		等待Executor注册完成，确保资源分配到位。</span><br><span class="line">    waitForRegistration()</span><br><span class="line">//		将应用状态设置为RUNNING，表示应用正在运行。</span><br><span class="line">    launcherBackend.setState(SparkAppHandle.State.RUNNING)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析-1"><a href="#（2）代码解析-1" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>代码路径为：org.apache.spark.SparkContext</li>
<li>通过调用 TaskSchedulerImpl 类的 start() 方法，启动任务调度器的后端，准备接收资源分配和任务调度请求</li>
<li>通过 TaskSchedulerImpl 类的 start 方法，来启动 StandaloneSchedulerBackend 类的 start 方法，该 start 方法建立该 app 的描述信息，启动客户端，开始与集群管理器建立 rpc 通信，根据 app 应用状态来更新该 app 的状态</li>
</ul>
<h4 id="（3）SparkContext-初始化过程中，函数调用流程图"><a href="#（3）SparkContext-初始化过程中，函数调用流程图" class="headerlink" title="（3）SparkContext() 初始化过程中，函数调用流程图"></a>（3）SparkContext() 初始化过程中，函数调用流程图</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">+---------------------+</span><br><span class="line">| SparkContext 构造函数 | → 入口：`class SparkContext(config: SparkConf)`</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| validateSettings()   | → 校验配置合法性（如 `master` 和 `appName` 非空）</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| createSparkEnv()     | → 创建 Spark 运行时环境</span><br><span class="line">|   ↓                 |</span><br><span class="line">|   SparkEnv.create()  | → 核心子组件初始化：</span><br><span class="line">|     |- createRpcEnv() → 创建 RPC 通信环境（如 NettyRpcEnv）</span><br><span class="line">|     |- serializerManager → 序列化管理器</span><br><span class="line">|     |- blockManager → 块管理器</span><br><span class="line">|     |- securityManager → 安全管理器</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| createTaskScheduler()| → 根据 `master` 参数创建调度器</span><br><span class="line">|   ↓                 |</span><br><span class="line">|   match master:      |</span><br><span class="line">|     |- &quot;local&quot; → TaskSchedulerImpl</span><br><span class="line">|     |- &quot;yarn&quot; → YarnScheduler</span><br><span class="line">|     |- &quot;k8s&quot; → KubernetesClusterScheduler</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| taskScheduler.start()| → 启动 TaskScheduler</span><br><span class="line">|   ↓                 |</span><br><span class="line">|   SchedulerBackend.start() → 与集群管理器通信（如 YARN RM）</span><br><span class="line">|     |- 注册 Driver，申请 Executor 资源</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| new DAGScheduler()   | → 初始化 DAG 调度器</span><br><span class="line">|   ↓                 |</span><br><span class="line">|   eventProcessLoop → 启动 DAGSchedulerEventProcessLoop</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| setupHeartbeatReceiver() → 启动心跳接收器</span><br><span class="line">|   ↓                 |</span><br><span class="line">|   HeartbeatReceiver → 监听 Executor 心跳</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| 初始化其他组件        | → 包括：</span><br><span class="line">|   |- ContextCleaner → 清理无用 RDD/Shuffle 数据</span><br><span class="line">|   |- MetricsSystem → 监控指标系统</span><br><span class="line">|   |- EventLoggingListener → 事件日志记录</span><br><span class="line">+---------------------+</span><br><span class="line">           ↓</span><br><span class="line">+---------------------+</span><br><span class="line">| _status = Live      | → 标记 SparkContext 为激活状态</span><br><span class="line">+---------------------+</span><br></pre></td></tr></table></figure>
<h3 id="3，代码解析为-DAG"><a href="#3，代码解析为-DAG" class="headerlink" title="3，代码解析为 DAG"></a>3，代码解析为 DAG</h3><h4 id="（1）代码-2"><a href="#（1）代码-2" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    /**</span><br><span class="line">       * Return an array that contains all of the elements in this RDD.</span><br><span class="line">       *</span><br><span class="line">       * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">       * all the data is loaded into the driver&#x27;s memory.</span><br><span class="line">       */</span><br><span class="line">      def collect(): Array[T] = withScope &#123;</span><br><span class="line">        val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)</span><br><span class="line">        Array.concat(results: _*)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">-   代码路径：org.apache.spark.rdd.RDD#collect</span><br><span class="line">-   collect() 是一个动作（Action）操作，它会触发 Spark 的作业（Job）执行，调用 SparkContext 里面的 runJob 函数</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   /**</span><br><span class="line">   * RDD.count def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum 启动调用</span><br><span class="line">   * Run a function on a given set of partitions in an RDD and pass the results to the given</span><br><span class="line">   * handler function. This is the main entry point for all actions in Spark.</span><br><span class="line">   *</span><br><span class="line">   * 在 Spark 3.3.0 的源码中，SparkContext 类的 runJob 方法是 Spark 作业执行的核心入口，</span><br><span class="line">   * 负责将用户定义的计算逻辑（通过 func 函数）分发到集群执行，并处理结果。</span><br><span class="line">   *</span><br><span class="line">   * @param rdd target RDD to run tasks on</span><br><span class="line">   * @param func a function to run on each partition of the RDD</span><br><span class="line">   * @param partitions set of partitions to run on; some jobs may not want to compute on all</span><br><span class="line">   * partitions of the target RDD, e.g. for operations like `first()`</span><br><span class="line">   * @param resultHandler callback to pass each result to</span><br><span class="line">   */</span><br><span class="line">  def runJob[T, U: ClassTag](                      // 泛型参数：</span><br><span class="line">                                    rdd: RDD[T],   // - T: 输入 RDD 的元素类型</span><br><span class="line">                                    func: (TaskContext, Iterator[T]) =&gt; U,  // - U: 每个分区的计算结果类型</span><br><span class="line">                                    partitions: Seq[Int],    // 要处理的分区 ID 列表（如不指定，默认所有分区）</span><br><span class="line">                                    resultHandler: (Int, U) =&gt; Unit    // 结果处理回调函数（按分区处理结果）</span><br><span class="line">                            ): Unit = &#123;</span><br><span class="line">//    确保 SparkContext 未关闭。若已关闭，抛出异常，避免在无效状态下提交作业。</span><br><span class="line">    if (stopped.get()) &#123;</span><br><span class="line">      throw new IllegalStateException(&quot;SparkContext has been shutdown&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">//    获取调用栈信息（如用户代码中触发 runJob 的位置），用于日志和调试</span><br><span class="line">    val callSite = getCallSite</span><br><span class="line">//    作用: 对用户传入的 func 进行闭包清理（Closure Cleaning），确保闭包中的变量可序列化</span><br><span class="line">//    背景: Spark 需要将 func 序列化后发送到 Executor，闭包中可能包含不可序列化的引用（如 this 指针），需通过清理避免序列化失败</span><br><span class="line">    val cleanedFunc = clean(func)</span><br><span class="line">    logInfo(&quot;Starting job: &quot; + callSite.shortForm)</span><br><span class="line">//    作用: 如果配置了 spark.logLineage=true，输出 RDD 的血缘关系（递归依赖链），用于调试</span><br><span class="line">    if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) &#123;</span><br><span class="line">      /**</span><br><span class="line">       * 示例输出:</span><br><span class="line">       * (4) MapPartitionsRDD[3] at map at MyApp.scala:25 []</span><br><span class="line">       *  |  ShuffledRDD[2] at groupByKey at MyApp.scala:23 []</span><br><span class="line">       *  +-(4) ParallelCollectionRDD[1] at parallelize at MyApp.scala:22 []</span><br><span class="line">       */</span><br><span class="line">      logInfo(&quot;RDD&#x27;s recursive dependencies:\n&quot; + rdd.toDebugString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 作用: 将作业提交给 DAGScheduler，触发 DAG 的构建、Stage 划分和 Task 调度。</span><br><span class="line">     *  参数传递:</span><br><span class="line">     *  rdd: 目标 RDD。</span><br><span class="line">     *  cleanedFunc: 清理后的用户函数。</span><br><span class="line">     *  partitions: 需处理的分区列表。</span><br><span class="line">     *  callSite: 调用位置信息。</span><br><span class="line">     *  resultHandler: 结果回调函数。</span><br><span class="line">     *  localProperties.get: 当前线程的本地属性（如 Spark SQL 的 spark.sql.sessionState）</span><br><span class="line">     */</span><br><span class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">//    作用: 如果启用了进度条（通过 spark.ui.showConsoleProgress=true），标记所有任务完成。</span><br><span class="line">//    背景: Spark 在控制台显示作业进度条，需在作业完成后更新状态。</span><br><span class="line">    progressBar.foreach(_.finishAll())</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 作用: 如果 RDD 启用了检查点（rdd.checkpoint() 被调用过），将 RDD 的物化数据持久化到可靠存储（如 HDFS）。</span><br><span class="line">     * 背景: 检查点用于切断 RDD 的血缘关系，避免重复计算长依赖链</span><br><span class="line">     */</span><br><span class="line">    rdd.doCheckpoint()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径为：org.apache.spark.SparkContext#runJob</li>
<li>将作业提交给 DAGScheduler，触发 DAG 的构建、Stage 划分和 Task 调度</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">  def runJob[T, U](</span><br><span class="line">      rdd: RDD[T],   // 目标 RDD，作业的计算起点</span><br><span class="line">      func: (TaskContext, Iterator[T]) =&gt; U,  // 每个分区上执行的计算逻辑</span><br><span class="line">      partitions: Seq[Int],  // 指定要处理的分区 ID 列表</span><br><span class="line">      callSite: CallSite,  // 调用位置信息（用于日志和调试）</span><br><span class="line">      resultHandler: (Int, U) =&gt; Unit,  // 处理每个分区计算结果的回调函数</span><br><span class="line">      properties: Properties  // 作业属性（如调度优先级、作业组）</span><br><span class="line">                  ): Unit = &#123;</span><br><span class="line">//    作用：记录作业提交的起始时间，后续用于计算作业执行耗时</span><br><span class="line">    val start = System.nanoTime</span><br><span class="line">    /**</span><br><span class="line">     *  调用 submitJob 方法提交作业到 DAGScheduler，返回一个 JobWaiter 对象</span><br><span class="line">     * submitJob 的流程：</span><br><span class="line">     *  生成唯一的作业 ID（通过原子计数器递增）。</span><br><span class="line">     *  创建 JobWaiter：用于监听作业完成事件，统计任务完成状态。</span><br><span class="line">     *  发送 JobSubmitted 事件：将作业提交到 DAGScheduler 的事件队列。</span><br><span class="line">     *  触发 DAG 调度：DAGScheduler 的事件循环线程处理该事件，开始划分 Stages。</span><br><span class="line">     */</span><br><span class="line">    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class="line">    /**</span><br><span class="line">     * 作用：当前线程（通常是 Driver 线程）阻塞等待，直到作业完成（或失败）。</span><br><span class="line">     * 实现细节：</span><br><span class="line">     *  waiter.completionFuture 是一个 Future 对象，表示作业的异步执行结果。</span><br><span class="line">     *  Duration.Inf 表示无限等待，直到 Future 完成。</span><br><span class="line">     */</span><br><span class="line">    ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf)</span><br><span class="line">    /**</span><br><span class="line">     * 作业成功：</span><br><span class="line">     *  记录作业完成的日志，包含作业 ID、调用位置和耗时。</span><br><span class="line">     *  作业失败：</span><br><span class="line">     *  记录作业失败的日志。</span><br><span class="line">     *  合并堆栈追踪：将当前线程（Driver）的堆栈追踪追加到原始异常中，便于调试时定位用户代码触发错误的位置。</span><br><span class="line">     *  抛出异常，终止当前任务。</span><br><span class="line">     */</span><br><span class="line">    waiter.completionFuture.value.get match &#123;</span><br><span class="line">      case scala.util.Success(_) =&gt;</span><br><span class="line">        logInfo(&quot;Job %d finished: %s, took %f s&quot;.format</span><br><span class="line">          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))</span><br><span class="line">      case scala.util.Failure(exception) =&gt;</span><br><span class="line">        logInfo(&quot;Job %d failed: %s, took %f s&quot;.format</span><br><span class="line">          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))</span><br><span class="line">        // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span><br><span class="line">        val callerStackTrace = Thread.currentThread().getStackTrace.tail</span><br><span class="line">        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">        throw exception</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>调用 submitJob 方法提交作业到 DAGScheduler，返回一个 JobWaiter 对象</li>
<li>当前线程（通常是 Driver 线程）阻塞等待，直到作业完成（或失败）</li>
<li>等异步执行全部完成后，记录作业成功或者失败</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Submit an action job to the scheduler.</span><br><span class="line">   *</span><br><span class="line">   * @param rdd target RDD to run tasks on 任务的输入 RDD。</span><br><span class="line">   * @param func a function to run on each partition of the RDD  每个分区的计算函数，接收 TaskContext 和分区的数据迭代器，返回计算结果。</span><br><span class="line">   * @param partitions set of partitions to run on; some jobs may not want to compute on all  需要计算的分区列表。</span><br><span class="line">   *   partitions of the target RDD, e.g. for operations like first() 提交作业的代码位置，用于跟踪任务来源。</span><br><span class="line">   * @param callSite where in the user program this job was called 用于处理分区计算结果的回调函数。</span><br><span class="line">   * @param resultHandler callback to pass each result to  用于处理分区计算结果的回调函数。</span><br><span class="line">   * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name 作业属性，包括用户设置的本地属性等。</span><br><span class="line">   *</span><br><span class="line">   * @return a JobWaiter object that can be used to block until the job finishes executing</span><br><span class="line">   *         or can be used to cancel the job.  用于等待作业完成的同步机制，支持结果处理。</span><br><span class="line">   *</span><br><span class="line">   * @throws IllegalArgumentException when partitions ids are illegal</span><br><span class="line">   *</span><br><span class="line">   * 函数入口是 submitJob，这是外部向 DAGScheduler 提交 Job 的主要入口。它会根据提交的 RDD 和 Action，生成一个新的 Job 对象，并调用 submitStage 方法。</span><br><span class="line">   *</span><br><span class="line">   */</span><br><span class="line">  def submitJob[T, U](</span><br><span class="line">      rdd: RDD[T],</span><br><span class="line">      func: (TaskContext, Iterator[T]) =&gt; U,</span><br><span class="line">      partitions: Seq[Int],</span><br><span class="line">      callSite: CallSite,</span><br><span class="line">      resultHandler: (Int, U) =&gt; Unit,</span><br><span class="line">      properties: Properties): JobWaiter[U] = &#123;</span><br><span class="line">    // Check to make sure we are not launching a task on a partition that does not exist.</span><br><span class="line">//    获取 RDD 的分区数量（maxPartitions）。</span><br><span class="line">    val maxPartitions = rdd.partitions.length</span><br><span class="line">//    检查 partitions 中的分区索引是否超出范围或小于 0。如果存在非法分区索引，则抛出 IllegalArgumentException。</span><br><span class="line">//        目的：确保提交的任务不会访问不存在的分区。</span><br><span class="line">    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;</span><br><span class="line">      throw new IllegalArgumentException(</span><br><span class="line">        &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +</span><br><span class="line">          &quot;Total number of partitions: &quot; + maxPartitions)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // SPARK-23626: `RDD.getPartitions()` can be slow, so we eagerly compute</span><br><span class="line">    // `.partitions` on every RDD in the DAG to ensure that `getPartitions()`</span><br><span class="line">    // is evaluated outside of the DAGScheduler&#x27;s single-threaded event loop:</span><br><span class="line">//    递归计算 rdd 和所有父 RDD 的分区信息。</span><br><span class="line">//    解决了 RDD.getPartitions() 可能在单线程事件循环中造成的性能瓶颈问题（参考 SPARK-23626）。</span><br><span class="line">    eagerlyComputePartitionsForRddAndAncestors(rdd)</span><br><span class="line">//    获取下一个作业的唯一 ID（nextJobId 是一个原子计数器）。</span><br><span class="line">//    用途：每个作业都需要一个唯一 ID，方便作业追踪和调度。</span><br><span class="line">    val jobId = nextJobId.getAndIncrement()</span><br><span class="line">//      如果 partitions 是空的，则立即标记作业成功，无需实际提交任务。</span><br><span class="line">    if (partitions.isEmpty) &#123;</span><br><span class="line">//      克隆作业属性（properties）以确保隔离性。</span><br><span class="line">      val clonedProperties = Utils.cloneProperties(properties)</span><br><span class="line">      if (sc.getLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION) == null) &#123;</span><br><span class="line">//        如果作业没有描述信息，设置默认描述（callSite.shortForm）。</span><br><span class="line">        clonedProperties.setProperty(SparkContext.SPARK_JOB_DESCRIPTION, callSite.shortForm)</span><br><span class="line">      &#125;</span><br><span class="line">      val time = clock.getTimeMillis()</span><br><span class="line">//      使用 listenerBus 发布 SparkListenerJobStart 和 SparkListenerJobEnd 事件。</span><br><span class="line">      listenerBus.post(</span><br><span class="line">        SparkListenerJobStart(jobId, time, Seq.empty, clonedProperties))</span><br><span class="line">      listenerBus.post(</span><br><span class="line">        SparkListenerJobEnd(jobId, time, JobSucceeded))</span><br><span class="line">      // Return immediately if the job is running 0 tasks</span><br><span class="line">//      返回一个空的 JobWaiter，作业完成。</span><br><span class="line">      return new JobWaiter[U](this, jobId, 0, resultHandler)</span><br><span class="line">    &#125;</span><br><span class="line">//    确保分区非空（assert(partitions.nonEmpty)）。</span><br><span class="line">    assert(partitions.nonEmpty)</span><br><span class="line">//    将用户传入的 func 强制转换为通用的函数类型（func2）。</span><br><span class="line">    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</span><br><span class="line">//    创建 JobWaiter：用于跟踪作业的完成状态。包含作业的 ID、分区数量以及结果处理逻辑。</span><br><span class="line">    val waiter = new JobWaiter[U](this, jobId, partitions.size, resultHandler)</span><br><span class="line">//    包括作业的基本信息（jobId、rdd、func2、分区等）。</span><br><span class="line">//    克隆作业属性（properties）以保证线程安全。</span><br><span class="line">//    将事件发送到 eventProcessLoop（调度器的事件循环）。事件会在 DAG Scheduler 的事件队列中处理，并最终触发任务调度。</span><br><span class="line">    eventProcessLoop.post(JobSubmitted(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      Utils.cloneProperties(properties)))</span><br><span class="line">    waiter</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>将事件发送到 eventProcessLoop（调度器的事件循环）。事件会在 DAG Scheduler 的事件队列中处理，并最终触发任务调度</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123;</span><br><span class="line">    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>eventProcessLoop.start() 启动了事件循环，doOnReceive 函数接收了 JobSubmitted 消息，触发了 handleJobSubmitted 函数</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  private[scheduler] def handleJobSubmitted(jobId: Int,</span><br><span class="line">      finalRDD: RDD[_],</span><br><span class="line">      func: (TaskContext, Iterator[_]) =&gt; _,</span><br><span class="line">      partitions: Array[Int],</span><br><span class="line">      callSite: CallSite,</span><br><span class="line">      listener: JobListener,</span><br><span class="line">      properties: Properties): Unit = &#123;</span><br><span class="line">    var finalStage: ResultStage = null</span><br><span class="line">    try &#123;</span><br><span class="line">      // New stage creation may throw an exception if, for example, jobs are run on a</span><br><span class="line">      // HadoopRDD whose underlying HDFS files have been deleted.</span><br><span class="line">//      这一段代码的作用是创建一个表示整个 Job 最终计算阶段的 ResultStage，从而为 DAGScheduler 提交并管理这个 Job 提供核心抽象。</span><br><span class="line">      /**</span><br><span class="line">       * 为何优先创建 ResultStage？</span><br><span class="line">       * 1，ResultStage 是 Job 的最终输出阶段</span><br><span class="line">       * 每个 Spark Job 的任务是从输入数据出发，通过一系列依赖（RDD 转换）生成最终的结果（例如写入磁盘、显示结果等）。</span><br><span class="line">       * ResultStage 是 DAG 的叶子节点，代表 Job 的最终结果。创建 ResultStage 意味着为 Job 定义了目标阶段，DAGScheduler 可以根据这个阶段向上推导其所有依赖（父阶段）。</span><br><span class="line">       * 2,计算整个 DAG 的依赖关系：</span><br><span class="line">       * createResultStage 不仅创建了 ResultStage，还触发了对整个 DAG 的依赖关系分析。</span><br><span class="line">       * 它会检查 finalRDD 的所有依赖链，找到其父 ShuffleMapStage，从而明确了任务调度的顺序。</span><br><span class="line">       * 这样，DAGScheduler 可以从 ResultStage 开始向上逐层解析所有需要提交的阶段，确保依赖的父阶段（如 ShuffleMapStage）已被正确计算。</span><br><span class="line">       * 3,便于提交任务：</span><br><span class="line">       * handleJobSubmitted 的最终目的是提交 Job 对应的所有任务。创建 ResultStage 后，DAGScheduler 会调用 submitStage(finalStage)</span><br><span class="line">       * 方法，递归提交当前阶段及其所有未完成的父阶段。</span><br><span class="line">       * ResultStage 是整个任务提交的起点，其递归逻辑负责确保父阶段（如 ShuffleMapStage）优先执行。</span><br><span class="line">       */</span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: BarrierJobSlotsNumberCheckFailed =&gt;</span><br><span class="line">        // If jobId doesn&#x27;t exist in the map, Scala coverts its value null to 0: Int automatically.</span><br><span class="line">        val numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">          (_: Int, value: Int) =&gt; value + 1)</span><br><span class="line"></span><br><span class="line">        logWarning(s&quot;Barrier stage in job $jobId requires $&#123;e.requiredConcurrentTasks&#125; slots, &quot; +</span><br><span class="line">          s&quot;but only $&#123;e.maxConcurrentTasks&#125; are available. &quot; +</span><br><span class="line">          s&quot;Will retry up to $&#123;maxFailureNumTasksCheck - numCheckFailures + 1&#125; more times&quot;)</span><br><span class="line"></span><br><span class="line">        if (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">          messageScheduler.schedule(</span><br><span class="line">            new Runnable &#123;</span><br><span class="line">              override def run(): Unit = eventProcessLoop.post(JobSubmitted(jobId, finalRDD, func,</span><br><span class="line">                partitions, callSite, listener, properties))</span><br><span class="line">            &#125;,</span><br><span class="line">            timeIntervalNumTasksCheck,</span><br><span class="line">            TimeUnit.SECONDS</span><br><span class="line">          )</span><br><span class="line">          return</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          // Job failed, clear internal data.</span><br><span class="line">          barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">          listener.jobFailed(e)</span><br><span class="line">          return</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        logWarning(&quot;Creating new stage failed due to exception - job: &quot; + jobId, e)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line">    // Job submitted, clear internal data.</span><br><span class="line">    barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line"></span><br><span class="line">    val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">    clearCacheLocs()</span><br><span class="line">    logInfo(&quot;Got job %s (%s) with %d output partitions&quot;.format(</span><br><span class="line">      job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">    logInfo(&quot;Final stage: &quot; + finalStage + &quot; (&quot; + finalStage.name + &quot;)&quot;)</span><br><span class="line">    logInfo(&quot;Parents of final stage: &quot; + finalStage.parents)</span><br><span class="line">    logInfo(&quot;Missing parents: &quot; + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">    val jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">    jobIdToActiveJob(jobId) = job</span><br><span class="line">    activeJobs += job</span><br><span class="line">    finalStage.setActiveJob(job)</span><br><span class="line">    val stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">    val stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">    listenerBus.post(</span><br><span class="line">      SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos,</span><br><span class="line">        Utils.cloneProperties(properties)))</span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>开始从最终结果 finalStage 倒退整个依赖关系，经dag 转化为 stage，最终转化为 task</li>
</ul>
<h4 id="（2）代码解析-2"><a href="#（2）代码解析-2" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>简易的代码调用逻辑：org.apache.spark.rdd.RDD#collect -&gt;  org.apache.spark.SparkContext#runJob -&gt; org.apache.spark.scheduler.DAGScheduler#runJob -&gt; org.apache.spark.scheduler.DAGScheduler#submitJob -&gt; org.apache.spark.scheduler.DAGSchedulerEventProcessLoop#doOnReceive#JobSubmitted -&gt; org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted</li>
<li>在 DAGScheduler 里面，是通过 eventProcessLoop.post 将 JobSubmitted 调用 case JobSubmitted。</li>
<li>eventProcessLoop 为调度器的事件循环，事件会在 DAG Scheduler 的事件队列中处理，并最终触发任务调度</li>
</ul>
<h4 id="（3）代码转换为-DAG-函数调用流程图"><a href="#（3）代码转换为-DAG-函数调用流程图" class="headerlink" title="（3）代码转换为 DAG 函数调用流程图"></a>（3）代码转换为 DAG 函数调用流程图</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------+</span><br><span class="line">| Action 触发 (如 collect())   | → 调用 `RDD` 的行动方法</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">| SparkContext.runJob()       | → 入口：`core/src/main/scala/org/apache/spark/SparkContext.scala`</span><br><span class="line">|   ↓ 多次调用runJob()函数    |</span><br><span class="line">|   dagScheduler.runJob()     | → 提交 Job 到 DAGScheduler</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">| dagScheduler.submitJob()    | → 入口：`core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala`</span><br><span class="line">|   ↓ eventProcessLoop.post    |</span><br><span class="line">|   dagScheduler.doOnReceive() | → 提交事件循环信息 JobSubmitted 到 eventProcessLoop，触发 doOnReceive</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+  </span><br><span class="line">| DAGScheduler.handleJobSubmitted() | → 核心逻辑入口：`core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala`</span><br><span class="line">|   ↓                        |</span><br><span class="line">|   1. 创建 ResultStage           | → `newResultStage()`</span><br><span class="line">|   2. 递归查找父 Stage           | → `getMissingParentStages()`</span><br><span class="line">|   3. 提交 Stage 依赖链          | → `submitStage()`</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">| 宽依赖（ShuffleDependency）判断   | → 源码：`core/src/main/scala/org/apache/spark/rdd/ShuffleDependency.scala`</span><br><span class="line">|   ↓                        |</span><br><span class="line">|   if (依赖是宽依赖)             | → 划分新 Stage（ShuffleMapStage）</span><br><span class="line">|   else                      | → 窄依赖，合并到当前 Stage</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">| 递归回溯 RDD 依赖链            | → 函数调用链：</span><br><span class="line">|   ↓                        |   - `getShuffleDependencies()` 收集所有 Shuffle 依赖</span><br><span class="line">|   |                        |   - `getOrCreateParentStages()` 创建父 Stage</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">| Stage 划分完成               | → 生成 Stage 列表（ResultStage + ShuffleMapStages）</span><br><span class="line">+----------------------------+</span><br><span class="line">           ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">| DAGScheduler.submitMissingTasks() | → 生成 TaskSet</span><br><span class="line">|   ↓                        |</span><br><span class="line">|   - 为 Stage 生成 Task（ShuffleMapTask/ResultTask）</span><br><span class="line">|   - 调用 `taskScheduler.submitTasks()` 提交 TaskSet</span><br><span class="line">+----------------------------+</span><br></pre></td></tr></table></figure>
<h3 id="4，DAG-分解为-Task"><a href="#4，DAG-分解为-Task" class="headerlink" title="4，DAG 分解为 Task"></a>4，DAG 分解为 Task</h3><h4 id="（1）代码-3"><a href="#（1）代码-3" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">  override def submitTasks(taskSet: TaskSet): Unit = &#123;</span><br><span class="line">    val tasks = taskSet.tasks</span><br><span class="line">    logInfo(&quot;Adding task set &quot; + taskSet.id + &quot; with &quot; + tasks.length + &quot; tasks &quot;</span><br><span class="line">      + &quot;resource profile &quot; + taskSet.resourceProfileId)</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      val stage = taskSet.stageId</span><br><span class="line">      /**</span><br><span class="line">       * val stageTaskSets = Map(</span><br><span class="line">       *  1 -&gt; TaskSetManager(</span><br><span class="line">       *  taskSet = taskSet1,  // 对应任务集 taskSet1</span><br><span class="line">       *  isZombie = false     // 当前 TaskSetManager 不是僵尸状态</span><br><span class="line">       *  ),</span><br><span class="line">       *  2 -&gt; TaskSetManager(</span><br><span class="line">       *  taskSet = taskSet2,  // 对应任务集 taskSet2</span><br><span class="line">       *  isZombie = true      // 当前 TaskSetManager 被标记为僵尸</span><br><span class="line">       *  )</span><br><span class="line">       *  )</span><br><span class="line">       *  taskSet1 和 taskSet2 是两个不同的任务集，它们可能属于同一个阶段（stageId 为 1），但是由于 stageAttemptId 不同，</span><br><span class="line">       *  它们分别被分配给了不同的 TaskSetManager 实例。</span><br><span class="line">       *  isZombie 字段标记了该任务集是否处于僵尸状态。当新的任务集尝试加入时，旧的 TaskSetManager 被标记为僵尸，</span><br><span class="line">       *  表示它不再活跃，但仍保留其状态。</span><br><span class="line">       *</span><br><span class="line">       * case class TaskSetManager(</span><br><span class="line">       *  taskSet: TaskSet,        // 任务集</span><br><span class="line">       *  isZombie: Boolean        // 是否是僵尸状态</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  // 示例任务集</span><br><span class="line">       *  val taskSet1 = TaskSet(</span><br><span class="line">       *  id = 101,               // 任务集 ID</span><br><span class="line">       *  stageId = 1,            // 阶段 ID</span><br><span class="line">       *  stageAttemptId = 1,     // 阶段尝试 ID</span><br><span class="line">       *  tasks = List(task1, task2) // 任务集包含的任务</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  val taskSet2 = TaskSet(</span><br><span class="line">       *  id = 102,               // 任务集 ID</span><br><span class="line">       *  stageId = 1,            // 阶段 ID</span><br><span class="line">       *  stageAttemptId = 2,     // 阶段尝试 ID</span><br><span class="line">       *  tasks = List(task3, task4) // 任务集包含的任务</span><br><span class="line">       *  )</span><br><span class="line">       *</span><br><span class="line">       *  val task1 = Task(id = 1, data = &quot;task1_data&quot;)</span><br><span class="line">       *  val task2 = Task(id = 2, data = &quot;task2_data&quot;)</span><br><span class="line">       *  val task3 = Task(id = 3, data = &quot;task3_data&quot;)</span><br><span class="line">       *  val task4 = Task(id = 4, data = &quot;task4_data&quot;)</span><br><span class="line">       *  taskSet1 和 taskSet2 代表两个不同的任务集，它们属于同一阶段（stageId = 1），但是具有不同的 stageAttemptId（1 和 2）。</span><br><span class="line">       *  TaskSetManager 会根据这些任务集创建，并根据状态是否为“僵尸”来管理它们。</span><br><span class="line">       */</span><br><span class="line">      val stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])</span><br><span class="line"></span><br><span class="line">      // Mark all the existing TaskSetManagers of this stage as zombie, as we are adding a new one.</span><br><span class="line">      // This is necessary to handle a corner case. Let&#x27;s say a stage has 10 partitions and has 2</span><br><span class="line">      // TaskSetManagers: TSM1(zombie) and TSM2(active). TSM1 has a running task for partition 10</span><br><span class="line">      // and it completes. TSM2 finishes tasks for partition 1-9, and thinks he is still active</span><br><span class="line">      // because partition 10 is not completed yet. However, DAGScheduler gets task completion</span><br><span class="line">      // events for all the 10 partitions and thinks the stage is finished. If it&#x27;s a shuffle stage</span><br><span class="line">      // and somehow it has missing map outputs, then DAGScheduler will resubmit it and create a</span><br><span class="line">      // TSM3 for it. As a stage can&#x27;t have more than one active task set managers, we must mark</span><br><span class="line">      // TSM2 as zombie (it actually is).</span><br><span class="line">      /**</span><br><span class="line">       * 在 Spark 中，任务集（TaskSetManager）是用于管理某个阶段（stage）的任务的。当一个任务集（TaskSetManager）进入僵尸状态时，</span><br><span class="line">       * 它表示该任务集不再活跃，不能继续调度新的任务。因此，将现有的 TaskSetManager 设置为僵尸，主要是为了防止其继续参与到后续的任务调度中。</span><br><span class="line">       *</span><br><span class="line">       * 在 Spark 中，每个阶段（stage）只能有一个活动的任务集管理器（TaskSetManager）。当重新调度任务时，</span><br><span class="line">       * 新的任务集管理器会被添加到 stageTaskSets 中，并且必须标记所有现有的任务集管理器为 &quot;僵尸&quot;，防止它们继续影响任务调度。</span><br><span class="line">       * 这种操作可以避免出现多个任务集管理器同时试图调度同一个阶段的任务的情况。因为如果两个任务集管理器都试图调度同一个阶段，</span><br><span class="line">       * 可能会导致资源竞争或任务调度的混乱。</span><br><span class="line">       * 处理任务集失败的情况：当一个任务集中的任务失败并需要重新调度时，当前的 TaskSetManager 被认为已经不再有效，因此将其标记为 &quot;僵尸&quot;。</span><br><span class="line">       * 然后新的任务集管理器（manager）将接管当前阶段的任务调度。</span><br><span class="line">       * 防止错误的任务集状态：如果不标记现有的 TaskSetManager 为僵尸，可能会导致任务集在某些情况下被错误地认为仍然是活动状态，</span><br><span class="line">       * 进而继续尝试调度任务，这会破坏任务调度的一致性。</span><br><span class="line">       */</span><br><span class="line">      stageTaskSets.foreach &#123; case (_, ts) =&gt;</span><br><span class="line">        ts.isZombie = true</span><br><span class="line">      &#125;</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">//    schedulableBuilder 在初始化的时候，在initialize函数里面指定了FIFOSchedulableBuilder模式或者FairSchedulableBuilder模式</span><br><span class="line">//    调用addTaskSetManager函数是调用的类SchedulableBuilder.scala里面的FIFOSchedulableBuilder或者FairSchedulableBuilder里面的方法</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">      if (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">        starvationTimer.scheduleAtFixedRate(new TimerTask() &#123;</span><br><span class="line">          override def run(): Unit = &#123;</span><br><span class="line">            if (!hasLaunchedTask) &#123;</span><br><span class="line">              logWarning(&quot;Initial job has not accepted any resources; &quot; +</span><br><span class="line">                &quot;check your cluster UI to ensure that workers are registered &quot; +</span><br><span class="line">                &quot;and have sufficient resources&quot;)</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              this.cancel()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)</span><br><span class="line">      &#125;</span><br><span class="line">      hasReceivedTask = true</span><br><span class="line">    &#125;</span><br><span class="line">//  调用在 CoarseGrainedSchedulerBackend.scala 中实现了 reviveOffers 方法</span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径：org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks</li>
<li>schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties) 将包装后的 taskSet 添加到指定的调度队列里面</li>
<li>代码 backend.reviveOffers() 调用在 CoarseGrainedSchedulerBackend.scala 中实现了 reviveOffers 方法</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  override def reviveOffers(): Unit = Utils.tryLogNonFatalError &#123;</span><br><span class="line">//    CoarseGrainedSchedulerBackend：负责与集群中的 Worker 进行通信，管理 Executor，调度任务等。</span><br><span class="line">//    ReviveOffers 类型是 相同的，即固定的 ReviveOffers 消息</span><br><span class="line">//    在每次调用 reviveOffers 方法时，ReviveOffers 都是同一个类型和实例</span><br><span class="line">//    ReviveOffers 的主要功能是通知 Driver端 的调度器重新评估可用资源，并尝试为任务分配资源</span><br><span class="line">//    触发本类的receive方法</span><br><span class="line">    driverEndpoint.send(ReviveOffers) </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径：org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend#reviveOffers</li>
<li>driverEndpoint.send(ReviveOffers) 向 CoarseGrainedSchedulerBackend 内部的 DriverEndpoint 实例发送 ReviveOffers 消息，触发 case ReviveOffers &#x3D;&gt; makeOffers()</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 它的主要作用是将活跃的执行器资源包装成 WorkerOffer 对象，并调用调度器的 resourceOffers() 方法，尝试为任务分配资源。</span><br><span class="line">     * 最后，如果有任务被成功分配，则通过 launchTasks() 将任务发送到对应的 Executor 上运行</span><br><span class="line">     */</span><br><span class="line">    // Make fake resource offers on all executors</span><br><span class="line">    private def makeOffers(): Unit = &#123;</span><br><span class="line">      // Make sure no executor is killed while some task is launching on it</span><br><span class="line">//      用锁机制保护代码块，确保任务分配过程中数据的一致性</span><br><span class="line">      val taskDescs = withLock &#123;</span><br><span class="line">        // Filter out executors under killing</span><br><span class="line">//  executorDataMap 是一个包含所有注册 Executor 信息的映射，其中 key 是 Executor ID，value 是 ExecutorData。</span><br><span class="line">//  通过调用 isExecutorActive 方法，过滤出当前未被标记为 “正在关闭” 的 Executor。</span><br><span class="line">  /**</span><br><span class="line">   * executorDataMap = Map(</span><br><span class="line">   *  &quot;exec-1&quot; -&gt; ExecutorData(..., executorHost = &quot;host1&quot;, freeCores = 4, ...),</span><br><span class="line">   *  &quot;exec-2&quot; -&gt; ExecutorData(..., executorHost = &quot;host2&quot;, freeCores = 8, ...),</span><br><span class="line">   *  &quot;exec-3&quot; -&gt; ExecutorData(..., executorHost = &quot;host3&quot;, freeCores = 0, ...)</span><br><span class="line">   *  )</span><br><span class="line">   * activeExecutors = Map(</span><br><span class="line">   *  &quot;exec-1&quot; -&gt; ExecutorData(...),</span><br><span class="line">   *  &quot;exec-2&quot; -&gt; ExecutorData(...)</span><br><span class="line">   *  )</span><br><span class="line">   */</span><br><span class="line">        val activeExecutors = executorDataMap.filterKeys(isExecutorActive)</span><br><span class="line">//  每个活跃的 Executor 被包装成一个 WorkerOffer，它描述了当前 Executor 提供的资源信息，例如可用的 CPU 核数、主机地址等。</span><br><span class="line">  /**</span><br><span class="line">   * workOffers = IndexedSeq(</span><br><span class="line">   *  WorkerOffer(&quot;exec-1&quot;, &quot;host1&quot;, 4, Some(&quot;host1:12345&quot;), Map(&quot;GPU&quot; -&gt; Buffer(&quot;GPU-1&quot;)), 0),</span><br><span class="line">   *  WorkerOffer(&quot;exec-2&quot;, &quot;host2&quot;, 8, Some(&quot;host2:12345&quot;), Map(&quot;GPU&quot; -&gt; Buffer(&quot;GPU-2&quot;, &quot;GPU-3&quot;)), 0)</span><br><span class="line">   *  )</span><br><span class="line">   */</span><br><span class="line">        val workOffers = activeExecutors.map &#123;</span><br><span class="line">          case (id, executorData) =&gt;</span><br><span class="line">            new WorkerOffer(id, executorData.executorHost, executorData.freeCores,</span><br><span class="line">              Some(executorData.executorAddress.hostPort),</span><br><span class="line">              executorData.resourcesInfo.map &#123; case (rName, rInfo) =&gt;</span><br><span class="line">                (rName, rInfo.availableAddrs.toBuffer)</span><br><span class="line">              &#125;, executorData.resourceProfileId)</span><br><span class="line">        &#125;.toIndexedSeq</span><br><span class="line">//  调用调度器（TaskSchedulerImpl）的 resourceOffers 方法，将生成的 WorkerOffer 列表传入。</span><br><span class="line">  //  调度器根据任务需求尝试分配资源，并返回被分配的任务描述（TaskDescription）。</span><br><span class="line">        scheduler.resourceOffers(workOffers, true)</span><br><span class="line">      &#125;</span><br><span class="line">//      如果调度器返回的任务描述（taskDescs）不为空，则调用 launchTasks() 将任务发送到对应的 Executor 上运行。</span><br><span class="line">      /**</span><br><span class="line">       * taskDescs = Seq(</span><br><span class="line">       *  TaskDescription(..., executorId = &quot;exec-1&quot;, ...),</span><br><span class="line">       *  TaskDescription(..., executorId = &quot;exec-2&quot;, ...)</span><br><span class="line">       *  )</span><br><span class="line">       */</span><br><span class="line">      if (taskDescs.nonEmpty) &#123;</span><br><span class="line">        launchTasks(taskDescs)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径：org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#makeOffers</li>
<li>scheduler.resourceOffers(workOffers, true) 将取出调度队列的 taskSet 与 资源进行组合绑定，组成 taskDescs</li>
<li>如果 taskDescs 不为空，调用 launchTasks 函数</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    // Launch tasks returned by a set of resource offers</span><br><span class="line">    private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = &#123;</span><br><span class="line">      for (task &lt;- tasks.flatten) &#123;</span><br><span class="line">        /**</span><br><span class="line">         * TaskDescription(</span><br><span class="line">         *  taskId = 1234,</span><br><span class="line">         *  executorId = &quot;exec-1&quot;,</span><br><span class="line">         *  index = 0,</span><br><span class="line">         *  attemptNumber = 1,</span><br><span class="line">         *  name = &quot;Map Task 0&quot;,</span><br><span class="line">         *  serializedTask = ByteBuffer.wrap(taskData)</span><br><span class="line">         *  )</span><br><span class="line">         */</span><br><span class="line">        val serializedTask = TaskDescription.encode(task)</span><br><span class="line">        if (serializedTask.limit() &gt;= maxRpcMessageSize) &#123;</span><br><span class="line">          Option(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach &#123; taskSetMgr =&gt;</span><br><span class="line">            try &#123;</span><br><span class="line">              var msg = &quot;Serialized task %s:%d was %d bytes, which exceeds max allowed: &quot; +</span><br><span class="line">                s&quot;$&#123;RPC_MESSAGE_MAX_SIZE.key&#125; (%d bytes). Consider increasing &quot; +</span><br><span class="line">                s&quot;$&#123;RPC_MESSAGE_MAX_SIZE.key&#125; or using broadcast variables for large values.&quot;</span><br><span class="line">              msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize)</span><br><span class="line">              taskSetMgr.abort(msg)</span><br><span class="line">            &#125; catch &#123;</span><br><span class="line">              case e: Exception =&gt; logError(&quot;Exception in error callback&quot;, e)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">//          根据任务分配的 executorId，从 executorDataMap 中获取对应 Executor 的状态信息</span><br><span class="line">          val executorData = executorDataMap(task.executorId)</span><br><span class="line">          // Do resources allocation here. The allocated resources will get released after the task</span><br><span class="line">          // finishes.</span><br><span class="line">//          获取 Executor 的资源配置文件 ID</span><br><span class="line">          val rpId = executorData.resourceProfileId</span><br><span class="line">//          根据 ID 获取具体的 ResourceProfile（定义 CPU、GPU 等资源需求）</span><br><span class="line">          val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)</span><br><span class="line">//          从配置文件中读取任务所需的 CPU 核数（默认 1）</span><br><span class="line">          val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)</span><br><span class="line">//          减少 Executor 的空闲 CPU 核数</span><br><span class="line">          executorData.freeCores -= taskCpus</span><br><span class="line">//          遍历任务所需的资源（如 GPU），检查 Executor 是否具备该资源，并调用 acquire 方法分配具体资源地址</span><br><span class="line">          task.resources.foreach &#123; case (rName, rInfo) =&gt;</span><br><span class="line">            assert(executorData.resourcesInfo.contains(rName))</span><br><span class="line">            executorData.resourcesInfo(rName).acquire(rInfo.addresses)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          logDebug(s&quot;Launching task $&#123;task.taskId&#125; on executor id: $&#123;task.executorId&#125; hostname: &quot; +</span><br><span class="line">            s&quot;$&#123;executorData.executorHost&#125;.&quot;)</span><br><span class="line">//          使用 RPC 调用将 LaunchTask 消息发送到目标 Executor，其中包含序列化的任务数据。</span><br><span class="line">//          接收方org.apache.spark.executor.CoarseGrainedExecutorBackend.receive</span><br><span class="line">          executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径为：org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#launchTasks</li>
<li>使用代码 executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))) 发送 rpc 消息 LaunchTask，接收方为：org.apache.spark.executor.CoarseGrainedExecutorBackend.receive#case LaunchTask</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    case LaunchTask(data) =&gt;</span><br><span class="line">      if (executor == null) &#123;</span><br><span class="line">        exitExecutor(1, &quot;Received LaunchTask command but executor was null&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">//        TaskDescription.decode：将二进制数据反序列化为 TaskDescription 对象，包含以下关键信息：</span><br><span class="line">        /**</span><br><span class="line">         * case class TaskDescription(</span><br><span class="line">         *  taskId: Long,            // 任务唯一标识</span><br><span class="line">         *  executorId: String,      // 目标 Executor ID</span><br><span class="line">         *  index: Int,              // 任务在 TaskSet 中的索引</span><br><span class="line">         *  attemptNumber: Int,      // 任务尝试次数</span><br><span class="line">         *  name: String,            // 任务名称（如 &quot;Map Task 0&quot;）</span><br><span class="line">         *  serializedTask: ByteBuffer, // 序列化的任务代码</span><br><span class="line">         *  resources: Map[String, ResourceInformation] // 资源需求（如 GPU 地址）</span><br><span class="line">         *  )</span><br><span class="line">         */</span><br><span class="line">        val taskDesc = TaskDescription.decode(data.value)</span><br><span class="line">        logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)</span><br><span class="line">//        记录任务资源信息,当任务完成或失败时，通过这些记录释放资源，确保资源被正确回收</span><br><span class="line">        taskResources(taskDesc.taskId) = taskDesc.resources</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * Executor.launchTask 的作用：</span><br><span class="line">         * 创建 TaskRunner 对象，封装任务执行的逻辑</span><br><span class="line">         * 将 TaskRunner 提交到线程池（ThreadPoolExecutor）中执行</span><br><span class="line">         * 参数：</span><br><span class="line">         * this：当前 CoarseGrainedExecutorBackend 的引用，用于任务完成后回调状态</span><br><span class="line">         * taskDesc：任务描述，包含序列化后的任务代码和资源需求</span><br><span class="line">         */</span><br><span class="line">        executor.launchTask(this, taskDesc)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径：org.apache.spark.executor.CoarseGrainedExecutorBackend#receive</li>
<li>将包含序列化后的任务代码和资源需求 taskDesc 传入 Executor 的 launcheTask 函数</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = &#123;</span><br><span class="line">//    从任务描述中提取唯一任务ID，用于后续跟踪和管理</span><br><span class="line">    val taskId = taskDescription.taskId</span><br><span class="line">    /**</span><br><span class="line">     * createTaskRunner 的作用：</span><br><span class="line">     * 创建 TaskRunner 实例，封装任务执行逻辑，包括：</span><br><span class="line">     * 反序列化任务代码（用户定义的 Task 逻辑）</span><br><span class="line">     * 分配任务所需的资源（如 GPU 地址）</span><br><span class="line">     * 绑定任务完成后的回调方法（通过 ExecutorBackend 向 Driver 发送状态更新）</span><br><span class="line">     * 关键参数：</span><br><span class="line">     * context：ExecutorBackend 的引用，用于任务状态回调</span><br><span class="line">     * taskDescription：包含任务序列化数据及资源需求</span><br><span class="line">     */</span><br><span class="line">    val tr = createTaskRunner(context, taskDescription)</span><br><span class="line">    /**</span><br><span class="line">     * runningTasks 的结构：</span><br><span class="line">     * ConcurrentHashMap[Long, TaskRunner]，线程安全的哈希表，记录所有正在运行的任务</span><br><span class="line">     * 作用：</span><br><span class="line">     *  跟踪任务执行状态，便于后续查询或取消任务。</span><br><span class="line">     *  资源管理：确保任务结束后释放占用的资源。</span><br><span class="line">     */</span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">    /**</span><br><span class="line">     * killMarks 的作用：</span><br><span class="line">     * ConcurrentHashMap[Long, (Boolean, String)]，记录被外部请求终止的任务及其原因。</span><br><span class="line">     * 处理逻辑：</span><br><span class="line">     * 若任务在提交前已被标记为终止（如 Driver 发送了 KillTask 消息），立即调用 TaskRunner.kill() 终止任务。</span><br><span class="line">     * killMark._1：是否中断任务线程（true 表示强制终止）</span><br><span class="line">     * killMark._2：终止原因描述（如用户手动取消）</span><br><span class="line">     */</span><br><span class="line">    val killMark = killMarks.get(taskId)</span><br><span class="line">    if (killMark != null) &#123;</span><br><span class="line">      tr.kill(killMark._1, killMark._2)</span><br><span class="line">      killMarks.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 线程池配置：</span><br><span class="line">     * ThreadPoolExecutor，核心线程数由 spark.executor.cores 控制（默认1个线程/CPU核心）</span><br><span class="line">     * 作用：</span><br><span class="line">     *  将 TaskRunner 提交到线程池异步执行，实现任务并行化</span><br><span class="line">     * 任务执行流程：</span><br><span class="line">     *  反序列化任务代码。</span><br><span class="line">     *  执行用户逻辑（如 map、reduce 操作）。</span><br><span class="line">     *  将结果或异常通过 ExecutorBackend.statusUpdate() 发送给 Driver。</span><br><span class="line">     */</span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">    if (decommissioned) &#123;</span><br><span class="line">      log.error(s&quot;Launching a task while in decommissioned state.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>代码路径为：org.apache.spark.executor.Executor#launchTask</li>
<li>创建 TaskRunner 实例，封装任务执行逻辑</li>
<li>将 TaskRunner 提交到线程池异步执行，实现任务并行化</li>
</ul>
<h4 id="（2）代码解析-3"><a href="#（2）代码解析-3" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>当 Driver 收到 LaunchedExecutor(executorId) 消息时，表示某个 Executor 已成功启动并向 Driver 注册</li>
<li>重置该 Executor 的空闲资源（CPU 核心数），标记为“完全可用”</li>
<li>立即触发任务调度，将待处理的任务分配给该 Executor</li>
<li>触发 makeOffers 函数，将 Executor 的可用资源封装为 WorkerOffer，供调度器分配任务</li>
<li>调用 TaskSchedulerImpl 类里面的 resourceOffers 函数，调度器根据任务优先级、数据本地性、资源需求等条件，从待调度队列中选择合适的任务，生成任务描述</li>
<li>如果生成的任务描述 taskDescs 不为空，调用 launchTasks 函数<ul>
<li>executorData：根据任务分配的 executorId，从 executorDataMap 中获取对应 Executor 的状态信息</li>
<li>rpId：获取 Executor 的资源配置文件 ID</li>
<li>prof：根据 ID 获取具体的 ResourceProfile（定义 CPU、GPU 等资源需求）</li>
<li>taskCpus：从配置文件中读取任务所需的 CPU 核数（默认 1）</li>
<li>减少 Executor 的空闲 CPU 核数</li>
<li>遍历任务所需的资源（如 GPU），检查 Executor 是否具备该资源，并调用 acquire 方法分配具体资源地址</li>
<li>使用 RPC 调用将 LaunchTask 消息发送到目标 Executor，其中包含序列化的任务数据，接收方为 org.apache.spark.executor.CoarseGrainedExecutorBackend.receive</li>
</ul>
</li>
</ul>
<h3 id="5，详细分析-CoarseGrainedExecutorBackend-接收-LaunchTask-后的执行逻辑"><a href="#5，详细分析-CoarseGrainedExecutorBackend-接收-LaunchTask-后的执行逻辑" class="headerlink" title="5，详细分析 CoarseGrainedExecutorBackend 接收 LaunchTask 后的执行逻辑"></a>5，详细分析 CoarseGrainedExecutorBackend 接收 LaunchTask 后的执行逻辑</h3><h4 id="（1）代码-4"><a href="#（1）代码-4" class="headerlink" title="（1）代码"></a>（1）代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">    case LaunchTask(data) =&gt;</span><br><span class="line">      if (executor == null) &#123;</span><br><span class="line">        exitExecutor(1, &quot;Received LaunchTask command but executor was null&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        val taskDesc = TaskDescription.decode(data.value)</span><br><span class="line">        logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)</span><br><span class="line">        taskResources(taskDesc.taskId) = taskDesc.resources</span><br><span class="line">        executor.launchTask(this, taskDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = &#123;</span><br><span class="line">    val taskId = taskDescription.taskId</span><br><span class="line">//    创建 TaskRunner 对象</span><br><span class="line">    val tr = createTaskRunner(context, taskDescription)</span><br><span class="line">//     将 TaskRunner 注册到 runningTasks（用于跟踪运行中的任务）</span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">    val killMark = killMarks.get(taskId)</span><br><span class="line">    if (killMark != null) &#123;</span><br><span class="line">      tr.kill(killMark._1, killMark._2)</span><br><span class="line">      killMarks.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">//    提交到线程池执行</span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">    if (decommissioned) &#123;</span><br><span class="line">      log.error(s&quot;Launching a task while in decommissioned state.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）代码解析-4"><a href="#（2）代码解析-4" class="headerlink" title="（2）代码解析"></a>（2）代码解析</h4><ul>
<li>case LaunchTask 代码路径：org.apache.spark.executor.CoarseGrainedExecutorBackend#receive</li>
<li>launchTask 代码路径：org.apache.spark.executor.Executor#launchTask</li>
<li>taskDesc：将二进制数据反序列化为 TaskDescription 对象</li>
<li>使用 taskResources 记录任务资源信息,当任务完成或失败时，通过这些记录释放资源，确保资源被正确回收</li>
<li>executor.launchTask 创建 TaskRunner 对象，封装任务执行的逻辑，将 TaskRunner 提交到线程池（ThreadPoolExecutor）中执行<ul>
<li>tr 为创建 TaskRunner 对象</li>
<li>将 TaskRunner 注册到 runningTasks（用于跟踪运行中的任务）</li>
<li>killMark：ConcurrentHashMap[Long, (Boolean, String)]，记录被外部请求终止的任务及其原因</li>
<li>ThreadPoolExecutor，核心线程数由 spark.executor.cores 控制（默认1个线程&#x2F;CPU核心），将 TaskRunner 提交到线程池异步执行，实现任务并行化。线程池是调用 TaskRunner 实例里面的 run() 方法</li>
</ul>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h4 id="整体流程图"><a href="#整体流程图" class="headerlink" title="整体流程图"></a>整体流程图</h4><p><img src="/yunshenBlog.github.io/images/process_after_submit_app.png" alt="png图片"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://nrliangxy.github.io/yunshenBlog.github.io">nrliangxy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://nrliangxy.github.io/yunshenBlog.github.io/2025/05/20/process-after-submit-app/">https://nrliangxy.github.io/yunshenBlog.github.io/2025/05/20/process-after-submit-app/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/spark-3-3-0/">spark 3.3.0</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/standalone/">standalone</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/Executor/">Executor</a><a class="post-meta__tags" href="/yunshenBlog.github.io/tags/submit/">submit</a></div><div class="post-share"><div class="social-share" data-image="/yunshenBlog.github.io/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/" title="spark 在 standalone 模式，Master 是如何调度和启动 Executor"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">spark 在 standalone 模式，Master 是如何调度和启动 Executor</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数launchExecutor()通知 Worker 和 Driver 要启动一个新的 Executor，后续 Worker 和 Driver 端都是如何进行通信启动和执行 Executor？  流程梳理1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这种方式，Master 告诉 Worker 启动一个新的 Executor 并为其分配资源（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899case LaunchExecutor(masterUrl, appId, execId, appDesc,...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/" title="spark 在 standalone 模式，Master 是如何调度和启动 Executor"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-18</div><div class="info-item-2">spark 在 standalone 模式，Master 是如何调度和启动 Executor</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数launchExecutor()通知 Worker 和 Driver 要启动一个新的 Executor，后续 Worker 和 Driver 端都是如何进行通信启动和执行 Executor？  流程梳理1，消息会发送到 Worker 的 endpoint，这是 Worker 接收消息的端点。通过这种方式，Master 告诉 Worker 启动一个新的 Executor 并为其分配资源（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899case LaunchExecutor(masterUrl, appId, execId, appDesc,...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-28</div><div class="info-item-2">spark在standalone模式下，Master如何实现资源的调度和分配</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在 standalone 模式下，Master 通过函数schedule()来刷新资源情况，同时启动 Executor 的调度逻辑，为 Application 分配 Executor 资源  流程梳理1，调用scheduler()函数（1）代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960  /**   * Schedule the currently available resources among waiting apps. This method will be called   * every time a new app joins or resource availability changes.   * 调度集群资源，启动等待的 Driver 和 Executor。   * 核心逻辑：   * 检查 Master...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/02/25/spark-driver-master-rpc/" title="spark在standalone模式下，driver&#x2F;application与master的rpc通信流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-25</div><div class="info-item-2">spark在standalone模式下，driver&#x2F;application与master的rpc通信流程</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，在使用client模式下提交application的时候，driver运行在客户端机器上 如果客户端机器崩溃或者driver进程退出，application会失败 由于driver不在集群中，master无法直接管理driver的生命周期 本次只讨论在standalone模式下，使用client模式向master提交application的时候，driver和application与master的通信流程  流程梳理1.在客户端服务器上面提交application(1) 在客户端提交命令样例12345spark-submit --master spark://&lt;master-ip&gt;:&lt;master-port&gt; \  --deploy-mode client \  --class &lt;main-class&gt; \  &lt;application-jar&gt; \  &lt;application-args&gt;  (2)...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/21/zookeeper-master-select-leader/" title="spark在standalone模式下，使用zookeeper进行master选举流程剖析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-21</div><div class="info-item-2">spark在standalone模式下，使用zookeeper进行master选举流程剖析</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，为了确保master的高可用性，使用zookeeper来实现对master监控和选举 如果当前leader宕机后，如何实现leader的快速选举和相关worker节点，driver节点的快速恢复 本文要详细探讨一下leader选举和恢复的详细细节  流程梳理1.首次启动spark集群的master节点，使用start-master.sh来启动master节点(1) 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private[deploy] object Master extends Logging &#123;  val SYSTEM_NAME = &quot;sparkMaster&quot;  val ENDPOINT_NAME = &quot;Master&quot;  def...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">spark在standalone模式下，worker与master的rpc通信流程</div></div><div class="info-2"><div class="info-item-1">&emsp;&emsp; 背景 spark集群在standalone模式下，在集群的sbin目录下，启动start-worker.sh脚本，启动了worker端点 worker端点启动后，读取配置后，在spark的rpc通信框架里面注册rpc端点，和master节点进行rpc通信并进行心跳交互 本文就是要详细探讨一下work节点是如何与master节点进行rpc通信  流程梳理1.使用start-worker.sh脚本启动worker节点(1) 在客户端提交命令样例1bash start-worker.sh  (2) 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647  def main(argStrings: Array[String]): Unit = &#123;//    设置一个默认的未捕获异常处理器 SparkUncaughtExceptionHandler。//    exitOnUncaughtException = false...</div></div></div></a><a class="pagination-related" href="/yunshenBlog.github.io/2025/02/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="spark在standalone模式下，FIFO和FAIR调度模式的对象分析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-08</div><div class="info-item-2">spark在standalone模式下，FIFO和FAIR调度模式的对象分析</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/yunshenBlog.github.io/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/yunshenBlog.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">nrliangxy</div><div class="author-info-description"></div><div class="site-data"><a href="/yunshenBlog.github.io/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/yunshenBlog.github.io/tags/"><div class="headline">Tags</div><div class="length-num">12</div></a><a href="/yunshenBlog.github.io/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">流程梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%BC%8C%E9%80%9A%E8%BF%87-object-SparkSubmit-%E7%9A%84-main-%E5%87%BD%E6%95%B0%E9%87%8C%E9%9D%A2%E7%9A%84-submit-doSubmit-args-%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81"><span class="toc-number">2.1.</span> <span class="toc-text">1，通过 object SparkSubmit 的 main 函数里面的 submit.doSubmit(args) 提交代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81"><span class="toc-number">2.1.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.1.2.</span> <span class="toc-text">（2）代码解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89app-%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.1.3.</span> <span class="toc-text">（3）app 任务提交过程中，函数调用流程图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%EF%BC%8C%E5%90%AF%E5%8A%A8%E7%94%A8%E6%88%B7%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81%E9%87%8C%E9%9D%A2-SparkContext-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">2，启动用户提交代码里面 SparkContext() 初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-1"><span class="toc-number">2.2.2.</span> <span class="toc-text">（2）代码解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89SparkContext-%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.2.3.</span> <span class="toc-text">（3）SparkContext() 初始化过程中，函数调用流程图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%EF%BC%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B8%BA-DAG"><span class="toc-number">2.3.</span> <span class="toc-text">3，代码解析为 DAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.3.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-2"><span class="toc-number">2.3.2.</span> <span class="toc-text">（2）代码解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E4%BB%A3%E7%A0%81%E8%BD%AC%E6%8D%A2%E4%B8%BA-DAG-%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.3.3.</span> <span class="toc-text">（3）代码转换为 DAG 函数调用流程图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%EF%BC%8CDAG-%E5%88%86%E8%A7%A3%E4%B8%BA-Task"><span class="toc-number">2.4.</span> <span class="toc-text">4，DAG 分解为 Task</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-3"><span class="toc-number">2.4.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-3"><span class="toc-number">2.4.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%EF%BC%8C%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90-CoarseGrainedExecutorBackend-%E6%8E%A5%E6%94%B6-LaunchTask-%E5%90%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91"><span class="toc-number">2.5.</span> <span class="toc-text">5，详细分析 CoarseGrainedExecutorBackend 接收 LaunchTask 后的执行逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81-4"><span class="toc-number">2.5.1.</span> <span class="toc-text">（1）代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-4"><span class="toc-number">2.5.2.</span> <span class="toc-text">（2）代码解析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">3.0.1.</span> <span class="toc-text">整体流程图</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/05/20/process-after-submit-app/" title="spark在standalone模式下，提交代码后的整个处理流程">spark在standalone模式下，提交代码后的整个处理流程</a><time datetime="2025-05-19T23:25:51.000Z" title="Created 2025-05-20 07:25:51">2025-05-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/04/18/spark-master-to-executor-task/" title="spark 在 standalone 模式，Master 是如何调度和启动 Executor">spark 在 standalone 模式，Master 是如何调度和启动 Executor</a><time datetime="2025-04-17T22:53:11.000Z" title="Created 2025-04-18 06:53:11">2025-04-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/28/spark-master-dispatch/" title="spark在standalone模式下，Master如何实现资源的调度和分配">spark在standalone模式下，Master如何实现资源的调度和分配</a><time datetime="2025-03-28T01:57:10.000Z" title="Created 2025-03-28 09:57:10">2025-03-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/21/zookeeper-master-select-leader/" title="spark在standalone模式下，使用zookeeper进行master选举流程剖析">spark在standalone模式下，使用zookeeper进行master选举流程剖析</a><time datetime="2025-03-21T03:57:09.000Z" title="Created 2025-03-21 11:57:09">2025-03-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/yunshenBlog.github.io/2025/03/06/spark-work-master-rpc/" title="spark在standalone模式下，worker与master的rpc通信流程">spark在standalone模式下，worker与master的rpc通信流程</a><time datetime="2025-03-06T01:13:14.000Z" title="Created 2025-03-06 09:13:14">2025-03-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By nrliangxy</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/yunshenBlog.github.io/js/utils.js"></script><script src="/yunshenBlog.github.io/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>